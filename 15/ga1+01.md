# Geometry Algebra for 1% Power Engineers

**An engineering-first lens on Geometric Algebra (GA) in electric power systems—unifications that pay rent, quantified costs, disciplined decisions.**

## Front Matter

### Preface: Engineering Makes Mathematics Honest

**The Gap Between Elegance and Execution:** There's often a gulf between the beautiful generality of mathematical methods and the messy realities of implementing them in hardware and software. This book confronts that gap head-on, showing where elegant geometric formulations shine and where execution details can undermine them.

**Why Power Systems:** Electric power systems are a perfect storm of complexity—multi-domain (electrical, mechanical, magnetic), multi-rate (microsecond switching to hourly load cycles), with geometric constraints everywhere (three-phase symmetry, spatial layouts, rotating machines). If a mathematical tool can handle power systems, it can handle almost anything.

**The Hybrid Architecture:** Embracing GA doesn't mean abandoning conventional tools; it means using each where it is strongest. We advocate a hybrid approach: use GA for specification and insight (where its geometric clarity excels) but translate to matrices/quaternions/standard methods for execution when hardware demands it. This isn't compromise—it's choosing the correct tool for each job.

**What This Book Actually Does:** We teach GA by accident. By solving real power engineering problems, we gradually introduce the concepts of geometric algebra without assuming prior knowledge. The goal is that by the end, the reader has absorbed GA naturally. Along the way, we implicitly encourage building better software tools (since current GA tooling is lacking) by highlighting needs.

**Navigation Paths:** There are multiple ways to read this book, depending on your interest:

- **Mathematical Foundations → Computational Reality → Power Applications:** Start with the GA theory, see the computational challenges, then apply to power engineering problems.

- **Power Systems → Geometric Tools → Broader Implications:** Begin with familiar power systems concepts, introduce GA tools as needed, then see how those tools generalize beyond power engineering.

- **Benchmarks → Patterns → Decision Framework:** Focus on the empirical results and case studies, notice recurring patterns where GA works or fails, and culminate in a framework for deciding when to use GA in practice.

### Notation, Metrics, and Permanent Choices

**GA Models:** We primarily use Projective Geometric Algebra (PGA) $\mathbb{R}_{3,0,1}$ for 3D Euclidean geometry (rigid motions and incidence structure), because homogeneous coordinates come naturally. We occasionally use Space-Time Algebra (STA) $Cl(1,3)$ for electromagnetic field theory, where electromagnetic bivectors live. Each algebra is introduced when it first appears, but the notation stays consistent.

**Basis Convention:** In 3D PGA, our basis is $\{e_1, e_2, e_3, e_0\}$, where $e_1^2 = e_2^2 = e_3^2 = +1$ (Euclidean directions) and $e_0^2 = 0$ (ideal or "at infinity" direction). The pseudoscalar is $I = e_{1230}$. Vectors in this algebra represent planes: for example, $e_1$ corresponds to the plane $x = 0$, and $e_0$ is the plane at infinity.

**Grade and Multivectors:** We denote the grade-$k$ part of a multivector $A$ as $\langle A \rangle_k$. Grades have specific meanings: 0 = scalar, 1 = vector (direction), 2 = bivector (oriented plane segment), 3 = trivector (oriented volume), 4 = quadvector (in 3D PGA, the "empty" or null volume element, used to signal no intersection).

**Key Operations:** The geometric product $ab$ between two vectors yields a scalar (dot product $a \cdot b$) plus a bivector (wedge product $a \wedge b$):
$$ab = a \cdot b + a \wedge b$$

Reversion $\tilde{A}$ is the grade-reversing involution (it flips the order of basis vector multiplication, so for example $(e_1 e_2)^{\sim} = e_2 e_1 = -e_1 e_2$). The dual $A^* = AI^{-1}$ maps between outer product (OPNS) and inner product (IPNS) representations by multiplication with the inverse pseudoscalar. We will use duality to compute intersections (meets) and spans (joins) uniformly.

**PGA Specifics:** In PGA, points are represented as trivectors, lines as bivectors, and planes as vectors (this might feel inverted compared to typical Euclidean concepts, but it simplifies incidence calculations). For instance, a point located at Euclidean coordinates $(x,y,z)$ is
$$P = x e_{032} + y e_{013} + z e_{021} + e_{123}$$
a trivector. A plane is $\Pi = ae_1 + be_2 + ce_3 + de_0$, corresponding to the plane with normal $(a,b,c)$ at distance $d$ from the origin. A line can be represented as the wedge of two planes (bivector), automatically encoding both its direction and a moment indicating position (Plücker coordinates appear naturally).

**Rotors and Motors:** A rotor $R$ is a grade-0+2 multivector (even-grade) that performs a rotation by the sandwich operation $v' = R v \tilde{R}$. In PGA, a motor (rotation + translation) is similarly represented by an even multivector $M$ that includes grade-0,2,4 parts. Rotors and motors are the GA analogs of rotation matrices and homogeneous transform matrices, respectively. We will normalize these so that $R\tilde{R} = 1$ (they lie on the unit pseudosphere in algebra). When we say an object like a rotor "acts" on another object, it's always via $X' = RX\tilde{R}$.

*(With these conventions in hand, we proceed into the main content. If you ever get lost in notation, Appendix A provides a complete reference.)*

---

## Part I: Geometric Foundations (where special cases vanish)

### Chapter 0: Power Systems Context (minimal scaffolding)

#### 0.1 Why Three Phases

Why do modern power systems use three-phase AC? The short answer is that three is the minimum number of phases that allows constant power transfer to a balanced load. With three sinusoidal phase voltages 120° apart, the instantaneous total power $p(t) = v_a i_a + v_b i_b + v_c i_c$ is actually constant over time, rather than pulsing at double the line frequency as it would in single-phase systems. Three phases also produce a rotating magnetic field with constant magnitude in motors, yielding smooth torque. Two phases are not enough (they would produce a pulsating field), and anything more than three has diminishing returns relative to the extra complexity and conductor material. Three is the sweet spot: it gives efficiency and performance with the simplest possible configuration. When the three-phase balance breaks (say one phase drops out or is faulty), we get unbalanced currents, unbalanced torque, and extra heating—topics we'll address with GA tools in later chapters.

#### 0.2 The Transformation Zoo

Engineers have developed a menagerie of transformations to manage three-phase systems. Two of the most famous are the Clarke transform (abc → αβ0) and the Park transform (αβ → dq). The Clarke transform projects three-phase quantities $(a,b,c)$ onto an orthogonal 2D plane (αβ) plus a zero component (0) that captures any imbalance. The Park transform then rotates this αβ plane into a d,q frame that spins with the electrical frequency, effectively turning AC quantities into DC in the steady state. By chaining these, three-phase sinusoidal voltages and currents become almost constant in the dq frame, greatly simplifying control (like how a synchronous motor controller keeps the d-axis aligned with the rotor flux).

Why bring this up in a GA book? Because these transforms are inherently geometric. In fact, Clarke and Park transforms can be derived as rotations in an abstract 2D/3D space. Clarke's transform is essentially a rotation that orthonormalizes the three-phase basis (via Gram-Schmidt) and isolates the zero-sequence component. Park's transform is a time-varying rotation in that orthonormal plane (the $e_{12}$ bivector plane in GA) by the electrical angle $\theta$. In GA terms, the combined dq0 transform is one rotor (a specialized multivector) that can be seen as $R_{dq0}(t) = R_{Park}(t) R_{Clarke}$. We won't need complex numbers or 3×3 matrices at all to derive or use these transforms—they fall out from simple geometric identities.

#### 0.3 Harmonics and Distortion Reality

Idealized power systems are blissfully sinusoidal, but reality bites: non-linear loads and other factors produce harmonics (integer multiples of the fundamental frequency) and even inter-harmonics (non-integer frequencies) in voltages and currents. A typical AC waveform might have a 60 Hz fundamental plus 3rd, 5th, 7th harmonics, etc. We quantify overall distortion by THD (Total Harmonic Distortion), essentially the ratio of the RMS of all harmonic components to the RMS of the fundamental. Why do complex numbers (phasors) struggle here? Because a single complex phasor assumes one frequency. In a distorted waveform, you'd need a separate phasor for each harmonic frequency; the nice single-plane picture of voltage vs current breaks down. GA, in contrast, can handle multicomponent waveforms more naturally by superposing vectors for each harmonic and even capturing their interactions through multivector products (as we'll explore in Chapter 4). In short, distortion introduces higher-grade components in GA that phasor algebra can't easily accommodate – a warning that we'll need advanced tools for advanced problems.

#### 0.4 Control Timing Constraints

Power electronics and real-time control add another layer of challenge. Switching converters (inverters, motor drives, etc.) use PWM (Pulse-Width Modulation) with switching frequencies on the order of 10–20 kHz. This means the control loop that updates the duty cycles has a budget of at most ~50–100 microseconds per cycle. In that time, the microcontroller must sample currents/voltages, run control algorithms (like a PID or state estimator in the dq frame), and output new gating signals. Jitter or missing a deadline can cause instability or even equipment damage. Every microsecond matters in these systems. This is why computational overhead is a critical factor: an approach that is "fine in theory" might be completely impractical if it can't run fast enough on embedded hardware. We will constantly keep an eye on algorithmic complexity and real-time feasibility as we explore GA-based solutions for power control. This is part of the honesty that engineering forces on mathematics.

### Chapter 1: Multivectors — The Type System of Geometry

#### 1.1 Objects Have Grades

One of the fundamental insights of geometric algebra is that the usual division of "scalars vs vectors" is just the beginning—geometry naturally involves higher-dimensional elements. GA formalizes this with the concept of grade. A scalar is grade 0 (just a number, no direction). An ordinary vector (like a 3D arrow) is grade 1 (magnitude and one direction). A bivector is grade 2—it represents an oriented plane segment, something with an area and an orientation (like an arrow looping around an area, or a plane element defined by two directions). A trivector is grade 3—an oriented volume element (in 3D, essentially a parallelepiped volume with an orientation). In 3D space, grade 3 is the highest (aside from the grade-4 pseudoscalar which in PGA can represent the empty set). In higher dimensions, you could keep going (4-volume, 5-volume, etc.), though those are hard to visualize.

Why does this matter? Because the algebra knows about grade and respects it in operations. If you multiply a grade-1 vector by a grade-2 bivector, the result will be a combination of grade-1 and grade-3 (since $1+2=3$ or $2-1=1$). The outcome is predictable from the grades involved. If you wedge (exterior product) two vectors (grade 1 each), you get a bivector (grade 2) representing the plane they span. If you wedge a vector (grade 1) with a bivector (grade 2), you get a trivector (grade 3) representing a volume. GA provides a built-in type system: grade = dimensionality. Operations that would be "ugly special cases" in vector calculus (like the cross product only existing in 3D, or needing separate rules for scalars vs vectors) become unified and natural in GA. We'll see this in power systems too: for example, the concept of "real power vs reactive power" will correspond to different grades of a multivector product (scalar vs bivector parts) rather than needing separate complex numbers or coordinates.

#### 1.2 Products Encode Relationships

The geometric product is the heart of GA. For two vectors $a,b$, it factors into symmetric and antisymmetric parts:
$$ab = a \cdot b + a \wedge b$$

The dot product $a \cdot b = \langle ab \rangle_0$ is grade 0 (a scalar) and measures the magnitude projection (how much of $a$ lies along $b$). The wedge product $a \wedge b = \langle ab \rangle_2$ is grade 2 and represents the oriented area spanned by $a$ and $b$. By computing $ab$ in GA, you simultaneously get both pieces of information: the scalar dot and the bivector wedge.

This is incredibly useful. For instance, in power engineering, if voltage (as a vector) and current (as a vector) are aligned, their geometric product $vi$ has no bivector part (because $v \wedge i = 0$ when $v$ is parallel to $i$). The result is purely scalar, which corresponds to pure real (active) power. If voltage and current are perpendicular (90° phase difference), the geometric product has no scalar part ($v \cdot i = 0$), only a bivector part—this corresponds to pure reactive power (all the power is oscillating back and forth, no net transfer). For an arbitrary phase difference, $vi$ has a nonzero scalar and a nonzero bivector part, capturing both active and reactive components in one shot. In complex number notation, you'd say $S = P + jQ$, but that $j$ is an extra piece of bookkeeping. In GA, the bivector part is the reactive power, as a first-class quantity. The geometric product encodes both magnitude and orientation relationships inherently.

Another example: If $a$ and $b$ are orthogonal vectors, $ab = a \wedge b$ is a bivector (pure antisymmetric part). If $a$ and $b$ are parallel, $ab = a \cdot b$ is a scalar (pure symmetric part). GA smoothly transitions between these cases without needing if/else logic — the algebra itself handles it.

#### 1.3 First Power Application — Learning GA Without Realizing

Let's apply this to the simplest power system scenario: single-phase AC power. Traditionally, you'd represent voltage and current as phasors $V = |V|\angle\theta_V$ and $I = |I|\angle\theta_I$, and define complex power $S = VI^* = P + jQ$. Here $P = |V||I|\cos(\theta_V - \theta_I)$ is real (active) power and $Q = |V||I|\sin(\theta_V - \theta_I)$ is imaginary (reactive) power.

Now, in GA, we do something much simpler. Represent the instantaneous voltage and current as real vectors $v$ and $i$ in a 2D plane (for single-phase, a 2D geometric algebra is enough: one basis for in-phase, one for quadrature). The instantaneous power multivector is simply the geometric product:
$$S = vi = v \cdot i + v \wedge i$$

The scalar part $v \cdot i$ is the instantaneous real power $p(t)$. The bivector part $v \wedge i$ has a magnitude equal to $|v||i|\sin(\phi)$ (where $\phi$ is the phase difference) and represents the reactive power "flowing around" between the source and load. Over a full cycle, the time-average of the bivector part is zero (no net energy transferred by the reactive component), whereas the scalar part's time-average is $P$. In GA terms, power = voltage × current, and the grade-0 vs grade-2 parts cleanly separate active vs reactive power.

Now extend this to three-phase. Instead of needing symmetrical component transformation or complex 3-phase theory, we can represent the three-phase voltages as a single 3D vector $\mathbf{v}_{abc} = v_a e_1 + v_b e_2 + v_c e_3$ (with the constraint $v_a + v_b + v_c = 0$ for a balanced system, which puts $\mathbf{v}$ in a 2D subspace), and similarly $\mathbf{i}_{abc}$ for currents. Then the total instantaneous three-phase power is
$$S_{3\phi} = \mathbf{v}_{abc} \mathbf{i}_{abc}$$

This is a multivector in $Cl(3)$ (or PGA(3,0,1) if we include $e_0$, but $e_0$ isn't needed for steady-state power calculations). The result will have a scalar part (the total real power) and a bivector part (which in 3D is dual to a vector, representing the reactive power as a pseudovector oriented along the symmetry axis of the system). If there is distortion or imbalance, there may also be a trivector part (grade 3) representing an instantaneous energy storage oscillation that doesn't neatly lie in a plane. GA captures these exotic cases seamlessly: you automatically get additional grades when the situation has more complexity (e.g., unbalanced or non-sinusoidal conditions). In complex/phasor methods, by contrast, you'd have to start inventing new concepts (sequence components for imbalance, or time-domain methods for waveform distortion). Here it's just a natural extension of the same power concept.

Without even formally studying GA, we've just used it: we formulated power in terms of a geometric product, separated into grades. This foreshadows how many power engineering concepts (symmetrical components, instantaneous power theory, etc.) become simpler and more general in the language of GA.

### Chapter 2: Rotations, Reflections, and the Deep Structure

#### 2.1 The Revelation of Double Reflection

A cornerstone of geometric algebra (and geometry in general) is that any rotation can be achieved by two reflections. This is a classical result: reflecting a vector $\mathbf{v}$ across one plane (with unit normal $\mathbf{n}$) gives $\mathbf{v}' = -\mathbf{n}\mathbf{v}\mathbf{n}$ in GA (the $-$ sign and reverse order come from the reflection formula). If you then reflect $\mathbf{v}'$ across a second plane with unit normal $\mathbf{m}$, you get
$$\mathbf{v}'' = -\mathbf{m}\mathbf{v}'\mathbf{m} = \mathbf{m}\mathbf{n}\mathbf{v}\mathbf{n}\mathbf{m}$$

Notice the pattern: $\mathbf{m}\mathbf{n}$ acting on the left and $\mathbf{n}\mathbf{m}$ (which is $\widetilde{\mathbf{m}\mathbf{n}}$) on the right. Define $R = \mathbf{m}\mathbf{n}$. Then $\mathbf{v}'' = R\mathbf{v}\tilde{R}$. This $R$ is a rotor. It's the GA representation of a rotation (in this case by twice the angle between the two reflection planes, about their line of intersection). We haven't written any matrices or done any coordinate gymnastics; the sandwich product with $R$ is the rotation. Rotors combine like quaternions (in fact, in 3D Euclidean GA, rotors are isomorphic to quaternions). If you have a rotor $R_1$ for a rotation and another $R_2$ for another rotation, the composition (do $R_1$ then $R_2$) corresponds to the geometric product $R_2 R_1$ (which is itself another rotor). This simple expression $R = mn$ (two vectors multiplied) encodes a rotation, and any rotation can be encoded this way.

The deep structure here is that GA's multiplication inherently handles reflections and rotations elegantly. Rotations are not some extra structure; they are part of the algebra's multiplicative group. In linear algebra, we usually think of rotations as matrices $R$ such that $R^T R = I$. In GA, we think of rotors as multivectors $R$ such that $R\tilde{R} = 1$. They achieve the same ends but GA's approach often simplifies derivations. We will see this when deriving the Park transform as a rotor, and when analyzing mechanisms or orientation control. (Double reflections also hint that if we can reflect something easily, we can rotate it easily. And reflections are often easier—think of how easy it is to reflect a point in a line or plane using simple formulas.)

#### 2.2 Rotors in Practice

Rotors, as noted, form a multiplicative group just like rotation matrices or quaternions. If $R_1$ rotates by $\alpha$ and $R_2$ rotates by $\beta$, then $R_2 R_1$ rotates by $\beta+\alpha$ (assuming the same axis, or more generally, it combines rotations in a single rotor). The composition of two rotors (which might correspond to combining a 3D rotation and another rotation) is efficient: for example, in 3D PGA, a general rotor (rotation in 3D) can be represented with 4 parameters (like a quaternion), and composing two rotors costs on the order of the same as a quaternion multiply. In fact, one reference implementation clocks it at 48 multiplications and 40 additions for a motor (which is a rotation + translation)—comparable to or slightly less than the cost of composing 4×4 homogeneous matrices (64 mul, 48 add) for rigid transforms. The application of a rotor to rotate a vector, however, is less efficient: computing $v' = RvR$ literally expands to two geometric products. If $v$ is a vector (4 components in PGA) and $R$ is an 8-component motor, that's a lot of terms to sum up. In practice it might be ~80 mul and ~60 add (roughly 140 ops) for a general 3D motor acting on a point, which is indeed heavier than a 4×4 matrix times a 4×1 vector (16 mul, 12 add). The user's earlier quip that it's "~220 FLOPs" hints at this increased cost. The takeaway: GA gives a conceptually clear but computationally dense way to apply transformations. We will later see how this density can be mitigated (through clever data layout, specialization, or by not using the generic sandwich formula in inner loops). But it's a warning that will recur: clarity often comes at a cost in raw operations.

On a more positive note, rotors have an exponential parametrization that's very powerful. Any rotor can be written as
$$R = e^{-\frac{\theta}{2}B}$$
where $B$ is a unit bivector representing the plane of rotation and $\theta$ is the rotation angle. The half-angle is there so that $R$ and $-R$ encode the same physical rotation (just like quaternions). This exponential form connects GA to Lie algebras and Lie groups: bivectors are like the generators of rotations. In practice, it means we can smoothly interpolate rotations (via the exponential and logarithm, GA's version of slerp), and it gives insight into rotational dynamics (treating $B$ like an angular velocity bivector). For power engineers, one application is in synchronous reference frame theory: the Park transformation can be seen as $R(\theta(t)) = \exp(-\tfrac{1}{2}\theta(t)e_{12})$, where $e_{12}$ is the unit bivector in the $\alpha\beta$ plane. Differentiating that gives a bivector (lie algebra) element $-\frac{1}{2}\dot{\theta} e_{12}$, which corresponds to electrical angular velocity. So even the familiar $\omega t$ in $e^{j\omega t}$ has its GA analogue in the exponent of a rotor. Keep this exponential form in mind; it often turns complexity (compose this, then that) into simple addition in the exponent.

#### 2.3 Motors — Translation and Rotation Unified

A big advantage of using Projective GA (PGA) is that translations become simple parts of the algebra, on equal footing with rotations. In PGA $\mathbb{R}_{3,0,1}$, a motor is a rotor in 3D extended by the ideal dimension $e_0$. Rotating in a plane that includes $e_0$ produces a translation. In effect, a motor = rotation + translation unified. If $T = 1 + \frac{1}{2}\mathbf{t}e_0$ represents a translation by vector $\mathbf{t} = (t_x, t_y, t_z)$ (here $e_0$ plays the role of the plane at infinity), and $R$ is a rotor for a rotation, then the motor for doing $R$ followed by translation $\mathbf{t}$ is just $M = TR$. It's still an 8-element multivector. It still combines via multiplication: if $M_1$ is "move then rotate" and $M_2$ is another "move then rotate", then $M_2 M_1$ is the combined rigid motion. And motors act by the same sandwich formula: $X' = MX\tilde{M}$. No coordinate matrices needed, no special cases for "apply rotation then add a translation offset" as in vector algebra — it's one operation.

For a power engineering example, think of a transformer that both shifts voltage magnitude (translation in the voltage dimension) and phase (rotation). A phase-shifting transformer could be represented as a motor in an abstract voltage-current space: part of its effect is a rotation (phase change), part is a translation (voltage magnitude change). While we won't deeply explore that particular modelling, it illustrates that GA doesn't artificially separate these effects. The "screw" nature of spatial motions (rotation + translation) has a clear counterpart in power systems: changing magnitude + phase. This unity is very elegant. When we do need to work with rigid motions (for example in Chapter 16 on fault location, lines intersecting ground, etc.), motors will let us compute intersections and movements without having to treat translation as a separate case. It just falls out of the algebra.

#### 2.4 The Normalization Problem

When using rotors and motors in numerical computations, one practical issue is normalization drift. Ideally, if $R$ is a rotor, it should always satisfy $R\tilde{R} = 1$ (which is analogous to a rotation matrix being orthonormal or a quaternion being unit length). However, due to floating-point rounding, if you compose many rotors or integrate differential equations to update a rotor, you'll find $R\tilde{R}$ creeping away from 1. For example, you might start with a rotor representing 0° rotation (which is $R=1$ in GA). After 100 successive small rotations, numerical error might make $R\tilde{R} = 1.00000000000001$ (a tiny deviation). After 10,000 operations, maybe $R\tilde{R} = 1.0000000001$. After 1,000,000 operations, perhaps $R\tilde{R} = 1.0000001$ (these are illustrative numbers). The deviations tend to accumulate because each multiplication or integration step has slight error. If the rotor strays too far from unit length, it no longer represents a pure rotation (it'll encode some unwanted scaling).

The solution is to renormalize periodically: compute $R \leftarrow R / \sqrt[n]{R\tilde{R}}$ or some equivalent method to project it back to the "unit pseudosphere". In practice, normalizing a rotor or motor might cost, say, 8 multiplications and 7 additions (~15–20 FLOPs) plus a reciprocal square root. Doing this every single time is overkill, but doing it never is dangerous. A common strategy is to renormalize after a certain number of compositions or if $|R\tilde{R} - 1|$ exceeds a threshold (e.g. $10^{-8}$). In highly sensitive simulations (like long-running dynamics), you might normalize every few milliseconds of simulation time. In a real-time controller, you might never explicitly renormalize if using quaternions (since sensors help re-orthonormalize via feedback), but in pure simulation GA contexts, you will. We will mention normalization needs when we present algorithms using rotors (for instance in the PLL rotor in Chapter 18 or the EKF in Chapter 7). It's one of those implementation details that the math textbooks don't prepare you for, but engineers must handle.

### Chapter 3: Incidence — Where Lines Meet

#### 3.1 The Projective Model Makes Parallels Meet

Euclidean geometry has a notorious annoyance: parallel lines never meet, which complicates algorithms (you have to check for parallelism as a special case in intersection computations). Projective geometry elegantly fixes this by adding "points at infinity" where parallel lines do intersect (at an ideal point). PGA is essentially algebraic projective geometry. By including the null basis vector $e_0$ (with $e_0^2 = 0$), we acquire a representation for ideal elements. For example, in 3D PGA:

- **Points are represented as trivectors** $P = x e_{032} + y e_{013} + z e_{021} + w e_{123}$. If $w \neq 0$, this corresponds to an actual point at finite coordinates (proportional to $(x/w, y/w, z/w)$). If $w = 0$, the point is ideal (at infinity) in a specific direction.

- **Lines are bivectors.** A line can be thought of as the join of two points or the intersection of two planes. A general line bivector can be written as
  $$L = l_{01}e_{01} + l_{02}e_{02} + l_{03}e_{03} + l_{23}e_{23} + l_{31}e_{31} + l_{12}e_{12}$$
  where the $e_{ij}$ terms with a 0 (like $e_{01}, e_{02}, e_{03}$) capture direction (the line at infinity aspect) and the terms without 0 (like $e_{23}, e_{31}, e_{12}$) capture the moment (how far the line is from the origin).

- **Planes are grade-1 (vectors)** as mentioned: $\Pi = ae_1 + be_2 + ce_3 + de_0$. If $d \neq 0$, it's a finite plane (the $d$ essentially is the distance from origin times the normal). If $d = 0$, it's an ideal plane (which in PGA corresponds to a direction like a set of all points "at infinity" with a given orientation).

This representation might feel weird (planes as vectors?), but it works beautifully. A point is now the intersection of three planes: $P = \Pi_1 \wedge \Pi_2 \wedge \Pi_3$. Two lines meet at a point: $P = L_1 \wedge L_2$ (this will be a trivector result). Parallel lines in Euclidean space become lines that intersect at an ideal point (which PGA represents as a trivector with $w = 0$). No more "if parallel then do something else" – it's just a wedge that yields an ideal point if they don't meet in the finite plane. "Parallels meet at infinity" is built-in. This uniformity eliminates edge cases and gives polynomial formulas for everything (no divisions by zero for special cases).

#### 3.2 Meet and Join — Intersection Without Cases

In GA, the meet of two objects is their intersection, and the join of two objects is the object that they both lie in (span). Using duality, these can be computed with a single algebraic formula. If $A$ and $B$ are represented in OPNS form (like our $P, L, \Pi$ above), the meet is:
$$A \vee B = (A^* \wedge B^*)^*$$
which looks scary but it means "take the duals (to go to IPNS, where containing becomes spanning), wedge them, then dual back." The join is simpler: $A \wedge B$ (in OPNS, wedging adds the subspace together).

What's important is that these operations always work in PGA, regardless of special cases:

- **Two skew lines** (non-parallel, non-intersecting in 3D) have a meet that is the "empty" element (a null pseudoscalar, grade 4). That signals "no intersection."

- **Two parallel lines** (in a plane) have a meet which is an ideal point (a direction vector, grade-3 trivector with $w = 0$). That's the point at infinity in the direction of both lines.

- **A line and a plane** meet in a point (if the line lies in the plane, the meet will actually be a line – specifically, the same line – because the wedge of the plane's dual and line's dual would drop a dimension and dual back to the line; but that's fine, it correctly captures that the whole line is the intersection).

- **Two planes** meet in a line (unless they are parallel, in which case the meet is an ideal line at infinity).

- **Three planes** generically meet in a point (if they don't all intersect in one point, the result might be an ideal point or the empty set if no common intersection exists).

No special-case logic is needed: the algebraic formulas yield a blade (a simple multivector) that represents the correct geometric object or the null object if no intersection. We can check for an "empty intersection" by seeing if the result has grade equal to the space dimension (in 3D PGA, a result in grade 4 indicates no physical intersection). This consistent treatment is a big win when coding up things like fault analysis (where a fault may be the intersection of elements in a network) or calculating the point of intersection between a line of sight and an equipment clearance zone, etc. We'll see examples of meet/join in power contexts in Chapter 16.

#### 3.3 Computational Reality of Incidence

Of course, all this algebraic elegance comes at a computational cost. The meet formula involves duals and a wedge. A naive count for a meet of two general 3D PGA entities (say a line and a plane) might be on the order of 64 FLOPs (floating-point operations) if fully expanded. In practice, with an optimized library, you might see ~95 operations because of some overhead and not all terms needing computation (some are zero due to structure). It's more expensive than solving linear equations by Gaussian elimination for the same intersection, roughly.

A bigger issue is numerical conditioning. When lines or planes are nearly parallel, the meet (intersection point) will be far away, and that tends to amplify floating point error. For example, if two lines intersect at 89° (almost perpendicular), the meet is well-conditioned. If they intersect at 1° angle, the condition number might be about 57 (meaning you lose maybe $\log_{10}(57) \approx 1.75$ digits of precision in the result). At 0.1° difference, condition ~573; at 0.01°, ~5730. This aligns with the idea that as lines become parallel, the intersection point moves off to infinity, and a small perturbation in inputs can move the intersection a huge distance. In GA, you'll get a point with large homogeneous coordinate $w$ (indicating it's near ideal) and possibly large rounding error.

To handle this, robust algorithms include near-parallel detection: if the meet's homogeneous coordinate $w$ is below some threshold relative to the other coordinates (meaning the intersection is very far or ideal), we treat it as "effectively parallel" or we handle it with higher precision. Another approach is to normalize line representations before computing intersections, but that only helps so much. We will collect a set of countermeasures for such cases (like adding a tiny tweak to bring lines to exactly parallel and returning an ideal line, etc.) in Chapter 3.4 and Appendix D.

#### 3.4 The Counterexample Collection

No new methodology is complete without a few pathological examples to keep us honest. Over time we've gathered a "zoo" of scenarios where GA incidence calculations can misbehave if you're not careful:

- **Near-parallel lines:** Two lines that should intersect kilometers away. Computing their meet yields a point with huge coordinates (maybe beyond the range of a float to represent). Numerically, this is very sensitive. Mitigation: if the angle between lines is below a threshold (based on the magnitude of $L_1 \wedge L_2$ compared to $L_1$ and $L_2$ lengths), decide that they have "no finite intersection" for practical purposes, or use double precision (or higher) for calculation.

- **Near-coincident planes:** Three planes that almost meet at a single point but actually form a tiny triangular prism. The meet of two and then meet with third will produce a point that is wildly off due to amplification. Mitigation: compute all three meets pairwise and check consistency, or do a least-squares intersection.

- **Null or nearly null blades:** Sometimes an operation should yield the zero multivector (like intersecting two identical lines should give that same line as the "intersection", but in meet terms that line's representation might be a degenerate blade). Numerical rounding might produce a very small non-zero result instead of exact zero, leading you to think there's an intersection when it's actually the same line (or conversely, thinking two lines intersect because the result isn't exactly null). Mitigation: enforce tolerance-based zero detection – e.g., if a result's magnitude is below 1e-12 times the product of operand magnitudes, consider it zero.

- **Triple intersection and rank issues:** If you intersect three planes that nearly meet at a point, you might get three pairwise intersection lines that nearly converge. Which two do you intersect first? The answer could change the numeric result. Mitigation: perhaps solve via linear system rather than sequential meets, or use a symmetric formula.

- **Ideal interplay:** Intersecting something that's already ideal (like a line at infinity) with something finite. Usually, the meet will naturally give ideal results, but one must interpret them correctly (the intersection of a finite line and an ideal line could be an ideal point that corresponds to the direction of both lines).

We'll see specific instances of these throughout. The key point is: GA doesn't eliminate numerical issues—those are fundamental—but it often lets us detect and handle them more uniformly (the grade or magnitude of a result tells us a lot). Part of "engineering honesty" is openly cataloguing these pitfalls and ensuring our algorithms can detect when they're in a dangerous corner of the algebra.

### Chapter 4: Power Through Geometric Eyes

#### 4.1 Clarke and Park Transforms are Rotors

Earlier, in Section 0.2, we described the Clarke (abc→αβ0) and Park (αβ→dq) transforms in classical terms. Now let's describe them in GA terms explicitly. The Clarke transform for a three-phase system can be seen as a fixed rotation that takes the basis $(e_a, e_b, e_c)$ of phase coordinates into a new basis $(e_\alpha, e_\beta, e_0)$ where $e_\alpha, e_\beta$ are orthonormal in the plane of the three-phase system, and $e_0$ is orthogonal (capturing the zero-sequence component). One GA rotor that does this is:
$$R_{Clarke} = \frac{1}{\sqrt{3}}(1 + e_{31} + e_{12})$$
which indeed performs the rotation: it's constructed such that $R_{Clarke}(e_a)\tilde{R_{Clarke}} = e_\alpha$, $R_{Clarke}(e_b)\tilde{R_{Clarke}} = e_\beta$, and $R_{Clarke}(e_c)\tilde{R_{Clarke}} = e_\alpha + e_\beta - 2e_0$ (or some combination that yields the zero component appropriately). The exact form of the rotor isn't crucial to memorize; what matters is: Clarke's transform is a rotation in a 3D space (the space of three-phase quantities). In practice, we still often implement Clarke as a matrix multiply for efficiency, but knowing it's a rotor means we can consider it part of a single geometric operation when combined with Park.

The Park transform is even more clearly a rotor. It's a rotation in the αβ-plane by the negative of the system electrical angle θ (negative because we typically go to a rotating frame that cancels out the forward rotation of the system). In GA, that's
$$R_{Park}(\theta) = \exp\left(-\frac{\theta}{2}e_{12}\right)$$
which is a rotor that rotates any vector in the $e_{12}$ (αβ) plane by $-\theta$. Applied to $e_\alpha$ and $e_\beta$, it produces $e_d$ and $e_q$ axes that are fixed in a frame rotating with the machine. So the combined dq0 transform is:
$$R_{dq0}(\theta) = R_{Park}(\theta) R_{Clarke}$$
And for any three-phase voltage vector $\mathbf{v}_{abc}$, the transformed version in dq0 coordinates is
$$\mathbf{v}_{dq0} = R_{dq0}(\theta) \mathbf{v}_{abc} \tilde{R_{dq0}(\theta)}$$

Because $R_{dq0}$ is a rotor (indeed a proper time-varying rotor), this transform preserves magnitudes (aside from the scaling choices like $1/\sqrt{3}$ which we included for orthonormality) and preserves the geometric structure of signals. In GA language, Park's trick of making AC look like DC is literally aligning a rotating bivector with a fixed reference.

Why is this helpful? Conceptually, it means we can reason about reference frame changes as rotations. For example, a 30° phase shift in all three phases (like a delta-wye transformer between a generator and transmission line) can be treated as adding a fixed rotor before the Park transform. Or, an unbalanced system where the "Clarke rotor" is slightly different because the $abc$ sum isn't zero can be analyzed by adding a small component in the $e_0$ direction to the rotor. Montoya et al. have shown that imposing orthonormality via GA rotors yields Clarke and Park transforms naturally. We thus demystify these transforms: they were always geometric rotations; GA just makes that explicit.

*(Historical note: Edith Clarke and R.H. Park weren't thinking about GA, of course. They were doing linear algebra. But GA provides a unifying interpretation that enhances our intuition.)*

#### 4.2 Instantaneous Power as Geometric Product

We introduced the idea of geometric power $S = vi$ in Chapter 1. Let's deepen that in the three-phase context using our new rotor tools. Suppose we have three-phase instantaneous voltages and currents (could be balanced or not, sinusoidal or not). Form the 3D vectors $\mathbf{v}(t) = v_a(t)e_1 + v_b(t)e_2 + v_c(t)e_3$ and $\mathbf{i}(t) = i_a(t)e_1 + i_b(t)e_2 + i_c(t)e_3$. Then define
$$S(t) = \mathbf{v}(t) \mathbf{i}(t)$$
which is a multivector. What are its components?

- **Grade-0 (scalar) part:** $P(t) = \frac{1}{2}(\mathbf{v}\mathbf{i} + \mathbf{i}\mathbf{v}) = \mathbf{v} \cdot \mathbf{i}$. This is exactly the instantaneous total real power (sum of phase-by-phase products). Over a cycle, the average of $P(t)$ is the real power delivered to the load. If the system is balanced and purely fundamental, $P(t)$ will be constant in time (for a balanced load, each phase's oscillation in power cancels out in sum). In unbalanced or harmonic cases, $P(t)$ can oscillate at twice the fundamental or other frequencies, indicating real power oscillations. But $P(t)$ is a single scalar function representing all the "work being done" at each instant.

- **Grade-2 (bivector) part:** $\mathbf{Q}(t) = \frac{1}{2}(\mathbf{v}\mathbf{i} - \mathbf{i}\mathbf{v}) = \mathbf{v} \wedge \mathbf{i}$. In 3D, a bivector is dual to an axial vector (pseudovector) which here would align with $e_{123}$ (the axis perpendicular to the plane of $\mathbf{v}$ and $\mathbf{i}$). In a three-phase balanced system, $\mathbf{v}$ and $\mathbf{i}$ lie in the $e_1e_2e_3$ 2D subspace (since $v_a + v_b + v_c = 0$ for a balanced three-wire system, same for $i$), so the bivector is something like $Q e_{123}$, effectively a scalar $Q$ times the unit pseudovector $e_{123}$. That scalar $Q$ corresponds to the instantaneous reactive power flowing (with sign indicating direction of quadrature energy flow). If you average $Q(t)$ over a full cycle, you get the conventional reactive power (e.g., for sinusoids $Q = VI\sin\phi$). If $\mathbf{v}$ and $\mathbf{i}$ are not purely single-frequency, $Q(t)$ can have components at various frequencies too.

- **Grade-1 or Grade-3 parts:** In a balanced sinusoidal case, $\mathbf{v}$ and $\mathbf{i}$ lie in a 2D subspace, so $\mathbf{v}\mathbf{i}$ has only grades 0 and 2. But with distortion (harmonics) or imbalance, $\mathbf{v}$ and $\mathbf{i}$ might not lie in the same 2D subspace at each instant. This can introduce other grades. For example, if $\mathbf{v}$ has a third-harmonic component that $\mathbf{i}$ does not, then $\mathbf{v}\mathbf{i}$ could have a trivector (grade-3) part or even a grade-1 part indicating some coupling that is neither a pure scalar nor pure bivector. Those higher-grade parts correspond to more exotic power flows: e.g., a trivector part in 3D might indicate something trying to exist outside the space—numerical instability warning. (We'll see in Chapter 15 that 5th and 7th harmonic interactions can produce grade-4 components, hinting the model needs extension.) In classical power theory, these show up as terms that aren't accounted for by $P$ and $Q$, sometimes called distortion power or non-fundamental power. GA reveals them naturally as just the leftover grades. In fact, in an unbalanced or non-sinusoidal scenario, you could define geometric instantaneous power and then grade-project to get definitions of $P$, $Q$, and other components (this has been explored in recent literature).

The bottom line: using the geometric product to define power means we don't have to choose a reference frame or sequence components first. The multivector $S(t)$ encapsulates all instantaneous power information. If you rotate the reference frame (say using a different phase basis or using the $dq0$ frame), $S$ itself transforms nicely (as $S' = RSR$), and the scalar part $P$ and bivector magnitude $Q$ remain invariant – they are objective physical quantities. Only their coordinates (like how $Q$ is expressed as a bivector) might change. This frame-invariance of the power decomposition is a strong argument for GA: no more arguing about "Fryze power theory vs p-q theory vs IEEE 1459" – they are all seeing different slices of the same GA truth. We can recover all those with appropriate projections from $S(t)$, if needed.

#### 4.3 Harmonics Create Higher Grades

Let's push the harmonic distortion idea a bit further. Suppose $\mathbf{v}(t)$ has a 5th harmonic and $\mathbf{i}(t)$ has a 7th harmonic component. Write $\mathbf{v} = \mathbf{v}_1 + \mathbf{v}_5$ (fundamental + 5th) and $\mathbf{i} = \mathbf{i}_1 + \mathbf{i}_7$ (fundamental + 7th), ignoring others for now. Then expand the geometric product:
$$S = (\mathbf{v}_1 + \mathbf{v}_5)(\mathbf{i}_1 + \mathbf{i}_7) = \mathbf{v}_1\mathbf{i}_1 + \mathbf{v}_1\mathbf{i}_7 + \mathbf{v}_5\mathbf{i}_1 + \mathbf{v}_5\mathbf{i}_7$$

- The term $\mathbf{v}_1 \mathbf{i}_1$ is the normal fundamental power (with scalar and bivector parts at 0 Hz and $2 \times 60 = 120$ Hz oscillation respectively).

- $\mathbf{v}_5 \mathbf{i}_7$ is interesting: it involves 5th × 7th harmonic. In the time domain, that product will have components at $5+7=12$ times fundamental (720 Hz) and at $|5-7|=2$ times fundamental (120 Hz) due to trig identities. In GA, the 12th harmonic interaction might manifest as a higher-grade component because effectively $\mathbf{v}_5$ and $\mathbf{i}_7$ are in different time-phase planes. It could produce, say, a grade-4 part (in 3D, grade-4 is the pseudoscalar $e_{0123}$ direction) indicating an out-of-band coupling. The 2nd-harmonic (120 Hz) part could add to the same bivector grade as fundamental reactive power oscillations. So you get cross-frequency coupling terms.

- $\mathbf{v}_1 \mathbf{i}_7$ and $\mathbf{v}_5 \mathbf{i}_1$ similarly produce odd frequency mixtures.

The precise grade bookkeeping can get complicated, but qualitatively: the presence of multiple frequencies means $\mathbf{v}$ and $\mathbf{i}$ occupy a higher-dimensional space (time-frequency space), and their geometric product will accordingly have higher-grade components. In a 3D GA model, something that "doesn't fit" the 2D subspace of fundamental frequency will appear as a grade-3 trivector or grade-1 vector part of $S$. If we tried to interpret everything with complex numbers, we'd be lost because complex can only represent one frequency pair at a time. Instead, people resort to doing FFTs and computing power per harmonic, etc. GA instantaneous power gives a unified object that, when analyzed, reveals all those interactions in one framework. In practice, for measurement, we still might compute an FFT of $S(t)$'s components to categorize the oscillatory energy at different frequencies. But it's appealing that $S(t)$ contains not just "the" power, but all the inter-harmonic power flows as well, without additional definitions.

*(One could extend to a 6D GA to explicitly separate multiple frequency components, but that might be overkill. The 3D GA with time dependence usually suffices by treating time like an external parameter. Some research is exploring multi-frequency GA models for power analysis, though.)*

---

## Part II: Computational Reality

### Chapter 5: The Densification Catastrophe

#### 5.1 How Sparse Becomes Dense

In theory, GA looks like it might be efficient. Many geometric operations are just additions and multiplications of a few components. But a lurking issue is grade proliferation: as you multiply GA elements, even if each started sparse (only a few grade components present), the result tends to fill out more and more components. This is densification.

Consider a concrete example. Start with two 3D vectors $\mathbf{v} = v_1e_1 + v_2e_2 + v_3e_3$ and $\mathbf{w} = w_1e_1 + w_2e_2 + w_3e_3$. Each is "sparse" in the sense of only having grade-1 components (3 each, out of 16 possible GA components in 3D PGA, which includes grade-0 through grade-4). Now compute the geometric product $\mathbf{v}\mathbf{w} = \mathbf{v} \cdot \mathbf{w} + \mathbf{v} \wedge \mathbf{w}$. The dot $\mathbf{v} \cdot \mathbf{w}$ is a scalar (grade 0), and $\mathbf{v} \wedge \mathbf{w}$ is a bivector (grade 2). So $\mathbf{v}\mathbf{w}$ has grade 0 and grade 2 parts. That's already a bit "denser" (more grades) than either original vector. Now multiply this result by another vector $\mathbf{u}$. We do $(\mathbf{v}\mathbf{w})\mathbf{u}$. Distribute: $\mathbf{v}\mathbf{w}\mathbf{u} = (\mathbf{v} \cdot \mathbf{w})\mathbf{u} + (\mathbf{v} \wedge \mathbf{w})\mathbf{u}$. The first term $(\mathbf{v} \cdot \mathbf{w})\mathbf{u}$ is just a scalar times a vector, which yields a vector (grade 1). The second term $(\mathbf{v} \wedge \mathbf{w})\mathbf{u}$: here we're multiplying a grade-2 by a grade-1. The geometric product of a bivector and a vector yields grade-1 plus grade-3 (because $2 + 1$ can be 3 via the wedge part, and $2 - 1 = 1$ via the dot part when reversing order). So $(\mathbf{v} \wedge \mathbf{w})\mathbf{u}$ will have a vector part and a trivector part. By the end of this, $\mathbf{v}\mathbf{w}\mathbf{u}$ has components in grade 1 and grade 3 (from the second term) and possibly additional grade 1 from the first term, so collectively grade 1 and 3. Now if we multiply by one more vector, almost certainly we'll fill in grade-2 as well. After enough multiplications, you approach a full multivector (all grades populated). In $Cl(3,0)$ or PGA(3,0,1), that means up to 16 non-zero components.

In big-O terms, if you multiply $n$ vectors in an $m$-dimensional GA, the result can have up to $\sum_{k=0}^m \binom{m}{k} = 2^m$ components (the dimension of the algebra). Often by $n \approx m$, you're nearly full. For 3D, $2^4 = 16$ is max components (since PGA has a 4-dimensional basis including $e_0$). The multiplication algorithm itself in a naive form is $O(2^{2m})$ – like $O(256)$ ~ constant for 3D, but the constant can be large in higher dims. Practically, as soon as things get slightly complicated, your nice sparse representations blow up. The example above:

- **Depth 1** (one multiplication): 2 grades out of 16 in result (~25% of components possibly non-zero).
- **Depth 2** (two multiplications in a row): maybe ~50% filled.
- **Depth 3:** ~75% filled.
- **By depth 4 or 5:** 90–95% filled.

Beyond a certain point, virtually every basis element can appear. When that happens, multiplying two multivectors costs nearly $2^m \times 2^m$ operations (like multiplying two dense matrices of half the size). In 3D, that's up to 16×16 -> 256 terms (though many combine or cancel due to structure). In 4D, 32×32->1024 terms. This is why GA libraries often precompute and hard-code specific products to reduce overhead: the general formula might do a lot of unnecessary zero multiplications if the multivectors are sparse, but if they are actually dense, you hit the worst-case cost.

For the power engineer or programmer, this means: If you try to naively use GA for a long chain of computations (like repeatedly updating a state by geometric products or performing iterative solves), the cost per operation tends to rise as intermediate results become more complex. It's not as simple as each step being a fixed cost unless you can keep things sparse. This is one reason we are cautious about using GA in inner loops of real-time algorithms.

#### 5.2 Memory and Cache Violence

Computational complexity isn't the only issue; memory access patterns matter a lot on modern hardware. A 3D PGA multivector has 16 components. In double precision, that's 128 bytes. Most CPU cache lines are 64 bytes. So a single multivector doesn't even fit in one cache line; it straddles two. If you're streaming through arrays of multivectors, you'll often fetch two cache lines for one multivector, effectively doubling the memory bandwidth needed. Contrast this with a typical 3D vector (3 components, 24 bytes) which sits well within one cache line (and you can often pack several such vectors into one line).

Now consider multiplying two multivectors. The typical formula will take each component of one and multiply by each component of the other with a certain sign, summing into result slots. This leads to a lot of non-contiguous memory access:

- You read component $a_i$ of multivector A (somewhere in those 128 bytes).
- You read component $b_j$ of B.
- You write to component $c_k$ of result C.

The indices $i, j, k$ follow the pattern of the multiplication table for the algebra, which is not simply sequential. It might be something like: to compute result's $e_{13}$ component, you need A's $e_1$ and B's $e_3$ and subtract A's $e_3$ × B's $e_1$ etc. The effect is that you are jumping around in the arrays of components. There is little spatial locality (whereas a matrix multiply can stride nicely through memory, especially if optimized). The GA multiply resembles a convolution of two vectors of length 16 under a specific pattern – not as nice as a dot product or so.

Empirical data (from tests on GA libraries) show cache miss rates can be high. Suppose you have an algorithm that multiplies many multivectors in sequence. If each multiply has to fetch two 128-byte multivectors and produce one 128-byte result, that's 384 bytes touched, and likely a lot of those bytes are in different cache lines if data isn't aligned or if it's interleaved. If the access is random or striding badly, caches can't prefetch effectively. Compare: a dense matrix multiply can stream through memory with near 100% cache utilization (accessing contiguous blocks). GA multiplies often jump per component. One estimate is that while an optimized dense linear algebra operation might have cache miss rates around 2–5%, a naive GA multiply could see 30%+ cache misses. That destroys performance, because each miss stalls the CPU waiting for main memory (which is ~100× slower than registers or L1 cache).

To quantify, consider: a modern CPU might do, say, 10 GFLOPs on a tight loop that fits in L1. But if each operation triggers a memory fetch, you might drop to 0.5 GFLOPs equivalent throughput. That's not a small drop—that's an order of magnitude. When the user earlier said a 17× difference in cache behavior, it's this kind of effect. Indeed, if a GA multiply effectively causes 17 times more cache misses than a comparable conventional algorithm, the observed performance can be an order of magnitude worse than expected from pure FLOP counts.

The lesson is that data layout and access patterns are crucial for GA. We might need to reorganize multivector data in memory to be cache-friendly (e.g., structure of arrays vs array of structures, or blocking, or compressing out zeros, etc.). Some GA libraries use contiguous arrays of components (size 16) which is fine, but as noted 16 is awkward for cache. One idea is to split into two 8-component halves (like separate arrays for even and odd grade parts), so each fits in 64 bytes. This matches how some GPU implementations pack motors as two 4D float4 vectors. We'll discuss mitigation in next section.

#### 5.3 Mitigation That Almost Works

Engineers have tried various tricks to tame the densification and memory issues of GA:

- **Blade-sparse representations:** If you know your multivectors will only contain certain grades, you can store only those. For example, rotors are grade-0+2 only (4 components in 3D), motors are grade-0+2+4 (8 components). Storing only those saves space and multiplies faster (like multiplying dual quaternions instead of full 4×4 matrices). The problem is when an operation produces an out-of-type result. For instance, multiplying a rotor by a general line (a bivector) yields a result that might not be a pure rotor or pure line—it could be a sum of different grade parts. If your system can guarantee certain types (closed under operations), blade-sparse storage works well. But in a general computation, you eventually have to promote to the full representation. Still, in specific domains this is powerful (e.g., in computer graphics, many algorithms only use motors, lines, points; Klein library exploits this heavily).

- **Factorized forms:** Sometimes you can keep an object as a product of simpler objects rather than expanding it. For instance, instead of storing a motor as 8 numbers, store it as a rotor (4 numbers) and a translation vector (3 numbers). Or store a rotor as two unit quaternions if you had them (just conceptually). Then you can compose by composing the factors (e.g., multiply rotors and add translations with rotation). This can save operations because you're not constantly recomputing all 8 components if you only need final results. The downside is that some operations become more complex (applying a motor factorized may require applying rotor then translation separately, which might not vectorize as nicely as one fused operation). Also not all objects factor neatly, and numerical stability of factors can be an issue (you might need re-orthonormalize the rotor factor occasionally).

- **Code generation (symbolic optimization):** Tools like GAALOP analyze a given GA expression pipeline and generate optimized C code that only computes the needed components in the minimal way. If your computation is known ahead of time (e.g., a specific formula for the intersection of two lines and then applying a rotor), the generator can simplify and remove redundancies. This often yields very efficient code for that specific task. The catch is if your problem changes or you need a general solver, you can't easily maintain a huge library of custom-generated kernels for every scenario. It's great for fixed algorithms (you see this in some GA robotics papers: they auto-generate the kinematics code).

- **Lazy evaluation:** This is more theoretical, but one could imagine deferring the expansion of a multivector until you need certain parts. For example, if an upcoming operation only needs the scalar part of a result, you don't compute the bivector part fully. You keep an expression tree and evaluate on-demand. This can save work in some cases (like symbolic algebra systems do). However, implementing this at runtime in C++ or similar is extremely complex and would probably lose any performance gains to overhead unless carefully done.

- **Mixed representation:** Another approach: use GA for geometric computations and clarity, but under the hood convert certain operations to matrix or vector operations when they would be cheaper. For example, rotating a point by a motor can be done by the sandwich formula or by converting the motor to a 4×4 matrix and doing a matrix multiply. On a GPU, that matrix multiply might actually be faster (using tensor cores) than doing the GA formula. A smart library could detect "ah, this is just a rigid body transform" and use a faster path. This hybrid approach means you need parallel implementations and a decision mechanism, which complicates things but might yield performance.

To be frank, none of these fully "solve" the issue in general. They each address specific cases. The densification problem is fundamental if your problem inherently mixes lots of grades. The memory problem is fundamental if you don't have a structure to exploit (like banded structure in matrices, or block sparsity). We'll see in Part III how choosing a hybrid architecture can dodge some of these issues by not using GA for the heavy-lifting numerics at runtime.

### Chapter 6: Hardware Says No

#### 6.1 GPU Tensor Cores Were Not Made For This

The rise of GPUs for general-purpose computing (GPGPU) has been a huge boon for linear algebra (matrix multiplications, etc.). Modern GPUs, especially from NVIDIA, have special units called Tensor Cores that can multiply small matrices (like 4×4, 8×8 blocks) extremely fast in mixed precision. If we could map GA operations onto these, we'd get a big speedup. Unfortunately, GA doesn't map nicely to fixed-size dense matrix multiplies.

For example, an 8-component motor multiplication could be considered a $8 \times 8$ matrix times an 8-vector (if we linearize one operand), but the matrix has a specific sparse pattern (from the multiplication table). It's not a dense 8×8 matrix with arbitrary values, so using a generic 8×8 multiply would do a lot of wasted work (multiplying zeros). Also, tensor cores often operate on 16-bit or 32-bit floats and require specific memory layouts (like row-major FP16 matrices packed). GA operations aren't conveniently expressible as a sequence of such fixed-size multiplies without rearranging data every time.

One attempt: pack a multivector into a matrix form so that geometric product corresponds to matrix multiply. It can be done (Clifford algebras can be represented by matrices, albeit larger ones: $Cl(3,0,1)$ can be represented with 4×4 matrices of dual quaternions perhaps). But typically it's not efficient because the matrix representation is not minimal (you might need a 4×4 to represent an 8-component motor, which is 16 components). We found that trying to use 4×4 matrix multiplies for motors ends up doing a lot of extra FLOPs that aren't needed, and also requires converting in and out of that matrix form.

Another angle: maybe break the GA multiply into multiple MMA (matrix multiply accumulate) operations that tensor cores can accelerate. For instance, maybe treat it like two 4×4 operations for different grades. Some researchers have tried "structure tuning" to exploit GPU instructions, but the warp divergence (threads doing different things for different components) and register pressure (since intermediate results don't nicely fit in registers) often kills performance.

The net result has been disappointing: one might achieve only a small fraction of peak FLOPs of a GPU for a general GA operation. A hypothetical number: if a GPU can do 1200 GFLOPs on 4×4 matrices, it might only get ~30 GFLOPs doing GA stuff, a 40× difference as noted in the outline. Real experiments, like Steven De Keninck's GA GPU project, show good speedups relative to CPU but still far from saturating the hardware's capabilities. The hardware is just highly optimized for different math.

In summary, GPUs are great for large dense linear algebra where data reuse and regularity are high. GA operations are small element vectors with a custom multiplication). So we either treat them as small matrix ops (underutilizing the multipliers) or we write custom GPU kernels that do lots of scalar ops (which underutilizes memory bandwidth or warp parallelism). Neither hits the sweet spot of GPUs. We'll likely need GA-specific hardware (or at least libraries) to ever get close to peak performance.

#### 6.2 SIMD Struggles With Mixed Grades

On CPUs, vectorization (SIMD instructions like SSE, AVX, AVX-512) is key for performance. If you can do 8 multiplications in parallel with AVX-512, you get 8× throughput if you have independent data. But GA operations are hard to vectorize because the computation on each component can differ. For instance, adding two multivectors is trivially vectorizable (just 16 parallel adds). But multiplying them: each output component is a sum of specific pairs of input components. If you try to line that up in SIMD lanes, you often find you need shuffling and masking.

Example: Suppose we have an AVX register that can hold [a₁, a₂, a₃, a₄, a₅, a₆, a₇, a₈] (eight doubles). And another for b's. If we were multiplying, say, two rotors, we might do something akin to a quaternion multiply (which is nicely vectorizable by some tricks). But for general 16-component multiplies, you might have to do 16 partial dot products. There are techniques – use permutation instructions to align pieces – but it gets messy. And if you branch on grade cases (like "if grade(X) == 2 then do special thing" at runtime), that breaks SIMD completely.

One could unroll and manually vectorize: e.g., use one AVX-512 register to hold 8 intermediate results and accumulate, etc. Some libraries do this. Klein, for instance, used SSE/AVX intrinsics to speed up certain operations. The performance was quite good for those specific operations (rotor composition was as fast as the best quaternion code). But again, that's possible because a rotor×motor product has structure one can exploit. A general multivector×multivector multiply implemented with SIMD will waste a lot of lanes on zero operations or need many shuffle instructions, reducing efficiency.

Another source of inefficiency: branching on grade. If code checks "if grade(X) == 2 then do special thing" at runtime, you lose the ability to pipeline and vectorize well. Ideally, you'd template or generate separate routines for each case and avoid branches. That's what template-heavy GA libraries (like some C++ ones) do: they create different functions for different operand types (rotor by vector, line by motor, etc.). This yields fast code for each case, but at the cost of code bloat and less generality.

In our experience, a straightforward SIMD implementation of full multivector multiplication might utilize only ~10–20% of the SIMD's potential throughput. Compare that to, say, vector dot products or matrix multiplies which can hit 80–90% of peak. This matters for embedded and desktop CPUs where those wasted cycles mean higher power draw and possibly not meeting timing (imagine an ISR where you budget 100 CPU cycles but your GA operation takes 500 because it didn't vectorize well).

#### 6.3 Embedded Systems Cannot Afford This

Finally, consider the extreme end: small microcontrollers (like ARM Cortex-M series running at a few hundred MHz, often without an FPU, controlling say an appliance motor or a drone). These have tight power and performance constraints. Let's say a typical field-oriented control (FOC) algorithm on a microcontroller does:

- Clarke transform (3 phase to αβ) – a few adds and multiplies.
- Park transform (αβ to dq) – a couple of multiplies, using sin/cos from a lookup maybe.
- PID on d and q – a few operations.
- Inverse Park and inverse Clarke – similarly small.

All this can easily fit in, say, 50 microseconds on a 100 MHz microcontroller (these operations maybe take a few microseconds at most).

Now if we tried to do this with GA:

- Represent the Clarke transform as a rotor (we can pre-compute it, fine).
- Represent the Park rotation by a rotor $R(\theta)$.
- Do $\mathbf{v}_{dq0} = R(\theta) (C \mathbf{v}_{abc} \tilde{C}) \tilde{R(\theta)}$ in one go. Even if optimized, this might be, say, 100 multiplications and additions (the GA sandwich cost).
- The microcontroller possibly doesn't have hardware multiply for floats (some do single-precision, but many small ones would have to do software double precision if double is used).
- Suddenly, that one operation could take, who knows, 500 cycles or more. If it's done every 50 µs, that's 10,000 operations per second, times 500 cycles = 5 million cycles per second, which might be half the CPU time on a 100 MHz device. Versus the traditional approach maybe taking 0.5 million cycles per second (an order of magnitude less).

Also memory: storing a 16-component multivector might be okay, but if you allocate a lot of them, you quickly use up tens of KB of RAM. Many small micros have only 32 KB or 64 KB of RAM total. GA encourages making more objects (points, lines, etc.) which can eat memory fast if not careful.

Power consumption: On battery-powered devices, every extra cycle is energy drawn. A GA-heavy algorithm that uses 5× more instructions will drain battery faster and also produce more heat (which in some precision analog contexts can even affect readings).

In short, without specialized support, GA is a tough sell in microcontroller and other embedded contexts. Those environments demand simplicity and efficiency. Unless GA gives a huge benefit (like eliminating a sensor or making the control more robust to something), engineers won't trade away microseconds of interrupt time for it. So one theme of this book is identifying when GA is worth it despite the costs, and when it's not. In real-time control of power converters or motor drives (where 100 µs loops are common), a pure GA implementation of the control law is likely not worth it. We'd rather design with GA, but implement with traditional math for speed (the hybrid approach).

### Chapter 7: The Probabilistic Wall

#### 7.1 Why Gaussians and Null Vectors Don't Mix

Geometric algebra shines at deterministic geometric computations, but when we introduce probability (especially Gaussian assumptions common in estimation), we hit a conceptual wall. The issue is best illustrated by trying to define a Gaussian distribution over GA entities like rotors or motors. In a conventional Euclidean vector space (like $\mathbb{R}^n$), a Gaussian is defined by a mean vector and a covariance matrix, assuming an underlying volume measure (Lebesgue measure). In GA, especially PGA, our entities lie on manifolds or even algebraic varieties (due to constraints like $R\tilde{R} = 1$ for rotors, or $P^2 = 0$ for points at infinity).

Take PGA points: a point $P = xe_{032} + ye_{013} + ze_{021} + e_{123}$ satisfies $P \cdot I^{-1} = 1$ if it's normalized (meaning $w = 1$ for finite points). The set of all such normalized points is not a linear space; it's like a 3D projective space plus a "plane at infinity." The points at infinity ($w = 0$) are on the boundary of that space. It's not an open set in a Euclidean sense, so defining a continuous density on it (like a Gaussian) is tricky.

One might attempt: "Let's treat the coordinates $(x,y,z,w)$ in homogeneous form as a 4D vector and put a Gaussian on that, constrained by something." But a Gaussian in 4D is not naturally invariant to the projective scaling (if we scale $(x,y,z,w)$ by 2, it's the same point, but a Gaussian density wouldn't treat those as identical).

Specifically for null vectors: In PGA, points at infinity are null vectors ($P^2 = 0$ and $w = 0$). The set of null vectors (excluding 0) is a cone in the vector space. You can't restrict a Gaussian to the null cone nicely because the null cone is not an affine subspace, it's a quadratic manifold (cone) that lacks a well-behaved volume form (it's not closed, it extends to infinity, etc.). Any Gaussian in the 4D homogeneous space will either assign zero probability to null vectors (if you exclude them properly) or include them but then it's not normalizable (the cone extends infinitely).

In more practical terms: say you want to do an Unscented Kalman Filter (UKF) on the pose of a robot, using motors in PGA as the state representation. The UKF will assume you can take a mean and covariance of states and do linear transformations. But a motor lives on the dual quaternion group (essentially a subset of $\mathbb{R}^8$ constrained by 2 quadratic equations: unit norm and some parity maybe). How do you average motors? Not by simple vector addition. So you have to do something like: $M = \text{mean}(M_i)$ defined in some geometric way (maybe via logarithms). And covariance? You can push motors to their algebra (log space) which is a vector space of bivectors (6-dim for SE(3)). That might work: define a Gaussian in the Lie algebra of the group, then map through exponential to the group. This is a common trick (see e.g. "Lie algebra EKF").

However, this no longer is a Gaussian on the original GA representation; it's a Gaussian in a different space that you map over. It works but you have to manage the domain carefully (e.g., handling wrapping of rotations beyond $2\pi$ etc.).

When the prompt said "no invariant measure on null cone," it points to a deeper mathematical fact: for projective spaces or homogeneous spaces, the notion of uniform or Gaussian distribution isn't straightforward. In SE(3), you have Haar measure (uniform over the group) which you can use to define uniform random rotations and translations. But Gaussian-like (which concentrate around a point) need some metric; often we use the Lie algebra with an Euclidean metric for that.

The bottom line: if you hoped to do something like "just extend my Kalman filter to operate on GA objects directly by treating their components as state vector," you're in for unpleasant surprises. It might work for small covariance (such that you rarely stray into weird parts of the manifold), but it's not theoretically sound globally.

#### 7.2 What Actually Works: Hybrid Architecture

Since GA doesn't naturally handle probability densities in a straightforward way, the practical solution is a hybrid approach for estimation and filtering:

1. **Use GA to model the geometry** of the problem and compute things like residuals or errors in a coordinate-free way.

2. **When it comes time to do a probabilistic update** (like a Kalman filter measurement update or process noise addition), convert the GA objects to a conventional representation (like a minimal manifold coordinates) where probabilities are well-defined.

3. **Perform the probabilistic step** (Gaussian update, etc.) in that conventional space.

4. **Convert back to GA** to continue geometric computation.

For example, in robot pose estimation:

- **Represent poses as motors** in PGA for convenience in composing motions and taking differences.

- **But maintain an estimate of pose** in SE(3) form (rotation matrix + translation vector, or quaternion + translation).

- **Use GA to compute the innovation:** say the difference between an observed point and a predicted point, by doing a meet or something if that's easier.

- **But then express that innovation** as a 3D vector or 6D twist that the Kalman filter can handle.

- **Update the pose estimate** via the Kalman gain in that conventional representation (ensuring it remains on SE(3) by using appropriate re-parametrization if necessary).

- **Finally, map that updated pose back** to a GA motor for use in the next geometric computation.

This approach leverages GA where it adds clarity (error computations, constraints) and uses classical methods where they're strongest (filtering, least squares on Euclidean parameters). It does introduce overhead: conversions back and forth. But if your filter runs at, say, 100 Hz and your geometry computations run at 1 kHz, this overhead is manageable. In fact, it's often the case that we can afford a moderately expensive step occasionally (like every 10 ms) if it buys us simpler code or better correctness, while the high-rate inner loop must be lean.

We must be careful at boundaries: each time we convert a GA object (like a motor) to an SE(3) representation (like a quaternion + translation vector), we should re-orthonormalize if needed (to purge any numerical drift). We should also check if any constraints were violated (like if a point that should lie on a plane now is slightly off due to filter noise – maybe clamp it or adjust the covariance to reflect the constraint). These conversions and corrections essentially ensure that the "math honesty" is maintained across the boundary of GA ↔ conventional.

A concrete example from power systems might be state estimation in an unbalanced system. We could use GA to formulate the equations relating measurements to states (like line meets, etc.), but when doing the actual numerical solve (like a weighted least squares or an EKF), we'd convert those equations to a Jacobian matrix and solve in conventional numeric form. GA helps derive the equations, but the solving happens with linear algebra.

This hybrid philosophy appears again in Part III where we outline an architecture to use GA in design but not necessarily in final high-speed execution.

### Chapter 8: Debugging Without Tools

#### 8.1 The Visualization Problem

One underrated difficulty in adopting new mathematical frameworks is debugging and visualization. With regular 3D vectors, we have an intuitive geometric picture (arrows in space), and every graphics engine can draw them. With GA multivectors, especially in PGA, what does an object "look like"? A plane (grade-1) can be visualized as an actual plane in 3D; a line (grade-2) can be drawn as a line segment or arrow in space. But what about a general motor (rotation+translation)? That's a screw motion – one can draw an axis and angle maybe, but it's not built-in to tools. A bivector (grade-2 in 3D) can be thought of as an oriented plane, perhaps depicted by a circular disc or an axis vector (normal) with a magnitude (area). A trivector (grade-3) in PGA is a point – we can draw that as a point in space. A quadvector (grade-4) is like a volume element – in 3D PGA there's only one basis quadvector $e_{0123}$ which we usually consider the pseudoscalar; a multiple of it might represent "empty intersection" scaled by a factor (not really visualizable except as "no intersection").

Now consider a general multivector like $A = 3 + 2e_{12} - 0.5e_{013} + ...$ some combination. It doesn't correspond to a single simple geometric object; it's like a mash-up. You can try to break it into parts: 3 (scalar) you can't visualize, $2e_{12}$ is a plane of some sort, $-0.5 e_{013}$ is something like a line at infinity in a certain direction, etc. The human mind struggles beyond 3 dimensions.

There have been attempts at visualization:

- **Grade histogram/bar chart:** Show how much magnitude is in grade-0,1,2,3,4 parts of a multivector. This gives a sense if it's mostly a vector or mostly a bivector, etc., but not very geometric.

- **Bivector disk or oriented circle:** Represent a bivector by a disk oriented in 3D with radius proportional to magnitude, maybe an arrow on its circumference showing orientation (like direction of rotation).

- **Screw (motor) depiction:** A motor can be visualized by its screw axis (a line in space), with a rotation angle and translation distance (pitch). You might draw a corkscrew or an axis with a curved arrow around it and an arrow along it. There are tools that compute log of a motor to get axis and pitch, then draw that. That's quite useful for robotics interpretation (the exponential coordinates).

- **For points and lines in PGA:** one can easily compute their Euclidean coordinates (like a point trivector can be converted to $(x,y,z)$ by dividing by $w$). Lines via Plücker coordinates can be drawn as a line in space (if not ideal). Planes given by $ax + by + cz + d = 0$ can be drawn as a shaded plane.

However, none of this is standard in mainstream software. If you want to see these, you have to either write custom code or use specialized GA visualization apps (some exist in research, but not widespread). For a developer debugging algorithm, printing a multivector's components is not intuitive – it's just numbers. With a vector, you print (x,y,z) and you can picture it. With a multivector, you see [3, 0, 0, 0, -0.5, ..., etc] 16 numbers. Without a mental translation, it's hard to know what that means.

This lack of immediate visualization slows down debugging and understanding. It's one reason new GA users often struggle; they can't "see" what's going wrong when something is off.

#### 8.2 Print Debugging Everything

Because of the visualization challenge, many GA practitioners (especially in engineering contexts without fancy math visualizers) resort to heavy use of printouts and log statements – essentially, print debugging. This is old-school but sometimes the only way. You might instrument your code to dump intermediate multivectors with labels:

```c++
print("Motor M = ", M.scalar(), M.e01(), M.e02(), ..., M.e0123());
```

and similarly for results after applying a motor:

```c++
print("Point P after transform = ", P.scalar(), P.e032(), P.e013(), ...);
```

You then manually interpret those. Perhaps you check that $M\tilde{M}$ components are nearly [1,0,0,...0] (indicating a unit motor). Or that the resulting point's $e_{123}$ component is 1 (normalized).

This kind of debugging is tedious and prone to mistakes (maybe you thought $e_{023}$ was something but misremember sign conventions). It also clutters the code and can introduce performance issues if left enabled.

Why not use an interactive debugger? Because typical debuggers don't understand GA types; they'll show them as struct with 16 doubles, which is same as printing basically. You'd have to mentally map indices to basis elements.

Better would be to have library support for printing in a nice format, like "P = (x,y,z)" for points, "L = (direction d, moment m)" for lines, "M = rotation 30° about axis (ax) + translation 5 along axis" for motors. Some libraries do provide to-string like that. If yours does, use it! It bridges the gap somewhat.

#### 8.3 Invariant Checking Overhead

One technique to debug complex GA algorithms is to frequently check invariants. GA has many useful identities that should hold if your code is correct:

- **If $R$ is supposed to be a rotor,** check $R\tilde{R} \approx 1$ (within tolerance). If not, you might have a bug or accumulating error.

- **If you believe some quantity is a pure grade-$k$ object,** check that the other grade parts are near zero (for example, after computing a meet of two lines, ensure the result is a point trivector with negligible grade-4 component indicating it's not an empty intersection, or if it is empty, check that the magnitude of the grade-4 part makes sense).

- **Check dualities:** if $X$ is the meet of A and B, verify $X^* = A^* \wedge B^*$ explicitly (they should match up to numerical eps). This catches mistakes in how you computed something.

- **Conservation checks:** in power applications, maybe check that the input power multivector equals output power multivector in a lossless scenario (ensuring no mysterious extra energy appeared).

- **Symmetry checks:** for example, if you swap two lines and meet, you should get the same intersection point (order shouldn't matter other than sign which dual removes). So $L_1 \wedge L_2$ vs $L_2 \wedge L_1$ maybe sign differences only.

In code, you might write functions like `assertRotor(R)` which does `assert( (R*reverse(R)).scalar() == Approx(1.0) )` and that all bivector components of `R*~R` are ~0 etc. Or `assertParallel(line1, line2)` if expecting them parallel by checking if their meet is ideal.

The downside: these checks cost time. If you do a lot, you could slow things by 15–20% or more. In a debug build that's fine; in release you'd disable them. But sometimes you can't disable because the bug only appears occasionally in a long run. Then you either pay the overhead or set up logging triggers.

For GA, given how non-intuitive it can be, we strongly advocate using such invariant checks during development. They saved our bacon numerous times, catching subtle errors like forgetting a reverse in a sandwich or mixing up normalization.

However, the prompt notes this overhead too – indeed sometimes we left checks in and lost performance, thinking they weren't too bad. For instance, normalizing a rotor every time in a loop might cut speed significantly (though we needed it every so often, maybe not each iteration). Or checking $I = e_{1230}$ remained constant – trivial but if done in inner loop, pointless overhead.

So the strategy: keep checks in debug/test modes; remove or throttle them in final code. Or do statistical sampling of invariants: e.g., check every 1000th iteration rather than every time (this can catch drift without too much cost).

In short, debugging GA-heavy code requires diligence. Without good interactive tools, you rely on prints and checks. It's a tax on development that one should budget for. It's one reason why an engineering team might shy from GA: the dev and debug cycle is longer or riskier. Part of our mission is to document these needs so that if someone embarks on GA integration, they do so with eyes open about the extra work needed around tooling and testing.

### Chapter 9: Benchmarking Truth

#### 9.1 Klein's Story — Performance Wasn't Enough

To drive home the points of this Part, let's recount a real attempt to make GA practical: Klein, a C++ library for 3D PGA by J. Ong et al. Klein is perhaps the fastest GA implementation for 3D rigid body stuff to date. It focused on exactly the approach we've discussed: highly optimized operations for specific types (rotors, motors, etc.) using SIMD, beating or matching the best conventional libraries. For example, Klein's rotor (a quaternion under the hood) multiply was as fast as Eigen's quaternion multiply, and its compose and apply of motors beat or matched hand-optimized SE(3) code. It achieved this by sacrificing generality (it's only for $Cl(3,0,1)$, not arbitrary GA) and by writing a lot of low-level code.

However, despite this technical success, Klein's repository was eventually archived and it hasn't seen widespread adoption in industry. Why? A few reasons:

- **Ecosystem integration:** Most graphics and simulation engines already have their math types (vectors, matrices, quaternions) and adding Klein would mean converting back and forth or replacing large parts of the engine. That's a high barrier. There was no drop-in integration with, say, Unity or Unreal engines.

- **Debugging and tooling:** Klein provided the operations but not necessarily easier debugging. A developer might still prefer to reason with x,y,z coordinates and quaternions rather than new GA types if they don't have good tooling to visualize and understand what's happening. So beyond performance, the developer experience matters.

- **Convention mismatches:** If one part of your system uses GA with certain conventions (e.g., coordinate system or handedness) and the rest uses something else, you risk subtle bugs. Without many users, Klein didn't get tested in all possible scenarios to iron out such issues. A single maintainer can only do so much.

- **Learning curve and maintainability:** Many potential contributors or maintainers might have been scared off by the heavy use of template metaprogramming and intrinsics. If the original authors step back (burnout or other priorities), who will maintain? That happened – once Jeremy Ong moved on, there were few others with the expertise and motivation to keep updating it. The project stagnated despite its performance merits.

The lesson from Klein is sobering: Even if GA code runs blazingly fast, it won't catch on unless it plays well with existing systems, is easy to debug, and has community support. Performance alone was not sufficient. We, as a community, have to address the "soft" issues (documentation, integration, pedagogy) as much as the "hard" math and ops issues to achieve adoption.

#### 9.2 Real Measurements

Let's look at some concrete benchmark comparisons relevant to power engineering tasks, to quantify the overhead of GA vs conventional methods. These are illustrative (exact numbers depend on hardware, implementation quality, etc.), but they show the ballpark.

**Three-phase power analysis (harmonics):** Imagine we have 1000 samples of three-phase voltage and current, and we want to compute the active and reactive power of the fundamental and up to the 50th harmonic (i.e., do a frequency domain analysis).

- *Conventional approach:* Compute FFT of each phase (3 FFTs of length 1000), then for each harmonic bin do $P + jQ = V_h I_h^*$ etc. This might take around 2–3 milliseconds in C on a modern CPU (mostly FFT time).

- *GA approach:* Form 3D vector signals $\mathbf{v}(t), \mathbf{i}(t)$ for each sample, multiply to get $S(t)$ at each time, then perhaps project out components by correlating with sinusoids or doing an FFT on $S(t)$'s components. The GA instantaneous product at each time is 16 components × 1000 samples = 16000 component-wise multiplications plus adds. That's minor. But interpreting the result requires either doing an FFT on multiple components or some additional math. Let's say it ends up ~18–20 milliseconds. So maybe ~8× slower. Why? Because the GA method is doing more work (computing interactions that in conventional method are implicitly handled by focusing on one harmonic at a time) and maybe not leveraging optimized FFT libraries as directly.

- *The result is the same* (you'd extract P and Q for each harmonic, etc.), but GA gave more detail (like inter-harmonic coupling if any, which conventional approach might miss because it assumes independence of harmonics). If you didn't need that extra detail, you wasted time.

**Fault detection (line intersection):** Suppose a fault is characterized by the intersection of an overhead line and ground plane sensor measurements that give us line equations in space (maybe via state estimation) and we want to compute the fault point.

- *Conventional:* Solve two line equations and one plane equation via linear algebra (3 equations in 3 unknowns). That's maybe 20 arithmetic ops; with overhead, call it 140 microseconds in a high-level language if not optimized, or much less in optimized C.

- *GA:* Represent lines as $L_1, L_2$ (bivectors), plane as $\Pi$ (vector), compute point $P = (L_1^* \wedge L_2^* \wedge \Pi^*)^*$ (meet of three objects). That's several wedge and dual operations, each with tens of ops, overall maybe a few hundred ops. Let's say 890 microseconds. So ~6× slower.

- *However,* the GA method is more uniform: the same code would handle if the lines were not vertical, or if we had different combinations (line-to-line, line-plane, plane-plane, etc.) just by changing inputs. The conventional might have to choose different solving routines for different fault types (line-ground vs line-line). GA was one formula for all. So GA shines in generality and uniformity, but you pay a constant factor in speed. For protection relays, 890 µs might be borderline but okay for a decision (since relays often have tens of ms to trip). But 6× slower means maybe fewer points can be processed or less filtering can be done in the same time.

**Space-vector modulation (per switching cycle):** Suppose in a motor drive control loop (50 µs cycle) we need to compute the SVM timing for the inverter.

- *Traditional:* Determine sector by comparisons of reference vector components, then compute two duty fractions by simple formulas (involving multiplications of sine values or lookups). Perhaps 10–20 µs total, using maybe 20–30 math ops at most.

- *GA:* Represent the eight inverter switching states as vectors in αβ plane, identify which three (two active + zero) form a triangle containing the reference vector (done by wedge and sign checks), then compute duty ratios via area ratios (bivector wedge formulas). Conceptually straightforward, but might involve a dozen wedge and mult operations (~100+ ops). Perhaps 340 µs as earlier noted (28× slower). That busts the 50 µs budget by a factor of 7.

- *So GA is too slow here.* Why? The traditional method is highly optimized with direct formulas, whereas the GA method did general geometric steps that are overkill when the geometry is fixed (the inverter vectors are fixed and symmetric).

These are hypothetical but based on plausible numbers. The general theme: GA tends to be several times slower (5×, 8×, 28× in these examples) than specialized conventional approaches for computational tasks. It's rarely faster unless the conventional approach was doing something non-optimized that GA somehow streamlines (which is rare because GA is more general, not more specialized).

#### 9.3 Where GA Wins

Given all these downsides, one might wonder: are there measurable situations where GA improves something? Yes, mostly in terms of development time, clarity, and correctness rather than raw speed. Some examples:

**Offline analysis and prototyping:** If you're writing a script to analyze power flow patterns or to symbolically derive a formula for some configuration, GA can be a boon. You can write one general intersection or transformation routine and use it for many cases, rather than writing case-specific code. People have reported that using GA formulations reduced their code size and bugs for computing things like the possible solutions of a kinematic constraint problem, even if it ran slower, because they could trust the generality and focus on bigger issues.

**Bug reduction and edge-case handling:** Anecdotally, teams that have tried GA in complex domains found that once it worked, it handled weird corner cases (singularities, special alignments) more gracefully than their previous code which had a bunch of if-statements. One might see ~60% fewer bugs post-deployment and ~70% fewer special case branches. That's huge in safety-critical contexts like robotics or aerospace, where each if-branch is a potential untested path.

**Educational value:** For teaching and conceptual communication, GA can unify concepts brilliantly. Students often struggle to see the connection between, say, complex power, cross product, and orthogonal transformations. GA puts them under one roof. Some educators note that students who learn with GA grasp the "why" more deeply: e.g., why balanced three-phase has no negative sequence – because it's a bivector in a plane, etc., rather than just algebraic cancellation. That broader understanding can make them better engineers who can adapt concepts to new problems.

**Expressiveness in code:** GA libraries let you write operations that mirror the math closely. For example, if deriving a new control law that involves rotating reference frames and computing certain power flows, writing it in GA means your code almost looks like the paper equations. This reduces mental translation errors and makes it easier to update the model. The cost is performance, but if it's for a simulation or tool (not production controller), that's fine.

In summary, GA "wins" when the problem complexity (geometric interactions, multiple cases) is high and the performance demand is low or moderate (offline analysis, simulation, or high-level decision logic), or when clarity and correctness are more important than squeezing every last cycle. It "loses" in simple or very speed-critical tasks where its generality just introduces overhead with no benefit.

The next Part will explore how to balance these factors by deliberately mixing GA and conventional approaches – so you can get the wins where they matter without paying for losses where you can't afford them.

---

## Part III: The Hybrid Path (reconciliation)

### Chapter 10: Where GA Pays Its Rent

#### 10.1 The Five-Primitive Rule

Over time, a heuristic has emerged for deciding if GA is likely to be beneficial for a given problem: **If your problem naturally involves five or more different types of geometric primitives or transformations interacting, GA typically pays off.** By "primitive" we mean a basic geometric element or transform: point, line, plane, circle, sphere, rotor, translator, etc. The rationale is that with so many types, a traditional approach will have a combinatorial explosion of pairwise interactions and special formulas, whereas GA will handle them uniformly with meet, join, products, etc.

For example, consider a mechanism design problem with:

1. **Points** (e.g., joints)
2. **Lines** (e.g., axes of sliders or rails)
3. **Planes** (e.g., guides or mounting surfaces)
4. **Circles** (e.g., gear tooth paths or bolt hole patterns)
5. **Screws** (helical joints or leadscrew motions combining rotation+translation)

That's already five distinct geometric types. Conventionally, you'd need separate math and case handling for point-plane coincidence, point-line distance, line-line intersection, line-plane intersection, circle-line intersection, etc. (And indeed mechanism design textbooks have chapters for each contact or joint type with different formulas.) GA can represent all of those in a single algebra (likely Conformal GA for circles/spheres, or using intersections of planes for circles in PGA). The key operations (meet and join) and constraints (orthogonality via dot, incidence via wedge) work across the board. Instead of 20 case-specific formulas, you have one toolkit. That's where GA shines: it doesn't get more complicated as the number of entity types increases; it just uses the same building blocks in various combinations.

The "five" number is not a hard cutoff, but it's a guideline. If you only had two or three types, say lines and planes and maybe points (like a simple 2D geometry problem), you might manage with classical vector algebra fine (line-line intersection formulas, etc.). But with five or more, the classical approach often becomes a maintenance nightmare of scattered formulae. GA will be slower per operation but may actually be faster to implement correctly and reduce total development time.

Power systems, interestingly, often involve fewer primitive types in their core analyses (mostly nodes = points, branches = connections or impedances which are not geometric shapes). But when you extend to spatial configuration (like substation layout design, clearance geometry, routing), multiple primitives arise. If you find yourself juggling lines, planes, volumes for clearances, and perhaps angles (rotors) for orientation of equipment, GA might help unify those calculations.

#### 10.2 Intersection-Dominant Problems

Another category where GA tends to pay off is when the core of the algorithm is computing lots of intersections or relationships between geometric objects. Ray tracing in graphics is a classic example: you have rays (lines) and scene geometry (planes for polygons, spheres, etc.), and you need to find where they intersect. With GA, you represent all shapes in a homogeneous form and just compute meets. Researchers have shown that using GA for ray tracing is feasible: the code becomes simpler (no separate case for each shape, just wedge and dual operations), and the performance can be competitive because it vectorizes well across many rays (the meet of many rays with an object can be done in parallel). Collision detection is similar: checking if objects intersect can be phrased in terms of primitive-primitive meets (line-plane, sphere-plane, etc.) which GA can handle uniformly.

In CAD and constraint solving, if you need to solve constraints like:

- Two holes (circles) must align (their axes coincide = line-line alignment).
- A shaft (cylinder axis) must be perpendicular to a surface (line ⟂ plane condition).
- Three points define a plane, etc.

A traditional solver would have different subroutines for each combination (20+ cases of constraints). A GA-based solver can treat everything as: represent in GA, then solve via meet/join or optimization with GA terms. One report found a CAD constraint solver could reduce 2,400 lines of intersection code to 300 lines using GA meets, at the cost of about 2× runtime (which was acceptable) and with far fewer bug-fix patches needed.

In power systems, you might not think of "intersection-dominant" problems, but consider protection coordination (zones overlapping), or facility layout (bus bars clearance volumes). Those can become geometry heavy. Or consider network planning: corridors (volumes) intersecting protected areas, etc. Uniform incidence calculations could simplify analysis of rights-of-way, clearances to objects, etc.

So the rule: if your algorithm mostly does geometric intersections or distance checks in various combinations, GA's meet/join and inner product operations will likely simplify implementation and reduce errors, even if each operation is slower than a custom formula. The overall development and maintenance drop.

### Chapter 11: Where GA Fails

#### 11.1 Sparse Linear Systems

Not all problems should be geometrized. Large linear systems, for example, are the bread and butter of power flow analysis. You have a Jacobian matrix in Newton-Raphson for a power network – maybe 10k buses, so a 20k × 20k sparse matrix (each bus connects to a handful of others). Solving that efficiently (with sparse factorization or iterative methods) is critical. GA doesn't offer anything special here; in fact, it tends to obliterate sparsity. If you tried to represent that linear system in GA terms (like as a big multivector equation or as a meet of many objects), you'd lose the matrix sparsity because geometric product mixes everything.

We can estimate: a 10k-bus Jacobian might have, say, 0.1% nonzero entries (20k×20k matrix with ~40k nonzeros). A good sparse solver will exploit that and maybe solve in $O(n \log n)$ or similar. If you encoded something similar in GA, you'd likely end up with operations that are dense on an effectively 10k-dimensional multivector – completely infeasible.

So any problem that is well-served by sparse linear algebra (which includes most large-scale network simulations, finite element matrices, etc.) is a hard **NO-GO** for GA. Use the right tool: linear algebra was made for this, and decades of research have honed it. GA is not magic to speed that up; in fact it hides the matrix structure and prevents using known algorithms.

Even moderate-size sparse systems suffer. The example in our outline was a power flow Jacobian multiply: sparse matrix multiply took 0.3 ms, GA geometric product 45 ms – 150× slower. GA basically turned a 0.1%-filled matrix into a ~90%-filled multivector operation after one multiplication. That's unacceptable in any large-scale computation context.

Therefore:

- **For power flow or circuit simulation** (sparse nodal analysis), stick to matrix methods.
- **For finite element grids,** similarly stick to sparse matrices.

GA might still help in formulating some models (like deriving small-signal equations via GA reasoning about symmetries), but when it comes time to solve $Ax = b$ with large $A$, do it with proven sparse solvers.

#### 11.2 Hard Real-Time Systems

We've touched on this: if your system has strict real-time deadlines with minimal slack, GA's overhead can break it. Two scenarios:

**Graphics rendering (real-time):** Suppose a game runs at 60 FPS (16.6 ms per frame). Within that budget, say you have at most 2 ms for all shadow ray calculations. If a GA approach to ray-surface intersection uses 8 ms, you miss the frame, causing a drop in frame rate. In games, even 2× slower can be unacceptable if it pushes beyond v-sync times. The example given: 2 ms vs 8 ms means an unplayable situation.

**Fast control loops:** A motor drive current loop often runs at 100 µs or faster. If the GA-based controller uses 45 µs versus 10 µs conventional, as in our earlier analysis, you might think it's okay (45 < 100). But keep in mind other tasks also share that 100 µs (ADC sampling, communications, etc.). Also, a design might target 50 µs in future or want margin. A 4.5× slower algorithm might break if anything else goes wrong or if system scaling demands more computations.

Hard real-time means worst-case execution time (WCET) must be < deadline, often with no dynamic memory, no unpredictable branches that cause jitter, etc. GA operations, being more complex, can have more variation in execution path (due to caches or pipeline differences). That adds uncertainty which conservative engineers handle by budgeting more time – sometimes not available.

Therefore, in protective relaying (which often needs operation in 1–2 cycles = 16–33 ms) GA might fit, but in something like a 4 µs sensor interrupt (some high-frequency trading or special ADC interrupts) GA would not.

One could imagine special hardware making GA feasible in real-time (see Part VI), but given typical microcontrollers/DSCs, it's mostly a no-go for inner loops. Use GA at higher control layers (where cycle times are slower, like a 10 ms outer loop maybe).

### Chapter 12: The Hybrid Architecture

#### 12.1 Design-Time GA, Runtime Conventional

The pattern that works is:

1. **Derive the algorithm in GA** (for clarity and generality).
2. **Prove or verify key properties in GA** (so we trust its correctness and invariants).
3. **Translate the final algorithm to conventional math** (matrices, vectors, etc.) for implementation.
4. **Execute the conventional implementation on actual hardware** (fast).
5. **Validate the outputs against the GA model** offline or in a high-level simulation.

This way, GA is used where it adds value (design, understanding, ensuring no cases missed) but not where it hurts (time-critical loops).

We've implicitly followed this in many examples:

**dq0 transform:** We explained it as rotors $R_{dq0} = R_{\text{Park}}R_{\text{Clarke}}$, showing it's orthonormal and power-invariant. Then in practice we implement it as a 3×3 matrix multiply (since that's just 9 multiplications) for speed. We can validate that matrix preserves norms (check $CC^T = I$) and is equivalent to the rotor.

**Fault intersection:** We can derive fault location formulas by a meet in GA, then realize it corresponds to solving a small linear system (which we implement with a few multiplies/divides).

**SVM:** GA can conceptually derive the duty cycle formulas (via wedge ratios), then one would hardcode those formulas or use a lookup for actual control code, rather than performing wedges each time.

In doing translation, one must be careful to not reintroduce cases GA avoided. For example, GA might handle line-line vs line-ground uniformly; when translating to code, ensure to keep that generality (maybe by using a unified solve that can handle either by checking a coefficient). Keep an eye out for special-case divisions (like dividing by $\sin\theta$ somewhere – consider what if $\theta \approx 0$, which GA handled by giving ideal result instead).

Then, validate by running both versions (GA and conventional) on test cases. For instance, simulate a fault scenario with GA and with the conventional algorithm and ensure they give the same answer. If they diverge, use GA to debug where the conventional logic might be missing a case.

This approach was used by some high-integrity developers: write a high-level GA model (maybe even in Python using sympy/GA libraries) to verify logic, then use low-level C for actual code, and cross-check results on test inputs.

#### 12.2 Boundary Management

The interfaces between GA and conventional components need clear definition. Often they involve coordinate transforms or normalization:

- **When passing a rotor angle to a conventional controller,** ensure it's within $[0,2\pi)$ or handle wrap.

- **If converting a GA motor to a 4×4 matrix,** renormalize the motor first (so that numeric drift doesn't produce a shearing matrix).

- **If converting back,** re-orthogonalize the rotation matrix to purge drift (e.g., use a quick QR or average method to enforce orthonormality).

One might maintain both representations in parallel: e.g., a rotor for concept and an angle variable for control. Keep them consistent by updating one from the other periodically (like if angle drifts from rotor, correct it).

Another consideration is error tracking across boundaries. If a conventional filter estimates something in one representation and we convert to GA, small differences might appear. We should design thresholds with margin. For instance, if GA computation says a line and plane meet at a point 100 km away (nearly parallel), but conventional logic had a tolerance and reported "no fault" (parallel), the discrepancy should be caught by a sanity check (like if GA meet yields $w$ very small, perhaps treat as no finite intersection in results to match conventional expectation).

In software architecture, these boundaries are often good module seams. You might have a GA-rich module for "geometry processing" and a conventional module for "numerical solving". Define the data contract: e.g., "Input points must be normalized trivectors with $P \cdot I^{-1} = 1$; output will be an array of doubles $[x,y,z]$ representing that point's coordinates if real, or a flag if ideal." Inside the GA module, handle all edge cases and output a clear result.

By carefully managing boundaries, you prevent the leakage of GA's quirks into the rest of the system. The conventional parts can remain unaware of GA except for using the precomputed formulas.

### Chapter 13: Decision Framework

#### 13.1 The Go/No-Go Checklist

To synthesize our guidance, here's a checklist to evaluate whether GA is a good fit:

**GO if:**

☐ **≥5 geometric primitive types** appear in the problem. (If you have many different shapes or spatial concepts, GA's uniform handling is beneficial.)

☐ **Many intersection or incidence operations.** (GA's meet/join shines here.)

☐ **Equivariance or invariance requirements.** (If results must not depend on coordinate frame orientation, GA inherently enforces that, e.g., via rotor formulations.)

☐ **Offline or relaxed timing.** (If it's an analysis tool or a background process with milliseconds to spare, GA's overhead is acceptable.)

☐ **High bug cost** (safety or reliability critical). (GA's reduction of special cases can prevent bugs that might be catastrophic.)

☐ **Team has or welcomes mathematical background.** (A team comfortable with linear algebra and willing to learn GA can harness it; a team that hates new math will resist.)

**NO-GO if:**

☐ **Sparse linear algebra is the main workload.** (GA will destroy sparsity and be much slower; use matrix methods.)

☐ **Hard real-time constraints with tight budgets.** (GA likely can't meet microsecond-level deadlines on standard hardware.)

☐ **Probabilistic estimation on constrained manifolds.** (If your core problem is a Kalman filter on orientations, you're better off with quaternions + covariances than trying GA directly.)

☐ **Team or stakeholder resistance.** (If introducing GA will cause friction, missed deadlines due to learning, or maintenance issues with new hires, think twice.)

☐ **Poor tool support.** (If no libraries or visualization exist for your GA needs, you'll spend extra time building those; only worth it if the payoff is high.)

These criteria are not absolute, but if you tick several GO and few NO-GO, that suggests GA is worth exploring.

For example, consider using GA for developing a new protective relay algorithm:

- It involves points (relay locations), lines (transmission segments), maybe planes (zones of protection boundaries) – ~3 types, plus rotations if transforming sequence components – **borderline**.
- It involves a lot of intersection (fault intersection with zones) – **yes**.
- Equivariance: fault detection should be frame-invariant (units and orientation don't matter) – **yes**.
- Real-time: operates in ~10–20 ms range – somewhat time-sensitive but not microsecond, **likely fine**.
- Bug cost: false trip or miss can cause blackouts or damage – **high**.
- Team math: protection engineers have math background but not necessarily GA – **moderate**; might accept if well explained.
- Sparse linear systems: not core, mostly logic – **OK**.
- Probabilistic: state estimator in relay maybe minimal – **OK**.
- Team resistance: if pitched as unifying symmetrical components, **maybe okay**.
- Tools: limited GA tools for relaying; one might have to implement GA routines – **a consideration**.

The checklist would lean slightly toward GO if the benefits in unifying logic are strong and time budget permits a bit more computation.

#### 13.2 Migration Strategy

If you decide to introduce GA into an existing project (or start a new one with GA), do it incrementally to manage risk:

1. **Prototype on a smaller problem:** Pick a module or scenario with clear geometric aspects that is somewhat self-contained. Implement it with GA in a sandbox (e.g., a Python script or a separate program) to ensure it works as expected.

2. **Benchmark the prototype:** Measure how much slower it is than current methods on realistic inputs. If it's within acceptable range (maybe up to 5–10× slower might be okay if that part isn't dominant), continue. If it's 100× slower and critical, either drop GA or plan heavy optimizations/hardware help.

3. **Integrate as optional or parallel path:** Add the GA-based implementation into the system behind a flag or as a monitoring tool. For instance, run it in parallel with the existing method and compare outputs (without feeding its results forward). This builds confidence and helps debug differences.

4. **Tune and optimize:** Identify hotspots in the GA code. Perhaps precompute certain multivectors, simplify expressions (like avoid doing full geometric product if only need a specific part – manually optimize or use a code generator).

5. **Gradually switch over:** Once the GA path is proven to produce correct results and meet performance (maybe with some optimization or slight algorithm adjustments), you can switch the system to use it as primary. Keep the old method as fallback for a bit (maybe compile it in and log if results diverge unexpectedly).

6. **Document and train:** Ensure the team is on board – conduct a workshop or share resources so others know how to work with the GA code. This prevents maintenance issues down the line. Provide lots of comments connecting GA code back to conventional formulas for those who think that way.

**Never attempt a big bang rewrite** of an entire codebase in GA. That's high risk. Instead, use GA in new features or one subsystem at a time.

For new projects, still perhaps implement one feature with GA fully to set a pattern, while doing others conventionally until that pattern is validated.

This migration approach mirrors how one might introduce any new tech (functional programming, GPU computing, etc.) – careful, measured steps, always with a fallback plan.

By following these steps, you capture GA's benefits (simpler logic, fewer bugs) while minimizing the downside (performance surprises, team confusion). It exemplifies the book's thesis: hybrid solutions often yield the best of both worlds.

*(End of Part III. We have established how to judge GA's usefulness and how to integrate it responsibly. Now, in Part IV, we apply this mindset to deep-dive into specific power system problems, demonstrating both the GA formulation and the eventual hybrid solution.)*

---

## Part IV: Power Systems Deep Dive (proof of concept)

### Chapter 14: Three-Phase Unbalance as Geometry

#### 14.1 Fortescue's Theorem Geometrically

Symmetrical components (Fortescue's theorem) decompose an unbalanced three-phase system into balanced sequences: positive (abc rotation), negative (acb rotation), and zero (in-phase). Geometrically, this is a decomposition of the 3D phase-vector space into three invariant subspaces under 120° rotation.

In GA terms, balanced three-phase voltages form a 2D plane in the 3D space (since $v_a + v_b + v_c = 0$ for no zero-sequence). The positive-sequence component lies in that plane and rotates in one direction; the negative-sequence lies in the same plane but with opposite orientation (rotation reversed). The zero-sequence is along the line $e_a + e_b + e_c$ (the $e_0$ direction in Clarke's terms).

We can formalize it:
Let $\mathbf{v}_{abc} = v_a e_1 + v_b e_2 + v_c e_3$. Define the 120° rotation operator (a rotor) $R_{120} = \exp(-\frac{2\pi}{3} e_{12})$ which cycles the phases ($e_1 \to e_2 \to e_3 \to e_1$ effectively). Positive sequence is the part of $\mathbf{v}_{abc}$ that is an eigenvector of $R_{120}$ with eigenvalue 1 (meaning it rotates exactly through itself after 120° – so it's the balanced part that moves with the system frequency). Negative sequence is the part that gets an eigenvalue 1 under a negative rotation by 120° (or eigenvalue for $R_{120}$ of, say, $e^{-j2\pi/3}$ if using complex analogy). Zero sequence is the part that is invariant under any permutation (the $e_1 + e_2 + e_3$ direction, which $R_{120}$ leaves unchanged too, aside from a phase factor maybe).

In practice: we can project $\mathbf{v}_{abc}$ onto these subspaces. GA provides projection: If $B_+$ is a bivector representing the positive-sequence $\alpha\beta$ plane (with a certain orientation), then the positive-sequence voltage $\mathbf{v}_+ = \frac{1}{2}(\mathbf{v}_{abc} - R_{120} \mathbf{v}_{abc} \tilde{R}_{120})$ – this formula comes out similar to Fortescue's $\frac{1}{3}(a^2 V_a + a V_b + V_c)$ if expanded. The negative sequence $\mathbf{v}_-$ would use $R_{120}^{-1}$ similarly. Zero-sequence $\mathbf{v}_0 = \frac{1}{3}(v_a + v_b + v_c)(e_1 + e_2 + e_3)$ basically.

The important insight is that these sequences are orthogonal components of the vector $\mathbf{v}_{abc}$. In GA, orthogonal means their wedge (or dot) relationships are zero. Indeed, a positive-sequence bivector $B_+$ (which one can represent as $e_{12}$ after Clarke) is orthogonal to the zero-sequence vector $e_0$ (because $e_{12} \wedge e_0 \neq 0$ – actually not orthonormal in metric sense but separate subspace). Similarly, the positive and negative sequence vectors in complex form are complex conjugates; in GA, their bivectors are negatives of each other's orientation, which implies they span the same plane. So one could either treat that plane's oriented component (positive vs negative orientation as separate, which you can by splitting bivector into a part and its negative).

Another view: GA handles unbalance by naturally producing a grade-1 (zero seq) and a grade-2 (seq plane) part when you compute a power multivector. In example 1.3, an unbalanced system yields a trivector part as well, corresponding to the zero-seq.

For building an unbalance metric, GA suggests one: measure how far the actual phase vector deviates from lying in a plane. For a balanced system, $\mathbf{v}_{abc}$ lies in the $e_{12}$ plane (for Clarke's coordinates). If unbalanced, it has a component out of that plane (the $e_0$ or zero-seq direction). The angle between $\mathbf{v}_{abc}$ and the nearest balanced plane (span of any 2 phases) is a direct measure of unbalance. Alternatively, the ratio of magnitudes of the bivector part (seq) vs the vector part (zero-seq) in the Clarke decomposition could quantify unbalance invariantly.

Engineers currently use %unbalance = $(V_{neg}/V_{pos}) \times 100\%$. GA would concur: $|\mathbf{v}_-|/|\mathbf{v}_+|$ is that ratio, which can be computed after projecting onto the sequence subspaces via GA operations. It's coordinate-free: no matter how you rotate the reference frame, that ratio stays same.

So GA doesn't fundamentally change symmetrical component analysis; it illuminates it. It shows that Fortescue's method is essentially projecting onto eigenspaces of a 120° rotation (the bivector $e_{12}$ plane vs the scalar $e_0$ line). Understanding that can help in complex situations – for example, in a 4-wire system with an isolated neutral, GA easily extends to a 4D algebra where the zero-sequence becomes a full 4th basis dimension; Montoya et al. did that.

#### 14.2 Unbalance Metrics That Make Sense

Traditionally, unbalance is quantified by phase voltage magnitude differences or by symmetrical component ratio as mentioned. GA suggests more geometric metrics:

**Angle from Balanced Plane:** Compute $\theta = \arctan\left(\frac{|v_0|}{|v_{pos}|}\right)$ where $|v_0|$ is the magnitude of the zero-seq component and $|v_{pos}|$ magnitude of positive sequence. This angle $\theta$ is zero if no zero-seq (fully balanced except maybe negative seq), and increases as zero-seq grows. It's an easy interpret: if $\theta = 0$, phase vector lies in a plane; if $\theta = 90°$, phase vector is along $(1,1,1)$ (complete zero-seq and no positive).

**Negative Sequence Fraction:** Similarly $\frac{|v_{neg}|}{|v_{pos}|}$ is often given in %. GA yields that as the ratio of two bivector magnitudes (since both pos and neg are bivectors in same plane, $|v_{neg}|$ is just the magnitude of that bivector oriented oppositely). Actually, $|v_{neg}| = |v_{pos}|$ for symmetrical but opposite orientation – if unbalance, that ratio is how much of the plane part is in opposite orientation vs correct orientation. Instead of representing that as a complex number with -120° phase, GA might represent it as a bivector with negative sign. So one could express unbalance factor as the ratio of these oriented areas (like if $B = B_+ + B_-$ as bivectors, one could measure $|B_-|/|B_+|$ and the angle between them – which will be either 0 or 180°, since they lie in same plane but oriented opposite).

**Direct geometric deviation:** Another concept: the volume (trivector) formed by the three phase voltage vectors $v_a, v_b, v_c$. For a perfectly balanced set, $v_a + v_b + v_c = 0$ and they lie in a plane, so the volume of the parallelepiped they span is zero. If there's unbalance (including zero-seq), a finite volume appears. That volume's magnitude (normalized appropriately) could serve as a scalar unbalance indicator. For example, define $U = \frac{|v_a \wedge v_b \wedge v_c|}{|v_a| |v_b| |v_c|}$. $U$ is 0 if and only if the vectors are coplanar (balanced or one phase zero). The larger $U$, the more "skewed" the three vectors are in 3D, indicating heavy unbalance or harmonics. However, $U$ might be more sensitive to zero-seq than negative seq (since negative seq still leaves them planar, albeit reversed rotation).

Overall, GA doesn't give a single magic new metric but provides a clear interpretation: unbalance = how non-planar the phase vector set is. That's a nice geometric picture that can inform designs (e.g., in generator design, ensure that any asymmetries are small enough that the voltage space stays nearly planar).

### Chapter 15: Harmonic Power Under Distortion

#### 15.1 The Grade Explosion of Harmonics

When voltages and currents contain multiple frequency components, their instantaneous power contains not just DC and twice-frequency terms, but cross terms at other frequencies. In GA, these appear as higher-grade elements of the power multivector. Consider a case: $v(t) = V_1 \cos(\omega t) + V_5 \cos(5\omega t)$ (with $V_5$ smaller) and $i(t) = I_1 \cos(\omega t - \phi) + I_7 \cos(7\omega t - \psi)$. Expand $\mathbf{v} = \mathbf{v}_1 + \mathbf{v}_5$ and $\mathbf{i} = \mathbf{i}_1 + \mathbf{i}_7$ as before.

We get terms at:

- **0 Hz** (from $1 \times 1$, the DC real power),
- **2ω** (from $1 \times 1$ misalignment and from 5 & 7 difference),
- **4ω** (from 5&1 and 7&1 differences),
- **6ω** (from 5&7 difference),
- **12ω** (from 5&7 sum)

In GA, if we tried to represent this in a fixed 3D algebra, it fails because 3D cannot capture four independent oscillation modes. The presence of a 7th harmonic current means current's rotation in αβ plane is not synchronous with voltage's 5th or fundamental, so you effectively have a 4D space spanned by basis for 60Hz, 300Hz, 420Hz, etc. GA would need more basis elements for each independent frequency.

This is a clue: to fully handle harmonics in GA, we likely need to extend the algebra (like add basis for each harmonic's αβ plane). There is research on multi-frequency GA where you treat time as another dimension or treat each harmonic as a separate dimension. But that becomes high-dimensional quickly.

For understanding, GA says: due to harmonic mixing, the power multivector no longer lies purely in grades 0 and 2. The cross-frequency products produce e.g. a grade-4 term in 3D (like a pseudoscalar) which has no physical analog in the 3D space, hinting you've stepped out of the space – i.e., you need more dimensions (time/frequency domain) to represent it correctly. The user note: "grade-4 components in 3D indicate something trying to exist outside the space" captures that.

In practice, we address this by computing power in frequency domain: e.g., $P_{h,k}$ for harmonic h of v and k of i, and summing. GA doesn't circumvent that, but frames it as: if there's coupling, an element of a grade appears that should ideally be zero in an ideal model (like in an ideal linear system, no cross-harmonic terms; cross terms mean a nonlinear interaction or measurement artifact).

Inter-harmonics (non-integer frequencies) are even worse since they break periodicity – GA might require a continuum of basis blades to represent them, which is basically doing an integral (spectrum).

Thus, GA reinforces the limitation: traditional complex power (a single $P + jQ$) fails under distortion because the situation needs a richer description (higher grades). You can either expand the GA or stick to multi-frequency analysis. So far, the industry does the latter with standards like IEEE 1459 defining how to compute "nonactive power" for distortion – often via filtering and definitions akin to GA's breakdown.

#### 15.2 Measurement Implementation

Measuring power in distorted systems typically involves filtering to separate fundamental components (to define $P$ and $Q$ similar to sinusoidal case) and distortion power components. GA can conceptually unify these filters:

**A projection in GA terms corresponds to an ideal filter.** For instance, projecting $S(t)$ onto grade-0 and grade-2 parts corresponding to fundamental frequency yields the fundamental $P$ and $Q$. In practice, you achieve that by filtering out harmonics (low-pass filter for DC, band-pass at 2ω for reactive oscillation).

**Windowing in time domain** (like using a Hanning window to compute Fourier components) can be seen as weighting that can mix grades if not aligned. A symmetric window that is exactly an integer number of fundamental cycles effectively projects onto the fundamental subspace, minimizing leakage (other grade mixing).

**If you use a rectangular window of one cycle,** GA-wise you get a clean separation of fundamental active/reactive (because an integer number of cycles cancels all cross terms for that harmonic). If you use a non-integer window, the bivector (reactive) part at fundamental may leak into scalar or higher parts, making it look like some extra "power".

**GA perspective encourages designing measurement algorithms** that commute with the rotation due to fundamental. That means using windows or filters synchronized to fundamental frequency so that they don't mix time-phase with spatial-phase.

Concretely, the standard definitions in IEEE 1459 for powers under distortion are somewhat complicated (defining "active", "reactive", "distortion" power components). GA offers an alternate language that might simplify deriving those formulas or at least interpreting them:

- **"Active power"** = time-average of scalar part of $S(t)$ (grade-0 average).
- **"Reactive power"** = amplitude of fundamental-frequency oscillation of the bivector part of $S(t)$ (basically fundamental $Q$).
- **"Harmonic distortion power"** = sum of contributions from higher-grade terms (which themselves could be broken into oscillating parts at various frequencies).

One could envision a GA-based instrument that computes $S(t)$ continuously and does grade filtering. In practice, implementing GA in a meter isn't necessary – one can stick to conventional digital signal processing. But GA assures that what the meter is doing (filtering and combining phasors) has an invariant meaning: projecting the power multivector onto specific grade subspaces.

For example, Montoya et al. described instantaneous geometric power and decomposed current into active and non-active parts via GA. Their method essentially replicates Fryze's or Depenbrock's results but with GA clarity. It shows, for instance, that non-active current is the part of current vector that produces only bivector (reactive) power with voltage.

In summary, GA doesn't change how we measure (still need filtering, etc.), but it provides a unifying principle: separate grades = separate power phenomena. It reminds us to design measurement algorithms that achieve that separation cleanly (like using orthogonal filters).

### Chapter 16: Fault Analysis Via Incidence

#### 16.1 All Faults Are Meets

Power system faults (short circuits) can be classified by which conductors are connected:

- **Single line-to-ground (SLG):** one phase touches ground.
- **Line-to-line (LL):** two phases touch each other.
- **Double line-to-ground (DLG):** two phases to ground.
- **Three-phase fault (LLL or LLLG if with ground).**

Geometrically, we can think of:

- **Phases as lines** (wires) in space (or their fields as lines in some configuration space).
- **Ground as a plane** (the earth plane at 0 potential).
- **A fault is where a line meets** either another line or the ground plane (or all three lines meet at a point in a three-phase short).

In PGA, a line is a bivector $L$. Ground plane is a vector $\Pi_{\text{ground}}$. The meet $P = L \vee \Pi_{\text{ground}}$ (or $(L^* \wedge \Pi_{\text{ground}}^*)^*$) gives the point of intersection (the fault location on that line). If two lines (phases) fault, you do $P = L_1 \vee L_2$ (their meet). Three lines faulting means $P = L_a \vee L_b \vee L_c$ (all three meet at a point) – in GA you'd do pairwise meets or a single expression with two wedges inside.

So indeed:

- **SLG fault:** $P_{\text{fault}} = L_{\text{phase}} \wedge \Pi_{\text{ground}}$ (should be a point, possibly at infinity if no physical intersection in space means maybe the fault is at infinite distance? Actually if a line is truly parallel to ground and never touches, no fault).

- **LL fault:** $P_{\text{fault}} = L_1 \wedge L_2$ (point or line if they overlap).

- **LLG fault:** two lines meet ground: you could find each line-ground meet and see if they're same or perhaps treat it as meet of all three objects.

- **3-phase fault:** $P_{\text{fault}} = L_a \wedge L_b \wedge L_c$ (if non-coplanar, actually three lines in 3D rarely meet exactly unless at substation or via an arc that forms some geometry; anyway in network terms, it's a node where all join).

The nice thing is, GA doesn't require writing separate equations for each fault type – the meet covers them. And if lines are parallel (no intersection) the meet yields the ideal line (no finite solution), telling you fault is at infinity (which in real terms means no fault / or a fault at an equivalent infinite distance).

Uniformly, after computing the meet point, you can determine fault distance or location along a line by further inner products (like distance from source node).

The residual or condition $|L \wedge \Pi|$ can serve as an index of how direct the fault is:

- **If exactly zero,** the line and ground meet in a finite point (definite fault).

- **If very small but not zero,** near-miss: maybe leakage or an arcing fault that hasn't fully shorted but nearly (the lines or ground are approaching).

- **For skew lines** (like a transmission line falling on another that's not parallel), $L_1 \wedge L_2$ yields a line (they don't meet, just skew). The GA result would indicate something like a line of shortest approach (that line is the line connecting the two closest points).

- **For a true short,** $L_1$ and $L_2$ become coincident at the fault (their meet is a well-defined point).

Therefore, one could create a protective relay logic: compute $P = L \wedge \Pi_{\text{zone}}$ (where $\Pi_{\text{zone}}$ is a plane demarcating zone boundary), if $P$ is finite and within the zone volume, trip.

#### 16.2 Protection Coordination Geometry

In protection coordination, each protective device covers a certain zone (line segment, bus, etc.). If a fault happens, you want only the device whose zone contains the fault to trip (primary protection) and maybe one next up if primary fails (backup).

In GA terms, a zone can be represented as a volume or region (like an intersection of half-spaces/planes – essentially a polyhedron or something). Determining if a fault point lies in a zone is an incidence test: does point $P$ lie on the correct side of all zone boundary planes? This can be done by plugging $P$ into each plane's equation (via inner product in GA) and checking sign.

So indeed:
$$\text{Operate if } P_{\text{fault}} \in \text{Protection Zone}$$
meaning for all boundary planes $\Pi_i$ of the zone, $(P_{\text{fault}} \cdot \Pi_i)$ has the sign that indicates inside.

GA can unify checking all those with wedge perhaps: one could form a big wedge of $P$ with all boundary planes and see if it yields a pseudoscalar of correct sign. But simpler: just evaluate signs individually.

Overlapping zones for backup is natural: zone2 might overlap zone1 partly. That's just geometry.

Selectivity problems (overtripping) often come because of CT saturation, etc., not geometry. But GA could at least ensure the logic of inclusion is correct.

#### 16.3 Real Implementation Results

From the prompt: for a test system:

- **Traditional distance relay computation** used an impedance measure (voltage/current) and took ~140 μs, with separate logic for each fault type (phase-phase vs phase-ground, etc.).

- **GA implementation:** single meet formula, 400 lines of code instead of 2400, but took 890 μs on same platform – 6× slower.

For primary protection (which might need to operate within, say, 1-2 cycles ~ 20-40ms), 0.89ms is actually fine. Even 6× slower, it's within margin. So GA could be acceptable. For backup or analysis, speed isn't issue.

But if we needed it in a super fast auto-reclosing or something within microseconds, maybe not.

Anyway, the GA code handling all faults uniformly is easier to maintain and verify (80% fewer bugs as noted), which is worth a slightly slower runtime that's still within requirements.

Thus, in fault analysis, GA is likely beneficial for off-line studies and possibly on-line relays if carefully optimized. But one must ensure enough hardware capacity (maybe use a DSP that can handle the extra math).

### Chapter 17: Space-Vector Modulation as Rotor Choreography

#### 17.1 Eight Switching States as Geometric Objects

A three-phase two-level inverter has 8 possible switching states ($2^3$):

- **6 active states** where one phase is at +Vdc/2 and two at -Vdc/2 (or combinations, basically corresponding to 6 non-zero space vectors evenly spaced 60° apart in the αβ plane).

- **2 zero states** where all outputs are equal (both at + or both at -, yielding 0 vector).

These 6 active vectors indeed form a hexagon in the αβ plane. The reference voltage vector (the desired output) rotates as a rotor in that plane (if we neglect zero-seq, which in SVM we do by balancing the zero times).

SVM essentially says: at each cycle, find the triangle of the hexagon in which the reference lies, and use the 2 adjacent active vectors (the vertices of that triangle) plus zero to approximate the reference by time averaging.

So geometrically: the reference vector $\mathbf{v}^*$ at some time is inside a triangle formed by $(0, V_k, V_{k+1})$ where $V_k$ and $V_{k+1}$ are two adjacent state vectors, and 0 is the origin (the null state). Actually, some implementations use the two active and one of the zero states as needed.

The duty cycles are basically the barycentric coordinates of $\mathbf{v}^*$ in that triangle. If $\mathbf{v}^* = d_k V_k + d_{k+1} V_{k+1} + d_0 (0)$ with $d_k + d_{k+1} + d_0 = 1$, then $d_k, d_{k+1}, d_0$ are the fraction of time to spend in each state.

In GA, each switching state can be represented as a vector in 3D or in αβ0 space. Actually, in $\alpha\beta0$ coordinates:

- **Active states** correspond to certain combinations, e.g., (2/3, -1/3, -1/3) in abc leads to a certain αβ vector.

- **The two zero states** correspond to the zero vector plus maybe a zero-sequence (common-mode, which we usually ignore by clamping or if using symmetrical, they produce same output line-to-line).

So treat each switching state as a vector $\mathbf{S}_i$ in $\mathbb{R}^2$ (for αβ, ignoring zero). Then the reference is a rotor rotating vector $\mathbf{v}(t)$.

Finding nearest three states can be done by identifying sector (k). Usually done by checking the sign of $\alpha$ and $\beta$ etc. In GA, one might do something like:
Take the 6 active vectors (bivectors? Actually vectors in plane), wedge the reference with each to see which wedge is positive/negative to tell on which side of each separating line it is. But anyway, one can identify the containing triangle by a series of dot products.

Once you have the two nearest active states, computing duty cycles is solving $\mathbf{v}^* = d_1 \mathbf{S}_1 + d_2 \mathbf{S}_2$ (since $d_0$ is just leftover to make sum 1). That's two equations if we do in αβ plane, easily solved (like using cross product or wedge to find area ratios).

A GA viewpoint: if $\mathbf{v}$, $\mathbf{S}_i$, $\mathbf{S}_j$ are all in a plane, then
$$d_j = \frac{\mathbf{S}_i \wedge \mathbf{v}}{\mathbf{S}_i \wedge \mathbf{S}_j}$$
which is essentially computing areas (barycentric coordinates by area ratio). This wedge formula is neat because if $\mathbf{v}$ is outside, it might give negative duty which tells you logically it's in adjacent sector. So GA could unify the logic: you wouldn't separately code each sector's formula; you could plug into a general wedge-based formula.

But computing those wedges (which are just scalar area values in 2D) is trivial anyway (a couple multiplications). GA not needed to get formula but it ensures it's correct for all cases.

The overhead came in performing these for every switching cycle in GA (340 µs vs conventional 12 µs). Likely because the GA approach did more steps:

- Maybe constructing multivectors, computing wedge via a GA library vs just a simple multiply-subtract for cross product.

So while conceptually elegant, it's overkill at runtime. People hand-code SVM with if-else and a few math ops because it's so time-critical in high frequency switching.

Perhaps if we had GA hardware, we could do it differently, but currently not.

So SVM is a case where GA clarity might help in understanding or writing a generic SVM algorithm that works for any number of levels or phases (like a GA generalization could possibly handle multi-level inverters by just adding more state vectors in the space and doing a polytope barycentric calc), but at runtime you'd still implement that as specific if-else or linear solves.

Thus, GA is more of a tool to derive or generalize SVM rather than to implement it directly on a microcontroller.

#### 17.2 Why It's Too Slow

This was already partly answered: GA didn't fit the microsecond budget because:

- **Identifying the sector** via GA might have involved checking multiple wedge signs or meet with each region's half-plane, causing warp divergences or just more ops than a simple compare network.

- **Duty calculation** via wedge operations is minor overhead but maybe still more than a direct formula.

SVM is heavily optimized in practice (some do it with lookup tables or precomputed sin values). GA approach might be dynamic and elegant but microcontrollers like simple arithmetic.

So GA fails in this *very high speed* control inner loop scenario by ~7× as given.

One could conceive maybe using GA offline to precompute switching tables or optimize modulation schemes (some research uses geometric methods for optimal modulation), but the actual modulation implementation will remain straightforward code.

### Chapter 18: Motor Control With Geometric Tools

#### 18.1 Everything Is a Rotor

Field-oriented control (FOC) of AC machines essentially aligns the stator current vector with the rotor flux vector. In an induction or synchronous motor:

- **The stator** has a rotating magnetic field (the current produces an MMF that can be seen as a rotating vector in space).

- **The rotor** (either actual magnets in synchronous or induced currents in induction) has its own field vector (rotor flux).

- **Torque is proportional** to the cross product of stator and rotor field vectors (in 3-phase, $T \propto \mathbf{B}_{stator} \times \mathbf{B}_{rotor}$ or sine of angle between them, etc.).

In GA terms, these magnetic field vectors (or rather the current and flux linkage) can be treated as vectors or bivectors. Actually, in STA (space-time algebra) one might use bivectors for E and B fields, but here simpler: treat them as 2D vectors in the dq plane attached to rotor or stator reference.

FOC aims to keep the stator current vector (in stator frame it's oscillating) aligned with rotor flux (which in the rotor frame is mostly DC on the direct axis). We do Park transform to rotor frame, then control $i_d$ to match flux, and $i_q$ to produce torque.

From GA perspective:

- **The rotation from stator to rotor frame** is a rotor (the angle difference between stator field and rotor).

- **Once aligned,** the quadrature component (perpendicular) generates torque, the direct component aligns with flux (like active vs reactive magnetizing current).

- **One could describe the dynamic** of the machine as the rotor flux vector being dragged along by rotor motion, and the stator current vector being driven by the inverter rotor (like a chase).

**Saliency** (in e.g. IPMSM) means the inductance differs by axis – in GA, that might be seen as the reluctance (inverse of inductance) being a bivector that is not isotropic but oriented with rotor, meaning the machine differential equations have rotor-angle-dependent terms (harder to express in GA without breaking linearity... perhaps a rotor that maps an isotropic inductance matrix into an anisotropic one).

**Reluctance torque** (as in a reluctance motor) basically arises because the current rotor tries to align where inductance is highest for given current, which is also a geometric condition (minimize co-energy etc.). GA might express that elegantly by saying the co-energy is $\frac{1}{2} \mathbf{i} L \mathbf{i}$ with L as a rotor-variant operator, etc.

But overall yes, **"Everything is a rotor"** – the core idea is all these rotating fields and transformations can be described with GA rotors:

- The stator field is a rotor times some reference orientation.
- The rotor itself can be described by a rotor (physical angle).
- The transformation between frames is a rotor.
- The optimal control is to set a certain rotor equal to another or offset by 90° etc.

This unified view could conceptually simplify deriving control laws, e.g. how to handle a sudden change in rotor angle: it's just updating one rotor variable.

#### 18.2 Why FOC Is Natural in GA

In GA, one can represent the rotor's rotation by a time-varying rotor $R_{rotor}(t) = \exp(-\frac{\theta_r(t)}{2} e_{12})$ (assuming $e_{12}$ is plane of rotation). The stator currents in stationary frame, represented as $\mathbf{i}_{\alpha\beta}$, can be "seen" in rotor frame by sandwich:
$$\mathbf{i}_{dq} = R_{rotor}(t) \mathbf{i}_{\alpha\beta} \tilde{R}_{rotor}(t)$$
which essentially does the Park transform (with $\theta_r$). If rotor angle is from encoder, we feed that in.

Now FOC says: control $\mathbf{i}_{dq}$ such that $i_q$ yields desired torque and $i_d$ yields desired flux (or is zero for surface PM). That means we want $\mathbf{i}_{dq}$ to align a certain way with rotor flux $\boldsymbol{\psi}_{r}$ (which might be mostly along d-axis if magnet or built by $i_d$ if induction).

In GA, alignment means the wedge of $\mathbf{i}_{dq}$ and $\boldsymbol{\psi}_r$ is zero (if aligned) or proportional to torque if perpendicular. So one can say: we want $\mathbf{i}_{dq}$ to equal some reference $\mathbf{i}^*_{dq}$. That can be enforced by controlling it via voltage (which is another vector in that frame).

So one rotor eq could be: $R_{control} = R_{\text{desired}} R_{rotor}^{-1}$ – basically the rotor that would rotate current into alignment with flux.

Actually: If rotor flux is at angle 0 (by definition d-axis), and we want current aligned with it (for d component), then set $i_q^* = 0$. Often, FOC implementation: measure current, transform to dq (via rotor angle), control with PI to force $i_d$ to reference (e.g. 0) and $i_q$ to reference (for torque), then transform voltage commands back to αβ and output to PWM.

This is literally composing and inverting rotors:
$$\mathbf{v}_{\alpha\beta} = \tilde{R}_{rotor}(t) [\mathbf{v}_d^* + \mathbf{v}_q^*] R_{rotor}(t)$$
since to command those $v_d, v_q$ in rotor frame, you invert Park to stator frame.

So one might say: the entire FOC loop is doing $R_{rotor}^{-1}$ on measurements, then $R_{rotor}$ on outputs. So in code, you do the Park and inverse Park. GA acknowledges that $R_{rotor}(t) R_{rotor}(t)^{-1} = 1$ so it's lossless transform except numerical stuff, and that $R_{rotor}$ being time-varying requires decoupling terms (in synchronous frame, that appears as $+\omega L i_q$ terms etc. – which GA can derive by differentiating the rotor action).

Anyway, GA doesn't change the control law results much (we still use PIs etc.), but it assures the geometric correctness. One can envision more exotic control using GA: e.g., directly command a rotor for voltage that rotates the voltage vector by some angle to achieve decoupling without computing separate d and q components – that might be an interesting GA-based controller (basically treating the complex voltage as one entity and using a complex controller that rotates outputs based on errors – which some sensorless control algorithms do with vector integrators).

So GA clarifies and potentially leads to alternative formulations (maybe for sensorless observer design, using rotors to represent error in angle and letting a estimator adapt that rotor via some gradient).

Thus, FOC is "natural" because it's just aligning rotors: one for electrical angle, one for control angle, etc. GA would handle all that elegantly if running on a platform that could do it (but we implement as usual due to speed).

## Part V: Beyond Power (surgical evidence)

### Chapter 19: Robotics—Seeing Singularities

In robot kinematics, a singularity occurs when the robot loses a degree of freedom (or ability to move in some direction) because joint axes align in a problematic way, causing the Jacobian to drop rank. Typically, one finds singularities by analyzing the determinant of the Jacobian matrix = 0. GA offers a geometric lens: the six joint screw axes of a 6-DOF manipulator, for instance, form six lines in space (for revolute joints, or lines at infinity for prismatic directions). A singularity happens when these six lines become geometrically dependent – intuitively, the volume of the parallelotope spanned by them in Plücker space goes to zero.

In GA (CGA or dual quaternions), one can compute the wedge of all six screw axes (each axis can be represented as a bivector in 3D PGA). If
$$L_1 \wedge L_2 \wedge \cdots \wedge L_6 = 0$$
then the screws are linearly dependent (singularity). If nonzero, its magnitude relates to how far from singular (the manipulability measure). Indeed, in 6D screw space, the magnitude of that 6-blade is proportional to the determinant of the Jacobian up to scale.

This GA view yields more insight: for example, if two joint axes become parallel (like elbow and wrist aligned), two of those $L_i$ wedge to zero (they produce a 4-blade instead of 6-blade), immediately telling you one less independent motion – singular. Or if three axes become coplanar (like shoulder, elbow, wrist axes in a plane), similarly a condition emerges. Each geometric relationship corresponds to a factorization of the wedge (some component dropping out).

A practical outcome: one can define a singularity index as
$$\sigma = \frac{\|L_1 \wedge \cdots \wedge L_n\|}{\|L_1\| \cdots \|L_n\|}$$
for an n-DOF robot. $\sigma = 0$ means singular. Nonzero $\sigma$ gives a normalized distance from singular. This is analogous to traditional manipulability (product of singular values of Jacobian) but derived in a coordinate-free way.

Robot designers can use GA to systematically avoid singularities: e.g., ensure that no two joint axes are ever designed to coincide for the workspace of interest (which is obvious, but GA can quantify margins). Also, in motion planning, one could incorporate $\sigma$ into the objective to keep away from singularities.

Beyond singularities, robotics uses GA for:

- **Representing poses** (motors) in a way that avoids gimbal lock (like quaternions do, but motors also cover translation).

- **Constraint solving:** If a robot end-effector must lie on a plane and point at a line, GA writes those as $\text{EndEffectorPoint} \wedge \Pi = 0$ and $\text{EndEffectorAxis} \wedge L = 0$ and solves by meet/join, rather than iterative or ad-hoc solutions.

In summary, GA doesn't replace numerical methods in robotics, but it can reveal why certain configurations are singular (all screws meeting at infinity, etc.) and provide invariant measures to avoid or detect them.

### Chapter 20: Computer Graphics—Natural Interpolation

For animating rotations (like in skeletal animation of characters), quaternions are used because SLERP (spherical linear interpolation) yields smooth shortest-path rotations. Quaternions have the quirk that $q$ and $-q$ represent the same rotation, so naive interpolation might go the long way around if you don't first ensure they are on the same "hemisphere" (dot product positive). That's a minor issue but a gotcha.

GA rotors share that double-cover property. However, if one sticks within GA's formulation, one naturally works with rotors in a continuous manner (because the exponential map yields a unique shortest arc for angle in $[0, \pi]$). Practically, any quaternion library addresses this by checking sign, so it's solved.

Where GA adds more is in combined translation+rotation interpolation (dual quaternions a.k.a. motors). Instead of interpolating orientation and position separately (which can cause unintended swerves), one can interpolate the motor directly:
$$M(t) = M_0 \exp(t \log(M_1 M_0^{-1}))$$
which yields a screw motion from pose $M_0$ to $M_1$. This is the proper interpolation in SE(3) – it results in the object moving along a helix if needed, which is usually more physically plausible than separate linear + angular interpolation (which can cause, say, orientation to lag position or vice versa).

In offline animation (like feature films), this is great; they want the best interpolation and have time to compute it. In real-time (60fps), doing a few 4×4 matrix lerps is trivial compared to a dual quaternion exp/log (which is maybe 2.6× heavier as given – though in practice dual quaternion SLERP is implemented and used in game engines for skinning, as it's still fast enough).

Another area: camera interpolation (in cinematic effects) – using GA versors ensures there is no sudden wobble (since you interpolate along a geodesic of the group).

So GA provides one framework for all interpolation:

- **Rotations** via rotors (same as quat).
- **Translations** via translators (straight line in that subspace).
- **Full 3D motions** via motors.

This can unify how an engine handles keyframes: store them as motors and always slerp (or spline) in motor space. Some 3D tools do exactly that behind the scenes.

The overhead of GA here is quite manageable (2–3× slower than quaternion+vector separate interpolation as user notes). Often the bottleneck in animation is elsewhere (skinning thousands of vertices, etc., which is done on GPU).

So in graphics, GA's acceptance has grown: dual quaternions are standard for character skinning (because they better preserve volume when blending rotations). That's essentially using a motor to blend two poses.

We likely will see more GA if AR/VR demands more dynamic motion blending – it's a niche but possibly relevant for robotics + graphics crossover (like animating a captured motion with constraints, GA can solve constraints and interpolation in one go).

In short, GA's rotor interpolation is an elegant generalization of quaternion SLERP to full rigid motions (and beyond: interpolation of lines or planes via simple formulas exists in CGA by lerping in homogeneous coordinates then reprojecting).

The trade-off remains: if extreme performance is needed (like tens of thousands of operations per frame), stick to simpler methods or GPU accelerated ones. But for most uses, the 2–3× overhead is fine for the benefits of better interpolation quality.

### Chapter 21: Machine Learning—Built-in Equivariance

Modern ML has discovered that encoding symmetries (equivariance/invariance) leads to more efficient learning. Examples: convolutional networks exploit translation invariance in images; graph neural nets exploit permutation invariance in graph nodes.

For 3D data (molecules, physical systems), we want models that are rotation-equivariant (rotate the input, the output rotates accordingly). GA provides a natural way to build such models: use multivectors as features, and build layers out of geometric products and sums. In principle, these operations automatically respect rotations.

For example, a GA layer could take as input some vectors (points positions) and outputs a multivector representing, say, distances (scalar), oriented planes (bivectors) or volumes (trivectors) depending on interactions. Because these are geometric, if you rotate the input, all these derived quantities rotate or remain invariant in predictable ways. A well-designed GA neural network could thus handle rotated versions of patterns without needing extra training data – the equivariance is baked in.

The Geometric Algebra Transformer (a hypothetical model) might represent each particle in a physical system by a multivector (embedding positions, velocities as different grade parts) and have attention layers that use GA operations so that the whole model is frame-equivariant. As the user notes, such a model learned a molecular property with 10× less data – likely referencing a result where encoding E(3) symmetry drastically reduced required samples by eliminating need to learn orientation effects.

The downside: training time was 3× longer. That could be because GA layers are more complex (thus more FLOPs per sample) and perhaps harder to optimize (more nonlinear relationships than linear layers). But in data-sparse regimes (like drug discovery – expensive to get samples), trading compute for data is a good deal.

There are indeed papers on "Clifford Neural Layers" or "Equivariant Neural Networks" that implicitly use GA (though often they use related frameworks like PoE (product of exponentials) or Lie algebra parameterizations). One specifically: "GA for Electrical and Electronic Engineers" by Chappell et al (2014) discussed an example of using GA in an ANN for power system monitoring – not exactly the same, but related trends.

So ML might incorporate GA more, especially as specialized hardware could accelerate those (if someone builds a library of geometric layers).

We're not fully there – most successful equivariant nets use more brute-force group theory (like sum over rotated copies or use spherical harmonics basis). GA is an alternative that might unify and simplify those under the hood.

Time will tell if GA becomes mainstream in ML – it depends on whether the benefits outweigh the unfamiliarity for most ML practitioners. But the results cited show potential: 10× data efficiency is huge. For high-stakes small-data problems (like medical or simulation-limited physics), that's compelling.

### Chapter 22: Crystallography—All 230 Space Groups

Crystallography enumerates 230 distinct space groups (3D symmetry groups) that crystals can have. Each is basically a set of rotations, reflections, inversions, and translations (combinations thereof) that leave a certain lattice invariant. Traditionally, each group is described by a specific set of generators in matrix form, tables of symmetry operations, etc.

In GA (specifically PGA or Conformal GA with appropriate signature to include inversion), one can represent each basic symmetry operation as a versor:

- **Rotations** as rotors.
- **Reflections** as vectors (since $-\mathbf{n}$ flips sign).
- **Translations** as translators (in PGA, or rotors in CGA with an ideal component).
- **Glide reflections or screw rotations** as products of above (which will also be versors).

Any space group element is then a product of some of these generators. Because GA multiplication is associative and closed, you can generate the entire group by multiplying these versors in all combinations (akin to generating them as a group in algebra).

Rather than coding 230 cases with 4×4 matrices, one could write a single algorithm:

1. **Input the set of generator versors** for a given group (these could be read from a standard specification, e.g., "$C_{4v}$ has a 90° rotation and a mirror plane" → encode those as $R, M$).

2. **Use GA to compute the full set of unique elements** (by multiplying generators in all needed combinations up to group order).

3. **Apply those to an initial motif** to get the full crystal structure.

This approach leverages GA's ability to handle any versor uniformly. The code doesn't change whether the generator is a rotation or screw or glide – just multiply and reduce.

Anecdotal evidence: one library by F.G. Montoya (GA-FuL) or others aimed to simplify crystallographic computations by such methods. Code reduction of 70% might come from eliminating special-case logic (like how to combine a 2-fold rotation with a translation – GA does it by simple multiplication, whereas matrix methods might need careful frame alignment). Bug reduction of 90% is plausible because those 230 sets are easy to slip up by a sign or a subtle difference; using GA with a generic algorithm ensures consistency (and one can validate each group by checking $V\tilde{V} = \pm 1$ etc., with GA making it easier to enforce normalization).

Performance being 2× slower is fine – in crystallography, generating symmetry operations is not the bottleneck (it's often computing scattering or something). And even if it were, it's small-scale (230 groups, each with finite number of elements).

Another benefit: GA can easily interpolate between symmetry operations (for exploring continuous symmetry breaking). While not typically needed in crystallography (which deals with discrete symmetry), it could assist in e.g. morphing one crystal structure into another by gradually changing a rotor's angle.

All in all, GA provides one consistent algebraic structure to encompass all classical symmetry operations – they all become "versors" in PGA or CGA. This unification is conceptually pleasing for group theory aficionados and can simplify algorithms that span multiple symmetry types (like a universal symmetry finder for a molecule could try fitting points with a versor and see if it repeats, rather than checking rotation matrices of various orders one by one).

Crystallography is a field with deeply ingrained conventional methods, so GA adoption might be slow. But as theoretical curiosity, it shows GA's reach: it naturally lives in the same space as any rigid-body symmetry, so covering all groups is straightforward.

---

## Part VI: Conditional Futures (what could change everything)

### Chapter 23: Dimensional Fluidity and Sparsity

#### 23.1 What If Dimension Isn't Fixed?

This is speculative, but mathematically one can consider extending the notion of dimension to non-integers via analytic continuation (thanks to formulas like those for $n$-sphere volume). If GA could be defined for a continuous number of basis vectors (like "2.5 vectors"), perhaps one could tune the algebra's complexity continuously.

For instance, earlier we saw how adding dimensions can represent more phenomena (like harmonics needing extra basis). But what if we could partially add a dimension? That sounds odd physically, but maybe it correlates with filtering out some interactions. Possibly a fractional dimension corresponds to only allowing certain grades to interact partially.

The formula $\frac{V_n}{S_n} = \frac{1}{n}$ for volume vs surface of n-ball holds for non-integer n via gamma functions. If we interpret an "n-ball" in GA terms (like pseudoscalar magnitude relating volumes), then continuing $n$ to non-integers is doable mathematically. Maybe a $2.5$-dimensional GA would have basis elements whose squares multiply to something like an intermediate signature? (Maybe intimately related to quaternions vs octonions in intermediate cases.)

This might allow an algebra that's sparser than 3D (which has 16 comps) but richer than 2D (which has 4 comps). If dimension = 2.5 yielded say 8 comps effectively, that's an interesting compromise. It's not clear how that would work practically – maybe through weighting certain basis contributions differently (like an algebra where $e_3^2 = 0.5$ instead of 1 or 0, effectively partially null? That might break algebra axioms though).

Alternatively, maybe mixture of metric signatures can simulate fractional effective dimension in operations count. E.g., $Cl(4,1)$ vs $Cl(3,0,1)$ – one has 32 comps, other 16, maybe in some contexts the 16 of one equate to an effective dimension of ~3.7 of the other if certain interactions are disallowed.

The main point: if one could reduce GA's component count by tailor-fitting the space to the problem (only include needed basis directions), complexity goes down. We already do that manually (blade-sparsity). A fractional approach suggests maybe smoothly turning off certain basis contributions as they become negligible (like if a system becomes nearly planar, one could treat it as "almost 2D" and simplify). Perhaps an algorithm could adjust dimension during run-time depending on needed accuracy (like model order reduction but for geometric dimension).

This is very theoretical; no known implementation. But it invites thinking outside the box: GA doesn't have to be in the given vector space – one could embed in a higher or lower space if it simplifies computation or representation, as long as results are projected back.

#### 23.2 p-Adic Geometric Algebra

p-adic numbers are a number system where "closeness" is defined by divisibility by a prime $p$. They're used in number theory and cryptography, and have some applications in modeling hierarchical or self-similar processes (ultrametric spaces).

What if distances in GA were measured in a p-adic sense? The triangle inequality becomes strong: the largest distance dominates, small differences are negligible (if two segments of different length, the longer effectively equals the sum).

If we had a p-adic GA, near-parallel vectors might be considered exactly parallel if the angle difference is small beyond a threshold (because p-adic norm would consider $\sin\theta$ maybe as divisible by something? This is a stretch). But possibly:

- Those small wedge products that cause numerical trouble (line meets nearly parallel) might be exactly zero in a p-adic metric if the difference is beyond precision. That means the GA would treat near-parallel lines as parallel (point at infinity) automatically once below a threshold. That could stabilize some computations at the cost of resolution – a sort of built-in regularization.

Another idea: representing uncertain geometry. p-adic coordinates might encode multi-scale uncertainty (like coarse location vs fine location as digits in the p-adic expansion). GA on that might allow ignoring lower significant "digits" (similar to how p-adic discards big differences, focusing on coarser structure first).

This could preserve sparsity: e.g., two lines that are almost coincident might be considered exactly coincident at one scale, simplifying logic in broad phase of collision detection, then refinements done at fine scale if needed.

Neuromorphic or analog computing sometimes exploit p-adic-like behavior (like using exponentially decayed precision). Maybe a p-adic GA implemented on analog hardware would naturally prune tiny terms (like below noise threshold) to zero, effectively reducing densification.

Again, this is far-out. But bridging number systems and GA could yield new algorithms:

- Consider solving incidence constraints using modular arithmetic or p-adic approximations to quickly detect gross structure (like two lines either obviously intersect or obviously parallel or need careful calcs if very close – you could detect "very close" via p-adic norm dropping significantly because the difference has a high power of p factor).

- Then you could treat near-parallel as exactly parallel in initial solving, then refine the solution separately if needed.

This resonates with multi-precision arithmetic: do a quick low-precision solve to see if something is degenerate, then high precision if needed. p-adic is like an alternate precision measure.

Certainly at least 10+ years away from any practical GA usage, but this shows GA could benefit from exotic math fields. Perhaps surprising cross-overs (some theoretical physics attempts to use p-adic numbers in string theory, maybe GA could unify with that in some quantum geometry context).

The mention of ultrametric prevents grade mixing is intuitive: if an ultrametric is in place, triangle degeneracies (like one side nearly sum of others) become linear in that metric. Possibly that means something like: if a bivector is much smaller (in p-adic norm) than the dot product, the geometric product might treat it effectively as zero at that scale. This could keep results "clean" (sparse) because small grade parts don't accumulate – they'd be dropped.

In summary, exploring alternative metrics (fractional, p-adic) is an open-ended frontier that could – in theory – address GA's complexity by cutting off unneeded detail. Whether that's feasible or just fantasy remains to be seen.

### Chapter 24: Hardware That Could Flip the Verdict

#### 24.1 FPGA Geometric Products

Custom hardware can accelerate GA significantly. An FPGA can be programmed to perform the 40–50 multiplications of a rotor composition fully in parallel (if enough multipliers available). A design from some research achieved a ~10× speedup for GA operations compared to a CPU, though at a cost of ~5× more power consumption (FPGAs aren't as power-efficient per FLOP as ASICs or highly optimized CPUs for general tasks, but they can beat them for specific tasks).

If such geometric units were integrated on a chip (like a "GA coprocessor" core), embedded systems could suddenly do GA without penalty. For example:

- A microcontroller with a GA unit could update a rotor 1000× faster than by software, making GA feasible in tight loops.

- A GPU with GA tensor cores could run massive GA simulations (maybe finite element on geometric objects or training GA neural nets) much faster.

We might see this if the demand grows – perhaps in AR (augmented reality), where you might want hardware to handle many 3D transformations and intersections quickly for scene understanding. If GA is recognized as a common pattern, chip designers might incorporate it (like how GPUs added tensor cores for ML when that boomed).

5× power increase might be acceptable in contexts where these computations replace heavier ones or if they operate intermittently. For instance, a self-driving car might have a GA accelerator for occupancy geometry calculations – if it speeds it up 10×, the added power might be justified for safety.

Another angle: specialized ASIC for GA in say robotics (some research teams did make ASICs for dual quaternion pose estimation). These haven't hit mainstream, but could if robotics market grows.

So yes, hardware could remove performance as a barrier. Then GA's clarity and generality can be utilized more freely, even in real-time.

#### 24.2 Neuromorphic GA

Neuromorphic hardware (like Intel's Loihi or analog neuromorphic chips) mimics brain neuron firing. They excel at parallel, event-driven tasks with associative memory.

One can imagine encoding geometric relationships in spikes:

- Perhaps represent a vector's components as phases or spike intervals across neurons.

- A bivector might be encoded by pairwise spike coincidences between two sets of neurons (like neuron pair (i,j) fires when vector component i and j spikes coincide – which could signal a specific bivector amplitude).

- A rotor could be represented by a pattern of delays between certain neuron firings – capturing an angle as a phase difference.

Multiplying multivectors might then map to detecting coincidences (like an AND logic in time domain) between spiking patterns. For instance, the geometric product formula's terms might be realized by neurons that fire only when specific pairs of input spikes overlap within a time window (computing the product of corresponding components). Summation could happen by downstream neurons integrating those spikes.

Neuromorphic systems do this kind of thing naturally for, say, Fourier transforms (phases as spike timings). Representing GA might be doable: one group did represent complex arithmetic in spikes by phase encoding.

If achieved, neuromorphic GA could perform geometric computations with extreme efficiency (the brain can do a lot with little power). It might naturally handle inexactness (robust to noise, so small grade terms might just not evoke a neuron if below threshold – again thresholding small interactions, making it sparse).

However, neuromorphic tech is still developing and mostly targeted at perception tasks. But perhaps as they look for more applications, someone will try encoding physics or control algorithms onto them. GA is a good candidate due to its parallel nature.

The user is right to say 10+ years – neuromorphic is still mostly lab stage for these uses. But if it pans out, a neuromorphic chip might compute all intersections in a scene by simply having one neuron per potential intersection and wiring it to fire if the corresponding meet conditions are satisfied by input spike patterns. That's massively parallel and could run in constant time relative to number of objects (all in parallel).

It sounds almost like science fiction, but that's how the brain likely does geometry for vision (neurons firing for certain alignment patterns etc.). So it's not totally outlandish.

In conclusion, the performance limitations that force our hybrid approach today are not fundamental. They are contingent on current computing paradigms. If and when hardware evolves – either by incremental improvements (GA units) or paradigm shifts (neuromorphic, analog) – GA could become as routine and fast as dot products are today. At that point, the careful hybrid compromises we make might become less critical: one could imagine a future engineer writing GA code directly and trusting hardware to handle it, much like we write high-level code now and trust compilers and CPUs to optimize it.

We aren't there yet, so for now we need to be savvy (as we've advised throughout) – but it's a hopeful note that GA's practicality is likely to improve over time, potentially dramatically.

---

## Part VII: Reference and Synthesis

### Appendix A: Core Mathematics

- **Reflection → Rotation (Versor theorem):** Proof that $-\mathbf{m}\mathbf{n} v \mathbf{n}\mathbf{m}$ rotates $v$ by $2\phi$ where $\phi$ is angle between planes with normals $\mathbf{m}, \mathbf{n}$. Derivation uses GA identities and results in $R = \mathbf{m}\mathbf{n}$ being a rotor.

- **Clarke and Park as Rotors:** Step-by-step derivation in GA of Clarke transform via Gram-Schmidt (showing $R_{\text{Clarke}}$ as given takes $abc$ basis to orthonormal $\alpha\beta0$), and Park transform as a rotor in $\alpha\beta$ plane. Verify their composition matches the standard matrix.

- **Power algebra:** Derive that $\mathbf{v}\mathbf{i}$ grade-0 part equals instantaneous $p$ and grade-2 magnitude equals $vi\sin$ phase (using identities $a \cdot b = |a||b|\cos\phi$, $|a \wedge b| = |a||b|\sin\phi$). Extend to three-phase showing how symmetrical components appear as separate grade contributions.

- **Meet/join duality:** Prove $(A^* \wedge B^*)^* = A \vee B$ (meet) by using that dual of join is intersection of duals etc. Provide a few examples (line-plane, plane-plane, line-line) to illustrate no special case needed.

- **Motor algebra:** Show that a motor $M = TR$ (translation times rotation) acts as expected: $MxM$ yields rotated+translated point. Expand to confirm it matches $RxR + t$ formula in coordinates.

- **Normalization and drift:** The effect of finite precision: estimate how $RR$ deviates after $n$ ops via error accumulation model (e.g., each multiply adds $\epsilon$, so after $n$, drift $\sim n\epsilon$). Justify renormalization interval chosen (say every 1000 ops for double gives $<10^{-12}$ error in norm).

*(This appendix is heavy on formulas and could include short code or pseudocode to illustrate how one computes these in practice.)*

### Appendix B: Performance Data

- **Operation counts (theoretical vs measured):** Table of FLOP counts for key GA ops (rotor compose ~88 ops, rotor apply ~128 ops, line-plane meet ~54 ops, etc.) and measured times on CPU (with and without SIMD) vs equivalent conventional ops.

- **Memory layout experiments:** Compare AoS vs SoA for multivectors on example multiply – show SoA improved cache hits significantly. Possibly show a small benchmark code listing.

- **Density growth:** Empirical plot of number of non-zero components vs multiplication depth from random initial blades (to confirm the densification percentages given in text).

- **Cache and pipeline:** Miss rate data for GA multiply vs dense matrix multiply. Perhaps an assembly or vectorization snippet to illustrate difficulty.

- **Platform specifics:** Include any interesting differences on GPU (maybe mention Steven De Keninck's result: motor comp 48 mul 40 add vs matrix 64 mul 48 add, but GPU usage was not optimal).

- **Test conditions:** mention CPU type, compiler flags, to allow readers to replicate basic performance tests.

*(Aim is to provide credibility and reference for the performance claims in Part II and Part IX.)*

### Appendix C: Convention Reconciliation

- **Phasors and GA:** Table mapping $a + jb$ in complex to $a + bI$ in GA (where $I = e_{12}$ for a plane) or to a bivector $ae_{23} + be_{31}$ etc. Example: $S = P + jQ$ corresponds to $P + Qe_{12}$ if $e_{12}$ is oriented 90° out-of-phase to define reactive axis.

- **Clarke/Park variants:** Compare power-invariant vs amplitude-invariant Clarke matrices, note GA rotor corresponds to power-invariant version (normalized).

- **Handedness and coordinates:** In graphics left-handed coordinates are common; GA can handle but one must flip sign on one basis (e.g., $e_2$) to match. Provide a cheat: if your existing system uses a certain convention, how to align GA basis (like $(e_1, e_3, e_2)$ instead of $(1,2,3)$ to mimic left-handed).

- **Electrical sign conventions:** GA is agnostic to reference direction for current; ensure consistent with passive sign convention (maybe mention that $\mathbf{v}\mathbf{i}$ yields positive $P$ if using passive sign convention, otherwise negative; so if using generator convention, interpret bivector sign flipped).

- **Dual units:** Complex power uses $j$ for quadrature; GA uses actual oriented plane (no $\sqrt{-1}$ but an $I$ that squares to -1 if using Euclidean plane). Clarify that $e_{12}$ in $\alpha\beta$ plane behaves like $j$ (commutes? Actually no, grade-2 doesn't commute with vectors, but within power calc context it pairs with scalar to mimic $j$). Possibly too deep, but at least note where GA and classical formulas differ sign or factor (like $1/\sqrt{3}$ factor in Clarke rotor vs Clarke matrix forms).

### Appendix D: Diagnostic Catalog

- **Near-parallel meet:** Provide a numeric example: line1 through (0,0,0) dir (1,0,0), line2 through (0,1,0) dir (1,0,0.001). Compute meet with GA (get a point far away). Show condition number ~1/0.001 ~1000 (consistent with angle ~0.057° giving cond ~1000). Solutions: raise precision or treat as parallel if tolerance 0.001 met.

- **Near-null blade:** Example of subtracting two almost identical lines yields a small residual line, causing ghost intersection. Advice: threshold small Plücker coordinates to zero if below measurement noise.

- **Gimbal lock in GA:** Actually absent if using rotors, but if someone stupidly uses euler angles inside GA formula, they'll still get it. Emphasize to always use rotor parameterization.

- **Normalization schedule:** Show an example where accumulating rotations without renormalizing leads to drift in orthogonality: e.g., composing 10000 small rotations of 0.018°; without renorm, final $RR = 1.00000001$. Suggest threshold $|RR-1| > 10^{-8}$ or every N multiplies. Possibly include pseudocode toggling normalization.

- **Precision pitfalls:** e.g., subtracting two large multivectors to get a small result loses precision. If needed, do operations in different order or use compensated summation. (Though GA libraries rarely do this).

- **Unit mismatches:** If mixing units (like position in meters and orientation dimensionless in a single multivector), ensure scaling such that both meaningful. GA won't stop you from adding a meter to a radian if they're just components of a multivector – it's up to user to keep track. Note to check units when interpreting components.

### Appendix E: The Library Graveyard

- **Klein:** Summarize Klein's features (SSE optimized, up to 8× faster than naive, supported PGA(3,0,1) only) and outcome (archived in 2024, due to lack of maintainers and limited adoption).

- **Versor (libvsr):** A C++ GA library by Pablo Colapinto, focus on CGA (conformal GA) for graphics. Beautiful demos (like ray tracing, procedural geometry) but not widely maintained now (last commit ~2016). Possibly note how it integrated with OpenGL and provided insight but fizzled as primary author moved on.

- **Ganja.js:** The JavaScript GA library by Enki (Steven De Keninck) – widely used in GA circles for quick demos. Not mentioned earlier, but worth listing as active project. Strength: interactive visualization, weakness: JS performance limits, but for teaching it's great.

- **GA-FuL:** A recent attempt at a general GA library in C++ templates by Eid et al (Math. 2024). Ambitious (wants to support arbitrary metric), but possibly incomplete or heavy to compile.

- **Others:** Garamon (efficient GA for small algebras via meta-programming), Cliffordan (Python lib built on sympy), etc. Mention if they're maintained.

- **Lessons:** Summarize common reasons for stagnation: lack of integration into larger ecosystems (no plugin for e.g. Unity or SciPy, so limited user base), steep learning curve for new devs, performance focus but not enough killer app to compel switch. Note successes: some concepts from GA libraries (like dual quaternion usage) slipped into mainstream without credit to GA.

Conclude that to sustain a GA project, it likely needs either corporate backing or integration into a widely used platform (so that users get benefits without themselves knowing GA – e.g., game engines using GA internally to manage rotations etc.). Without that, many GA projects end up academic curiosities despite technical merit.

*(This is partly to ensure readers know what tools exist or why they may not find a thriving GA library community in typical channels.)*

---

## Epilogue: Engineering Makes Math Honest

Geometric Algebra revealed unifying truths about physical systems – it made the math more honest by not hiding geometric relationships behind coordinate trickery. But implementing GA taught us humility: hardware realities demanded compromise and creativity, forcing us to be honest about computational cost and practical constraints. This interplay – elegant math and brute-force engineering – is at the heart of technological progress.

In power engineering (and elsewhere), we find that using GA as a design tool lets us derive cleaner, more general solutions (no special cases, clear invariants). This addresses the gap between elegance and execution concept: we get the elegance in our designs. Then, acknowledging hardware's demands, we implement those solutions in the most efficient form (be it matrices or code optimizations) – ensuring execution meets real-world requirements. That's using the hybrid architecture not as a compromise, but as the optimal solution: the right tool for the job at each stage.

Throughout this book, we saw that pattern:

- **Use GA to unify transformations** (Clarke/Park) conceptually, then realize them in code with simple multiplies.

- **Use GA to formulate power theories,** then use standard instruments (filters, etc.) to measure components.

- **Use GA to derive a new protection algorithm** (maybe via meets for faults), then implement it as conventional algebra for speed, all the while confident that GA's broad coverage means we haven't missed an edge-case.

Engineering thus "makes mathematics honest" in two senses:

1. **It forces our beautiful theories to prove themselves under real constraints** – time, memory, precision. We can't just admire an elegant formula; we must make it work in a finite, often resource-limited machine. That process often uncovers hidden complexity (like GA's densification) that the pure math didn't emphasize. It keeps us honest about what is feasible.

2. **It also ensures we use math in service of real objectives,** not for its own sake. We adopted GA not because it's novel, but because it solved multiple problems at once (unification, error reduction). And we dropped it when it didn't pay rent. Engineering mindset doesn't idolize any tool – it measures it against the mission. That pragmatic approach in this book led us to a nuanced view: GA is incredibly powerful, but must be applied judiciously, monitored, and sometimes translated or even abandoned when it doesn't serve the end goal.

Looking forward, we see that this narrow path (using GA where beneficial, using conventional methods where needed) could widen. As hardware evolves (Part VI's speculations), the trade-offs might shift in GA's favor, enabling more direct use. If that happens, readers of this book will be ready – having learned GA's full potential and its current pitfalls, they'll recognize new opportunities to deploy it as conditions change.

But even if GA remains a niche tool, the lessons here apply generally: embrace mathematical insight and respect hardware reality. The best engineers do both – they let deep understanding guide them, and then they ruthlessly optimize or simplify to get it built. GA gave us the deep geometric understanding; the hybrid approach delivered the working solution.

In the end, we have not been disappointed by GA, nor by hardware. We just had to make them shake hands and cooperate. The mathematics is more beautiful having been tested in the forge of implementation, and the implementation is more robust and general thanks to the mathematics. Each kept the other honest.

And that, ultimately, is the ethos of this book and perhaps a guiding principle for future technology: unite the powerful idea with the practical execution, without compromising either more than necessary.
