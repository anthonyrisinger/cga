# Geometry Algebra for 1% Power Engineers

*An engineering-first lens on Geometric Algebra (GA) in electric power systems—unifications that pay rent, quantified costs, disciplined decisions.*

## Front Matter

### Preface: Engineering Makes Mathematics Honest

**The Gap Between Elegance and Execution:** There's often a gulf between the beautiful generality of mathematical methods and the messy realities of implementing them in hardware and software. This book confronts that gap head-on, showing where elegant geometric formulations shine and where execution details can undermine them.

**Why Power Systems:** Electric power systems are a perfect storm of complexity—multi-domain (electrical, mechanical, magnetic), multi-rate (microsecond switching to hourly load cycles), with geometric constraints everywhere (three-phase symmetry, spatial layouts, rotating machines). If a mathematical tool can handle power systems, it can handle almost anything.

**The Hybrid Architecture:** Embracing GA doesn't mean abandoning conventional tools; it means using each where it is strongest. We advocate a hybrid approach: use GA for specification and insight (where its geometric clarity excels) but translate to matrices/quaternions/standard methods for execution when hardware demands it. This isn't compromise—it's choosing the correct tool for each job.

**What This Book Actually Does:** We teach GA by accident. By solving real power engineering problems, we gradually introduce the concepts of geometric algebra without assuming prior knowledge. The goal is that by the end, the reader has absorbed GA naturally. Along the way, we implicitly encourage building better software tools (since current GA tooling is lacking) by highlighting needs.

**Navigation Paths:** There are multiple ways to read this book, depending on your interest:

- **Mathematical Foundations → Computational Reality → Power Applications:** Start with the GA theory, see the computational challenges, then apply to power engineering problems.

- **Power Systems → Geometric Tools → Broader Implications:** Begin with familiar power systems concepts, introduce GA tools as needed, then see how those tools generalize beyond power engineering.

- **Benchmarks → Patterns → Decision Framework:** Focus on the empirical results and case studies, notice recurring patterns where GA works or fails, and culminate in a framework for deciding when to use GA in practice.

### Notation, Metrics, and Permanent Choices

**GA Models:** We primarily use Projective Geometric Algebra (PGA) $R_{3,0,1}$ for 3D Euclidean geometry (rigid motions and incidence structure), because homogeneous coordinates come naturally. We occasionally use Space-Time Algebra (STA) $Cl(1,3)$ for electromagnetic field theory, where electromagnetic bivectors live. Each algebra is introduced when it first appears, but the notation stays consistent.

**Basis Convention:** In 3D PGA, our basis is $\{e_1, e_2, e_3, e_0\}$, where $e_1^2 = e_2^2 = e_3^2 = +1$ (Euclidean directions) and $e_0^2 = 0$ (ideal or "at infinity" direction). The pseudoscalar is $I = e_{1230}$. Vectors in this algebra represent planes: for example, $e_1$ corresponds to the plane $x = 0$, and $e_0$ is the plane at infinity.

**Grade and Multivectors:** We denote the grade-$k$ part of a multivector $A$ as $\langle A \rangle_k$. Grades have specific meanings: 0 = scalar, 1 = vector (direction), 2 = bivector (oriented plane segment), 3 = trivector (oriented volume), 4 = quadvector (in 3D PGA, the "empty" or null volume element, used to signal no intersection).

**Key Operations:** The geometric product $ab$ between two vectors yields a scalar (dot product $a \cdot b$) plus a bivector (wedge product $a \wedge b$):
$$ab = a \cdot b + a \wedge b.$$

Reversion $\tilde{A}$ is the grade-reversing involution (it flips the order of basis vector multiplication, so for example $(e_1 e_2)^{\sim} = e_2 e_1 = -e_1 e_2$). The dual $A^* = AI^{-1}$ maps between outer product (OPNS) and inner product (IPNS) representations by multiplication with the inverse pseudoscalar. We will use duality to compute intersections (meets) and spans (joins) uniformly.

**PGA Specifics:** In PGA, points are represented as trivectors, lines as bivectors, and planes as vectors (this might feel inverted compared to typical Euclidean concepts, but it simplifies incidence calculations). For instance, a point located at Euclidean coordinates $(x,y,z)$ is
$$P = x e_{032} + y e_{013} + z e_{021} + e_{123},$$
a trivector in 3D PGA. A plane is $\Pi = a e_1 + b e_2 + c e_3 + d e_0$, corresponding to the plane with normal $(a,b,c)$ at distance $d$ from the origin. A line can be represented as the wedge of two planes (bivector), automatically encoding both its direction and a moment indicating position (Plücker coordinates appear naturally).

**Rotors and Motors:** A rotor $R$ is a grade-0+2 multivector (even-grade) that performs a rotation by the sandwich operation $v' = R v \tilde{R}$. In PGA, a motor (rotation + translation) is similarly represented by an even multivector $M$ that includes grade-0,2,4 parts. Rotors and motors are the GA analogs of rotation matrices and homogeneous transform matrices, respectively. We will normalize these so that $R\tilde{R} = 1$ (they lie on the unit pseudosphere in algebra). When we say an object like a rotor "acts" on another object, it's always via $X' = RX\tilde{R}$.

*(With these conventions in hand, we proceed into the main content. If you ever get lost in notation, Appendix A provides a complete reference.)*

---

## Part I: Geometric Foundations (where special cases vanish)

### Chapter 0: Power Systems Context (minimal scaffolding)

#### 0.1 Why Three Phases

Why do modern power systems use three-phase AC? The short answer is that three is the minimum number of phases that allows constant power transfer to a balanced load. With three sinusoidal phase voltages 120° apart, the instantaneous total power $p(t) = v_a i_a + v_b i_b + v_c i_c$ is actually constant over time, rather than pulsing at double the line frequency as it would in single-phase. Three phases also produce a rotating magnetic field with constant magnitude in motors, yielding smooth torque. Two phases are not enough (they would produce a pulsating field), and anything more than three has diminishing returns relative to the extra complexity and conductor material. Three is the sweet spot: it gives efficiency and performance with the simplest possible configuration. When the three-phase balance breaks (say one phase drops out or is faulty), we get unbalanced currents, unbalanced torque, and extra heating—topics we'll address with GA tools in later chapters.

#### 0.2 The Transformation Zoo

Engineers have developed a menagerie of transformations to manage three-phase systems. Two of the most famous are the Clarke transform (abc → αβ0) and the Park transform (αβ → dq). The Clarke transform projects three-phase quantities $(a,b,c)$ onto an orthogonal 2D plane (αβ) plus a zero component (0) that captures any imbalance. The Park transform then rotates this αβ plane into a d,q frame that spins with the electrical frequency, effectively turning AC quantities into DC in the steady state. By chaining these, three-phase sinusoidal voltages and currents become almost constant in the dq frame, greatly simplifying control (like how a synchronous motor controller keeps the d-axis aligned with the rotor flux).

Why bring this up in a GA book? Because these transforms are inherently geometric. In fact, Clarke and Park transforms can be derived as rotations in an abstract 2D/3D space. Clarke's transform is essentially a rotation that orthogonalizes the three-phase basis (via Gram-Schmidt) and isolates the zero-sequence component. Park's transform is a time-varying rotation in that orthonormal plane (the $e_{12}$ bivector plane in GA) by the electrical angle $\theta(t)$. In GA terms, the combined dq0 transform is one rotor (a specialized multivector) that can be seen as $R_{dq0}(t) = R_{Park}(t) R_{Clarke}$. We won't need complex numbers or 3×3 matrices at all to derive or use these transforms—they fall out from simple geometric ideas.

#### 0.3 Harmonics and Distortion Reality

Idealized power systems are blissfully sinusoidal, but reality bites: non-linear loads and other factors produce harmonics (integer multiples of the fundamental frequency) and even inter-harmonics (non-integer frequencies) in voltages and currents. A typical AC waveform might have a 60 Hz fundamental plus 3rd, 5th, 7th harmonics, etc. We quantify overall distortion by THD (Total Harmonic Distortion), essentially the ratio of the RMS of all harmonic components to the RMS of the fundamental. Why do complex numbers (phasors) struggle here? Because a single complex phasor assumes one frequency. In a distorted waveform, you'd need a separate phasor for each harmonic frequency; the nice single-plane picture of voltage vs current breaks down. GA, in contrast, can handle multicomponent waveforms more naturally by superposing vectors for each harmonic and even capturing their interactions through multivector products (as we'll explore in Chapter 4). In short, distortion introduces higher-grade components in GA that phasor algebra can't easily accommodate – a warning that we'll need advanced tools for advanced problems.

#### 0.4 Control Timing Constraints

Power electronics and real-time control add another layer of challenge. Switching converters (inverters, motor drives, etc.) use PWM (Pulse-Width Modulation) with switching frequencies on the order of 10–20 kHz. This means the control loop that updates the duty cycles has a budget of at most ~50–100 microseconds per cycle. In that time, the microcontroller must sample currents/voltages, run control algorithms (like a PID or state estimator in the dq frame), and output new gating signals. Jitter or missing a deadline can cause instability or even equipment damage. Every microsecond matters in these systems. This is why computational overhead is a critical factor: an approach that is "fine in theory" might be completely impractical if it can't run fast enough on embedded hardware. We will constantly keep an eye on algorithmic complexity and real-time feasibility as we explore GA-based solutions for power control. This is part of the honesty that engineering forces on mathematics.

### Chapter 1: Multivectors — The Type System of Geometry

#### 1.1 Objects Have Grades

One of the fundamental insights of geometric algebra is that the usual division of "scalars vs vectors" is just the beginning—geometry naturally involves higher-dimensional elements. GA formalizes this with the concept of grade. A scalar is grade 0 (just a number, no direction). A ordinary vector (like a 3D arrow) is grade 1 (magnitude and one direction). A bivector is grade 2—it represents an oriented plane segment, something with an area and an orientation (like an arrow looping around an area, or a plane element defined by two directions). A trivector is grade 3—an oriented volume element (in 3D, essentially a parallelepiped volume with an orientation). In 3D space, grade 3 is the highest (aside from the grade-4 pseudoscalar which in PGA can represent the empty set). In higher dimensions, you could keep going (4-volume, 5-volume, etc.), though those are hard to visualize.

Why does this matter? Because the algebra knows about grade and respects it in operations. If you multiply a grade-1 vector by a grade-2 bivector, the result will be a combination of grade-1 and grade-3 (since $1+2=3$ or $2-1=1$). The outcome is predictable from the grades involved. If you wedge (exterior product) two vectors (grade 1 each), you get a bivector (grade 2) representing the plane they span. If you wedge a vector (grade 1) with a bivector (grade 2), you get a trivector (grade 3) representing a volume. GA provides a built-in type system: grade = dimensionality. Operations that would be "ugly special cases" in vector calculus (like the cross product only existing in 3D, or needing separate rules for scalars vs vectors) become unified and natural in GA. We'll see this in power systems too: for example, the concept of "real power vs reactive power" will correspond to different grades of a multivector product (scalar vs bivector parts) rather than needing separate complex numbers or coordinates.

#### 1.2 Products Encode Relationships

The geometric product is the heart of GA. For two vectors $a,b$, it factors into symmetric and antisymmetric parts:
$$ab = a \cdot b + a \wedge b.$$

The dot product $a \cdot b = \langle ab \rangle_0$ is grade 0 (a scalar) and measures the magnitude projection (how much of $a$ lies along $b$). The wedge product $a \wedge b = \langle ab \rangle_2$ is grade 2 and represents the oriented area spanned by $a$ and $b$. By computing $ab$ in GA, you simultaneously get both pieces of information: the scalar dot and the bivector wedge.

This is incredibly useful. For instance, in power engineering, if voltage (as a vector) and current (as a vector) are aligned, their geometric product $vi$ has no bivector part (because $v \wedge i = 0$ when $v$ is parallel to $i$). The result is purely scalar, which corresponds to pure real (active) power. If voltage and current are perpendicular (90° phase difference), the geometric product has no scalar part ($v \cdot i = 0$), only a bivector part—this corresponds to pure reactive power (all the power is oscillating back and forth, no net transfer). For an arbitrary phase difference, $vi$ has a nonzero scalar and a nonzero bivector part, capturing both active and reactive components in one shot. In complex number notation, you'd say $S = P + jQ$, but that $j$ is an extra piece of bookkeeping. In GA, the bivector part is the reactive power, as a first-class quantity. The geometric product encodes both magnitude and orientation relationships inherently.

Another example: If a and b are orthogonal vectors, $ab = a \wedge b$ is a bivector (pure antisymmetric part). If a and b are parallel, $ab = a \cdot b$ is a scalar (pure symmetric part). GA smoothly transitions between these cases without needing if/else logic — the algebra itself handles it.

#### 1.3 First Power Application — Learning GA Without Realizing

Let's apply this to the simplest power system scenario: single-phase AC power. Traditionally, you'd represent voltage and current as phasors $V = |V| \angle \theta_V$ and $I = |I| \angle \theta_I$, and define complex power $S = VI^* = P + jQ$. Here $P = |V||I|\cos(\theta_V - \theta_I)$ is real (active) power and $Q = |V||I|\sin(\theta_V - \theta_I)$ is imaginary (reactive) power.

Now, in GA, we do something much simpler. Represent the instantaneous voltage and current as real vectors $v$ and $i$ in a 2D plane (for single-phase, a 2D geometric algebra is enough: one basis for in-phase, one for quadrature). The instantaneous power multivector is simply the geometric product:
$$S = v i = v \cdot i + v \wedge i.$$

The scalar part $v \cdot i$ is the instantaneous real power $p(t)$. The bivector part $v \wedge i$ has a magnitude equal to $|v||i|\sin(\phi)$ (where $\phi$ is the phase difference) and represents the reactive power "flowing around" between the source and load. Over a full cycle, the time-average of the bivector part is zero (no net energy transferred by the reactive component), whereas the scalar part's time-average is $P$. In GA terms, power = voltage * current, and the grade-0 vs grade-2 parts cleanly separate active and reactive power.

Now extend this to three-phase. Instead of needing symmetrical component transformation or complex 3-phase theory, we can represent the three-phase voltages as a single 3D vector $\mathbf{v}_{abc} = v_a e_1 + v_b e_2 + v_c e_3$ (with the constraint $v_a + v_b + v_c = 0$ for a balanced system, which puts $\mathbf{v}$ in a 2D subspace), and similarly $\mathbf{i}_{abc}$ for currents. Then the total instantaneous three-phase power is
$$S_{3\phi} = \mathbf{v}_{abc} \mathbf{i}_{abc}.$$

This is a multivector in $Cl(3)$ (or PGA(3,0,1) if we include $e_0$, but $e_0$ isn't needed for steady-state power calculations). The result will have a scalar part (the total real power) and a bivector part (which in 3D is dual to a vector, representing the reactive power as a pseudovector oriented along the symmetry axis of the system). If there is distortion or imbalance, there may also be a trivector part (grade 3) representing an instantaneous energy storage oscillation that doesn't neatly lie in a plane. GA captures these exotic cases seamlessly: you automatically get additional grades when the situation has more complexity (e.g., unbalanced or non-sinusoidal conditions). In complex/phasor methods, by contrast, you'd have to start inventing new concepts (sequence components for imbalance, or time-domain methods for waveform distortion). Here it's just a natural extension of the same power multivector concept.

Without even formally studying GA, we've just used it: we formulated power in terms of a geometric product, separated into grades. This foreshadows how many power engineering concepts (symmetrical components, instantaneous power theory, etc.) become simpler and more general in the language of GA.

### Chapter 2: Rotations, Reflections, and the Deep Structure

#### 2.1 The Revelation of Double Reflection

A cornerstone of geometric algebra (and geometry in general) is that any rotation can be achieved by two reflections. This is a classical result: reflecting a vector $\mathbf{v}$ across one plane (with unit normal $\mathbf{n}$) gives $\mathbf{v}' = -\mathbf{n}\mathbf{v}\mathbf{n}$ in GA (the $-$ sign and reverse order come from the reflection formula). If you then reflect $\mathbf{v}'$ across a second plane with unit normal $\mathbf{m}$, you get
$$\mathbf{v}'' = -\mathbf{m}\mathbf{v}'\mathbf{m} = \mathbf{m}\mathbf{n}\mathbf{v}\mathbf{n}\mathbf{m}.$$

Notice the pattern: $mn$ acting on the left and $nm$ (which is $\widetilde{mn}$) on the right. Define $R = mn$. Then $\mathbf{v}'' = R\mathbf{v}\tilde{R}$. This $R$ is a rotor. It's the GA representation of a rotation (in this case by twice the angle between the two reflection planes, about their line of intersection). We haven't written any matrices or done any coordinate gymnastics; the sandwich product with $R$ is the rotation. Rotors combine like quaternions (in fact, in 3D Euclidean GA, rotors are isomorphic to quaternions). If you have a rotor $R_1$ for a rotation and another $R_2$ for another rotation, the composition (do $R_1$ then $R_2$) corresponds to the geometric product $R_2 R_1$ (which is itself another rotor). This simple expression $R = m n$ (two vectors multiplied) encodes a rotation, and any rotation can be encoded this way.

The deep structure here is that GA's multiplication inherently handles reflections and rotations elegantly. Rotations are not some extra structure; they are part of the algebra's multiplicative group. In linear algebra, we usually think of rotations as matrices $R$ such that $R^T R = I$. In GA, we think of rotors as multivectors $R$ such that $R \tilde{R} = 1$. They achieve the same ends but GA's approach often simplifies derivations. We will see this when deriving the Park transform as a rotor, and when analyzing mechanisms or orientation control. (Double reflections also hint that if we can reflect something easily, we can rotate it easily. And reflections are often easier—think of how easy it is to reflect a point in a line or plane using simple formulas.)

#### 2.2 Rotors in Practice

Rotors, as noted, form a multiplicative group just like rotation matrices or quaternions. If $R_1$ rotates by $\alpha$ and $R_2$ rotates by $\beta$, then $R_2 R_1$ rotates by $\beta+\alpha$ (assuming the same axis, or more generally, it combines rotations in a single rotor). The composition of two rotors (which might correspond to combining a 3D rotation and another rotation) is efficient: for example, in 3D PGA, a general rotor (rotation in 3D) can be represented with 4 parameters (like a quaternion), and composing two rotors costs on the order of the same as a quaternion multiply. In fact, one reference implementation clocks it at 48 multiplications and 40 additions for a motor (which is a rotor with translation)—comparable to or slightly less than the cost of composing 4×4 homogeneous matrices (64 mul, 48 add) for rigid transforms. The application of a rotor to rotate a vector, however, is less efficient: computing $v' = R\,v\,\tilde{R}$ literally expands to two geometric products. If $v$ is a vector (4 components in PGA) and $R$ is an 8-component motor, that's a lot of terms to sum up. In practice it might be ~80 mul and ~60 add (roughly 140 ops) for a general 3D motor acting on a point, which is indeed heavier than a 4×4 matrix times a 4×1 vector (16 mul, 12 add). The user's earlier quip that it's "~220 FLOPs" hints at this increased cost. The takeaway: GA gives a conceptually clear but computationally dense way to apply transformations. We will later see how this density can be mitigated (through clever data layout, specialization, or by not using the generic sandwich formula in inner loops). But it's a warning that will recur: clarity often comes at a cost in raw operations.

On a more positive note, rotors have an exponential parametrization that's very powerful. Any rotor can be written as
$$R = e^{-\frac{\theta}{2}B},$$
where $B$ is a unit bivector representing the plane of rotation and $\theta$ is the rotation angle. The half-angle is there so that $R$ and $-R$ encode the same physical rotation (just like quaternions). This exponential form connects GA to Lie algebras and Lie groups: bivectors are like the generators of rotations. In practice, it means we can smoothly interpolate rotations (via the exponential and logarithm, GA's version of slerp), and it gives insight into rotational dynamics (treating $B$ like an angular velocity bivector). For power engineers, one application is in synchronous reference frame theory: the Park transformation can be seen as $R(\theta(t)) = \exp(-\tfrac{1}{2}\theta(t)\,e_{12})$, where $e_{12}$ is the unit bivector in the $\alpha\beta$ plane. Differentiating that gives a bivector (lie algebra) element $-\frac{1}{2}\dot{\theta} e_{12}$, which corresponds to electrical angular velocity. So even the familiar $\omega t$ in $e^{j\omega t}$ has its GA analogue in the exponent of a rotor. Keep this exponential form in mind; it often turns complexity (compose this, then that) into simple addition in the exponent.

#### 2.3 Motors — Translation and Rotation Unified

A big advantage of using Projective GA (PGA) is that translations become simple parts of the algebra, on equal footing with rotations. In PGA $R_{3,0,1}$, a motor is a rotor in 3D extended by the ideal dimension $e_0$. Rotating in a plane that includes $e_0$ produces a translation. In effect, a motor = rotation + translation unified. If $T = 1 + \frac{1}{2}te_0$ represents a translation by vector $\mathbf{t} = (t_x, t_y, t_z)$ (here $e_0$ plays the role of the plane at infinity), and $R$ is a rotor for a rotation, then the motor for doing $R$ followed by translation $\mathbf{t}$ is just $M = TR$. It's still an 8-element multivector. It still combines via multiplication: if $M_1$ is "move then rotate" and $M_2$ is another "move then rotate", then $M_2 M_1$ is the combined rigid motion. And motors act by the same sandwich formula: $X' = M X \tilde{M}$. No coordinate matrices needed, no special cases for "apply rotation then add a translation offset" as in vector algebra — it's one operation.

For a power engineering example, think of a transformer that both shifts voltage magnitude (translation in the voltage dimension) and phase (rotation). A phase-shifting transformer could be represented as a motor in an abstract voltage-current space: part of its effect is a rotation (phase change), part is a translation (voltage magnitude change). While we won't deeply explore that particular modelling, it illustrates that GA doesn't artificially separate these effects. The "screw" nature of spatial motions (rotation + translation) has a clear counterpart in power systems: changing magnitude + phase. This unity is very elegant. When we do need to work with rigid motions (for example in Chapter 16 on fault location, lines intersecting ground, etc.), motors will let us compute intersections and movements without having to treat translation as a separate case. It just falls out of the algebra.

#### 2.4 The Normalization Problem

When using rotors and motors in numerical computations, one practical issue is normalization drift. Ideally, if $R$ is a rotor, it should always satisfy $R \tilde{R} = 1$ (which is analogous to a rotation matrix being orthonormal or a quaternion being unit length). However, due to floating-point rounding, if you compose many rotors or integrate differential equations to update a rotor, you'll find $R \tilde{R}$ creeping away from 1. For example, you might start with a rotor representing 0° rotation (which is $R=1$ in GA). After 100 successive small rotations, numerical error might make $R \tilde{R} = 1.00000000000001$ (a tiny deviation). After 10,000 operations, maybe $R \tilde{R} = 1.0000000001$. After 1,000,000 operations, perhaps $R \tilde{R} = 1.0000001$ (these are illustrative numbers). The deviations tend to accumulate because each multiplication or integration step has slight error. If the rotor strays too far from unit length, it no longer represents a pure rotation (it'll encode some unwanted scaling).

The solution is to renormalize periodically: compute $R \leftarrow R / \sqrt[n]{R \tilde{R}}$ or some equivalent method to project it back to the "unit pseudosphere". In practice, normalizing a rotor or motor might cost, say, 8 multiplications and 7 additions (~15–20 FLOPs) plus a reciprocal square root. Doing this every single time is overkill, but doing it never is dangerous. A common strategy is to renormalize after a certain number of compositions or if $|R \tilde{R} - 1|$ exceeds a threshold (e.g. $10^{-8}$). In highly sensitive simulations (like long-running dynamics), you might normalize every few milliseconds of simulation time. In a real-time controller, you might never explicitly renormalize if using quaternions (since sensors help re-orthonormalize via feedback), but in pure simulation GA contexts, you will. We will mention normalization needs when we present algorithms using rotors (for instance in the PLL rotor in Chapter 18 or the EKF in Chapter 7). It's one of those implementation details that the math textbooks don't prepare you for, but engineers must handle.

### Chapter 3: Incidence — Where Lines Meet

#### 3.1 The Projective Model Makes Parallels Meet

Euclidean geometry has a notorious annoyance: parallel lines never meet, which complicates algorithms (you have to check for parallelism as a special case in intersection computations). Projective geometry elegantly fixes this by adding "points at infinity" where parallel lines do intersect (at an ideal point). PGA is essentially algebraic projective geometry. By including the null basis vector $e_0$ (with $e_0^2 = 0$), we acquire a representation for ideal elements. For example, in 3D PGA:

- **Points** are represented as trivectors $P = x e_{032} + y e_{013} + z e_{021} + w e_{123}$. If $w \neq 0$, this corresponds to an actual point at finite coordinates (proportional to $(x/w, y/w, z/w)$). If $w = 0$, the point is ideal (at infinity) in some direction.

- **Lines** are bivectors. A line can be thought of as the join of two points or the intersection of two planes. A general line bivector can be written as
$$L = l_{01} e_{01} + l_{02} e_{02} + l_{03} e_{03} + l_{23} e_{23} + l_{31} e_{31} + l_{12} e_{12},$$
where the $e_{ij}$ terms with a 0 (like $e_{01}, e_{02}, e_{03}$) capture direction (the line at infinity aspect) and the terms without 0 (like $e_{23}, e_{31}, e_{12}$) capture the moment (how far the line is from the origin).

- **Planes** are grade-1 (vectors) as mentioned: $\Pi = a e_1 + b e_2 + c e_3 + d e_0$. If $d \neq 0$, it's a finite plane (the $d$ essentially is the distance from origin times the normal). If $d = 0$, it's an ideal plane (which in PGA corresponds to a direction like a set of all points "at infinity" with a given orientation).

This representation might feel weird (planes as vectors?), but it works beautifully. A point is now the intersection of three planes: $P = \Pi_1 \wedge \Pi_2 \wedge \Pi_3$. Two lines meet at a point: $P = L_1 \wedge L_2$ (this will be a trivector result). Parallel lines in Euclidean space become lines that intersect at an ideal point (which PGA represents as a trivector with $w = 0$). No more "if parallel then do something else" – it's just a wedge that yields an ideal point if they don't meet in the finite plane. "Parallels meet at infinity" is built-in. This uniformity eliminates edge cases and gives polynomial formulas for everything (no divisions by zero for special cases).

#### 3.2 Meet and Join — Intersection Without Cases

In GA, the meet of two objects is their intersection, and the join of two objects is the object that they both lie in (span). Using duality, these can be computed with a single algebraic formula. If $A$ and $B$ are represented in OPNS form (like our $P, L, \Pi$ above), the meet is:
$$A \vee B = (A^* \wedge B^*)^*,$$
which looks scary but it means "take the duals (to go to IPNS, where containing becomes spanning), wedge them, then dual back." The join is simpler: $A \wedge B$ (in OPNS, wedging adds the subspace together).

What's important is that these operations always work in PGA, regardless of special cases:

- **Two skew lines** (non-parallel, non-intersecting in 3D) have a meet that is the "empty" element (a null pseudoscalar, grade 4). That signals "no intersection."

- **Two parallel lines** (in a plane) have a meet which is an ideal point (a direction vector, grade-3 trivector with $w = 0$). That's the point at infinity in the direction of both lines.

- **A line and a plane** meet in a point (if the line lies in the plane, the meet will actually be a line – specifically, the same line – because the wedge of the plane's dual and line's dual would drop a dimension and dual back to the line; but that's fine, it correctly captures that the whole line is the intersection).

- **Two planes** meet in a line (unless they are parallel, in which case the meet is an ideal line at infinity).

- **Three planes** generically meet in a point (if they don't all intersect in one point, the result might be an ideal point or the empty set if no common intersection exists).

No special-case logic is needed: the algebraic formulas yield a blade (a simple multivector) that represents the correct geometric object or the null object if no intersection. We can check for an "empty intersection" by seeing if the result has grade equal to the space dimension (in 3D PGA, a result in grade 4 indicates no physical intersection). This consistent treatment is a big win when coding up things like fault analysis (where a fault may be the intersection of elements in a network) or calculating the point of intersection between a line of sight and an equipment clearance zone, etc. We'll see examples of meet/join in power contexts in Chapter 16.

#### 3.3 Computational Reality of Incidence

Of course, all this algebraic elegance comes at a computational cost. The meet formula involves duals and a wedge. A naive count for a meet of two general 3D PGA entities (say a line and a plane) might be on the order of 64 FLOPs (floating-point operations) if fully expanded. In practice, with an optimized library, you might see ~95 operations because of some overhead and not all terms needing computation (some are zero due to structure). It's more expensive than solving linear equations by Gaussian elimination for the same intersection, roughly.

A bigger issue is numerical conditioning. When lines or planes are nearly parallel, the meet (intersection point) will be far away, and that tends to amplify floating point error. For example, if two lines intersect at 89° (almost perpendicular), the meet is well-conditioned. If they intersect at 1° angle, the condition number might be about 57 (meaning you lose maybe $\log_{10}(57)$ ~ 1.75 digits of precision in the result). At 0.1° difference, condition ~573; at 0.01°, ~5730. This aligns with the idea that as lines become parallel, the intersection point moves off to infinity, and a small perturbation in inputs can move the intersection a huge distance. In GA, you'll get a point with large homogeneous coordinate $w$ (indicating it's near ideal) and possibly large rounding error.

To handle this, robust algorithms include near-parallel detection: if the meet's homogeneous coordinate $w$ is below some threshold relative to the other coordinates (meaning the intersection is very far or ideal), we treat it as "effectively parallel" or we handle it with higher precision. Another approach is to normalize line representations before computing intersections, but that only helps so much. We will collect a set of countermeasures for such cases (like adding a tiny tweak to bring lines to exactly parallel and returning an ideal line, etc.) in Chapter 3.4 and Appendix D.

#### 3.4 The Counterexample Collection

No new methodology is complete without a few pathological examples to keep us honest. Over time we've gathered a "zoo" of scenarios where GA incidence calculations can misbehave if you're not careful:

- **Near-parallel lines:** Two lines that should intersect kilometers away. Computing their meet yields a point with huge coordinates (maybe beyond the range of a float to represent). Numerically, this is very sensitive. *Mitigation:* if the angle between lines is below a threshold (based on the magnitude of $L_1 \wedge L_2$ compared to $L_1$ and $L_2$ lengths), decide that they have "no finite intersection" for practical purposes, or use double precision (or higher) for that calculation.

- **Near-coincident planes:** Three planes that almost meet at a single point but actually form a tiny triangular prism. The meet of two and then meet with third will produce a point that is wildly off due to amplification. *Mitigation:* compute all three meets pairwise and check consistency, or do a least-squares intersection.

- **Null or nearly null blades:** Sometimes an operation should yield the zero multivector (like intersecting two identical lines should give that same line as the "intersection", but in meet terms that line's representation might be a degenerate blade). Numerical rounding might produce a very small non-zero result instead of exact zero, leading you to think there's an intersection when it's actually the same line (or conversely, thinking two lines intersect because the result isn't exactly null). *Mitigation:* enforce tolerance-based zero detection – e.g., if a result's magnitude is below 1e-12 times the product of operand magnitudes, consider it zero.

- **Triple intersection and rank issues:** If you intersect three planes that nearly meet at a point, you might get three pairwise intersection lines that nearly converge. Which two do you intersect first? The answer could change the numeric result. *Mitigation:* perhaps solve via linear system rather than sequential meets, or use a symmetric formula.

- **Ideal interplay:** Intersecting something that's already ideal (like a line at infinity) with something finite. Usually, the meet will naturally give ideal results, but one must interpret them correctly (the intersection of a finite line and an ideal line could be an ideal point that corresponds to the direction of both lines).

We'll see specific instances of these throughout. The key point is: GA doesn't eliminate numerical issues—those are fundamental—but it often lets us detect and handle them more uniformly (the grade or magnitude of a result tells us a lot). Part of "engineering honesty" is openly cataloguing these pitfalls and ensuring our algorithms can detect when they're in a dangerous corner of the algebra.

---

## Chapter 4: Power Through Geometric Eyes

#### 4.1 Clarke and Park Transforms are Rotors

Earlier, in Section 0.2, we described the Clarke (abc→αβ0) and Park (αβ→dq) transforms in classical terms. Now let's describe them in GA terms explicitly. The Clarke transform for a three-phase system can be seen as a fixed rotation that takes the basis $(e_a, e_b, e_c)$ of phase coordinates into a new basis $(e_\alpha, e_\beta, e_0)$ where $e_\alpha, e_\beta$ are orthonormal in the plane of the three-phase system, and $e_0$ is orthogonal (capturing the zero-sequence component). One GA rotor that does this is:
$$R_{Clarke} = \frac{1}{\sqrt{3}}(1 + e_{31} + e_{12}),$$
which indeed performs the rotation: it's constructed such that $R_{Clarke}(e_a)\tilde{R_{Clarke}} = e_\alpha$, $R_{Clarke}(e_b)\tilde{R_{Clarke}} = e_\beta$, and $R_{Clarke}(e_c)\tilde{R_{Clarke}} = e_\alpha + e_\beta - 2e_0$ (or some combination that yields the zero component appropriately). The exact form of the rotor isn't crucial to memorize; what matters is: Clarke's transform is a rotation in a 3D space (the space of three-phase voltages). In practice, we still often implement Clarke as a matrix multiply for efficiency, but knowing it's a rotor means we can consider it part of a single geometric operation when combined with Park.

The Park transform is even more clearly a rotor. It's a rotation in the αβ-plane by the negative of the system electrical angle θ (negative because we typically go to a rotating frame that cancels out the forward rotation of the system). In GA, that's
$$R_{Park}(\theta) = \exp\left(-\frac{\theta}{2}e_{12}\right),$$
which is a rotor that rotates any vector in the $e_{12}$ (αβ) plane by $-\theta$. Applied to $e_\alpha$ and $e_\beta$, it produces $e_d$ and $e_q$ axes that are fixed in a frame rotating with the machine. So the combined dq0 transform is:
$$R_{dq0}(\theta) = R_{Park}(\theta) R_{Clarke},$$
and for any three-phase voltage vector $\mathbf{v}_{abc}$, the transformed version in dq0 coordinates is
$$\mathbf{v}_{dq0} = R_{dq0}(\theta) \mathbf{v}_{abc} \tilde{R_{dq0}(\theta)}.$$

Because $R_{dq0}$ is a rotor (indeed a proper time-varying rotor), this transform preserves magnitudes (aside from the scaling choices like $1/\sqrt{3}$ which we included for orthonormality) and preserves the geometric structure of the signals. In GA language, Park's trick of making AC look like DC is literally aligning a rotating bivector with a fixed reference.

Why is this helpful? Conceptually, it means we can reason about reference frame changes as rotations. For example, a 30° phase shift in all three phases (like a delta-wye transformer between a generator and transmission line) can be treated as adding a fixed rotor before the Park transform. Or, an unbalanced system where the "Clarke rotor" is slightly different because the $abc$ sum isn't zero can be analyzed by adding a small component in the $e_0$ direction to the rotor. Montoya et al. have shown that imposing orthonormality via GA rotors yields Clarke and Park naturally. We thus demystify these transforms: they were always geometric rotations; GA just makes that explicit.

*(Historical note: Edith Clarke and R.H. Park weren't thinking about GA, of course. They were doing linear algebra. But GA provides a unifying interpretation that enhances our intuition.)*

#### 4.2 Instantaneous Power as Geometric Product

We introduced the idea of geometric power $S = vi$ in Chapter 1. Let's deepen that in the three-phase context using our new rotor tools. Suppose we have three-phase instantaneous voltages and currents (could be balanced or not, sinusoidal or not). Form the 3D vectors $\mathbf{v}(t) = v_a(t)e_1 + v_b(t)e_2 + v_c(t)e_3$ and $\mathbf{i}(t) = i_a(t)e_1 + i_b(t)e_2 + i_c(t)e_3$. Then define
$$S(t) = \mathbf{v}(t) \mathbf{i}(t),$$
which is a multivector. What are its components?

- **Grade-0 (scalar) part:** $P(t) = \frac{1}{2}(\mathbf{v}\mathbf{i} + \mathbf{i}\mathbf{v}) = \mathbf{v} \cdot \mathbf{i}$. This is exactly the instantaneous total real power (sum of phase-by-phase products). Over a cycle, the average of $P(t)$ is the real power delivered to the load. If the system is balanced and purely fundamental, $P(t)$ will be constant in time (for a balanced load, each phase's oscillation in power cancels out in sum). In unbalanced or harmonic cases, $P(t)$ can oscillate at twice the fundamental or other frequencies, indicating real power oscillations. But $P(t)$ is a single scalar function representing all the "work being done" at each instant.

- **Grade-2 (bivector) part:** $\mathbf{Q}(t) = \frac{1}{2}(\mathbf{v}\mathbf{i} - \mathbf{i}\mathbf{v}) = \mathbf{v} \wedge \mathbf{i}$. In 3D, a bivector is dual to an axial vector (pseudovector) which here would align with $e_{123}$ (the axis perpendicular to the plane of $\mathbf{v}$ and $\mathbf{i}$). In a three-phase balanced system, $\mathbf{v}$ and $\mathbf{i}$ lie in the $e_1e_2e_3$ 2D subspace (since $v_a + v_b + v_c = 0$ for a balanced three-wire system, same for $i$), so the bivector is something like $Q e_{123}$, effectively a scalar $Q$ times the unit pseudovector $e_{123}$. That scalar $Q$ corresponds to the instantaneous reactive power flowing (with sign indicating direction of quadrature energy flow). Montoya et al. define this bivector part as "non-active power". If you average $Q(t)$ over a full cycle, you typically get the conventional reactive power (e.g., for sinusoids $Q = V I \sin\phi$). If $\mathbf{v}$ and $\mathbf{i}$ are not purely single-frequency, $Q(t)$ can have components at various frequencies too.

- **Grade-1 or Grade-3 parts:** In a balanced sinusoidal case, $\mathbf{v}$ and $\mathbf{i}$ lie in a 2D subspace, so $\mathbf{v}\mathbf{i}$ has only grades 0 and 2. But with distortion (harmonics) or imbalance, $\mathbf{v}$ and $\mathbf{i}$ might not lie in the same 2D subspace at each instant. This can introduce other grades. For example, if $\mathbf{v}$ has a third-harmonic component that $\mathbf{i}$ does not, then $\mathbf{v}\mathbf{i}$ could have a trivector (grade-3) part or even a grade-1 part indicating some coupling that is neither a pure scalar nor pure bivector. Those higher-grade parts correspond to more exotic power flows: e.g., a trivector part in 3D might indicate an instantaneous energy storage oscillation that has no net transfer in any plane (like a rotating energy flow that cycles among phases). In classical power theory, these show up as terms that aren't accounted for by $P$ and $Q$, sometimes called distortion power or non-fundamental power. GA reveals them naturally as just the leftover grades. In fact, in an unbalanced or non-sinusoidal scenario, you could define geometric instantaneous power and then grade-project to get definitions of $P$, $Q$, and other components (this has been explored in recent literature).

The bottom line: using the geometric product to define power means we don't have to choose a reference frame or sequence components first. The multivector $S(t)$ encapsulates all instantaneous power information. If you rotate the reference frame (say using a different phase basis or using the $dq0$ frame), $S$ itself transforms nicely (as $S' = R S \tilde{R}$), and the scalar part $P$ and bivector magnitude $Q$ remain invariant – they are objective physical quantities. Only their coordinates (like how $Q$ is expressed as a bivector) might change. This frame-invariance of the power decomposition is a strong argument for GA: no more arguing about "Fryze power theory vs p-q theory vs IEEE 1459" – they are all seeing different slices of the same GA truth. We can recover all those with appropriate projections from $S(t)$, if needed.

#### 4.3 Harmonics Create Higher Grades

Let's push the harmonic distortion idea a bit further. Suppose $\mathbf{v}(t)$ has a 5th harmonic and $\mathbf{i}(t)$ has a 7th harmonic component, just as an example. Write $\mathbf{v} = \mathbf{v}_1 + \mathbf{v}_5$ (fundamental + 5th) and $\mathbf{i} = \mathbf{i}_1 + \mathbf{i}_7$ (fundamental + 7th), ignoring others for now. Then expand the geometric product:
$$S = (\mathbf{v}_1 + \mathbf{v}_5)(\mathbf{i}_1 + \mathbf{i}_7) = \mathbf{v}_1\mathbf{i}_1 + \mathbf{v}_1\mathbf{i}_7 + \mathbf{v}_5\mathbf{i}_1 + \mathbf{v}_5\mathbf{i}_7.$$

- The term $\mathbf{v}_1 \mathbf{i}_1$ is the normal fundamental power (with scalar and bivector parts at 0 Hz and 2*60=120 Hz oscillation respectively).

- $\mathbf{v}_5 \mathbf{i}_7$ is interesting: it involves 5th * 7th harmonic. In the time domain, that product will have components at 5+7=12 times fundamental (720 Hz) and at |5-7|=2 times fundamental (120 Hz) due to trig identities. In GA, the 12th harmonic interaction might manifest as a higher-grade component because effectively $\mathbf{v}_5$ and $\mathbf{i}_7$ are in different time-phase planes. It could produce, say, a trivector part (if one is trying to rotate faster than the other, an out-of-phase volumetric effect occurs). The 2nd-harmonic (120 Hz) part could add to the same bivector grade as fundamental reactive power oscillations. So you get cross-frequency coupling terms.

- $\mathbf{v}_1 \mathbf{i}_7$ and $\mathbf{v}_5 \mathbf{i}_1$ similarly produce odd frequency mixtures.

The precise grade bookkeeping can get complicated, but qualitatively: the presence of multiple frequencies means $\mathbf{v}$ and $\mathbf{i}$ occupy a higher-dimensional space (time-frequency space), and their geometric product will accordingly have higher-grade components. In a 3D GA model, something that "doesn't fit" the 2D subspace of fundamental frequency will appear as a grade-3 trivector or grade-1 vector part of $S$. If we tried to interpret everything with complex numbers, we'd be lost because complex can only represent one frequency pair at a time. Instead, people resort to doing FFTs and computing power per harmonic, etc. GA instantaneous power gives a unified object that, when analyzed, reveals all those interactions in one framework. In practice, for measurement, we still might compute an FFT of $S(t)$'s components to categorize the oscillatory energy at different frequencies. But it's appealing that $S(t)$ contains not just "the" power, but all the inter-harmonic power flows as well, without additional definitions.

*(One could extend to a 6D GA to explicitly separate multiple frequency components, but that might be overkill. The 3D GA with time dependence usually suffices by treating time like an external parameter. Some research is exploring multi-frequency GA models for power, though.)*

---

## Part II: Computational Reality

### Chapter 5: The Densification Catastrophe

#### 5.1 How Sparse Becomes Dense

In theory, GA looks like it might be efficient. Many geometric operations are just additions and multiplications of a few components. But a lurking issue is grade proliferation: as you multiply GA elements, even if each started sparse (only a few grade components present), the result tends to fill out more and more components. This is densification.

Consider a concrete example. Start with two 3D vectors $\mathbf{v} = v_1 e_1 + v_2 e_2 + v_3 e_3$ and $\mathbf{w} = w_1 e_1 + w_2 e_2 + w_3 e_3$. Each is "sparse" in the sense of only having grade-1 components (3 each, out of 16 possible GA components in 3D PGA, which includes grade-0 through grade-4). Now compute the geometric product $\mathbf{v}\mathbf{w} = \mathbf{v} \cdot \mathbf{w} + \mathbf{v} \wedge \mathbf{w}$. The dot $\mathbf{v} \cdot \mathbf{w}$ is a scalar (grade 0), and $\mathbf{v} \wedge \mathbf{w}$ is a bivector (grade 2). So $\mathbf{v}\mathbf{w}$ has grade 0 and grade 2 parts. That's already a bit "denser" (more grades) than either original vector. Now multiply this result by another vector $\mathbf{u}$. We do $(\mathbf{v}\mathbf{w})\mathbf{u}$. Distribute: $\mathbf{v}\mathbf{w}\mathbf{u} = (\mathbf{v}\cdot\mathbf{w})\mathbf{u} + (\mathbf{v}\wedge\mathbf{w})\mathbf{u}$. The first term $(\mathbf{v}\cdot\mathbf{w})\mathbf{u}$ is just a scalar times a vector, which yields a vector (grade 1). The second term $(\mathbf{v}\wedge\mathbf{w})\mathbf{u}$: here we're multiplying a grade-2 by a grade-1. The geometric product of a bivector and a vector yields grade-1 plus grade-3 (because $2 + 1$ can be 3 via the wedge part, and $2 - 1 = 1$ via the dot part when reversing order). So $(\mathbf{v}\wedge\mathbf{w})\mathbf{u}$ will have a vector part and a trivector part. By the end of this, $\mathbf{v}\mathbf{w}\mathbf{u}$ has components in grade 1 and grade 3 (from the second term) and possibly additional grade 1 from the first term, so collectively grade 1 and 3. Now if we multiply by one more vector, almost certainly we'll fill in grade-2 as well. After enough multiplications, you approach a full multivector (all grades populated). In Cl(3,0) or PGA(3,0,1), that means up to 16 non-zero components.

In big-O terms, if you multiply $n$ vectors in an $m$-dimensional GA, the result can have up to $\sum_{k=0}^m \binom{m}{k} = 2^m$ components (the dimension of the algebra). Often by $n \approx m$, you're nearly full. For 3D, $2^4 = 16$ is max components (since PGA has a 4-dimensional basis including $e_0$). The multiplication algorithm itself in a naive form is $O(2^{2m})$ – like $O(256)$ ~ constant for 3D, but the constant can be large in higher dims. Practically, as soon as things get slightly complicated, your nice sparse representations blow up.

This is the densification catastrophe: start with sparse, end with dense after a few operations. If you do a sequence of operations (say a cascade of transformations, intersection computations, etc.), intermediate results balloon in component count, making each subsequent multiplication heavier. We saw above:

- **Depth 1** (one multiplication): 2 grades out of 16 in result (~25% of components possibly non-zero).
- **Depth 2** (two multiplications in a row): maybe ~50% filled.
- **Depth 3:** ~75% filled.
- **By depth 4 or 5:** 90–95% filled.

Beyond a certain point, virtually every basis element can appear. When that happens, multiplying two multivectors costs nearly $2^m \times 2^m$ operations (like multiplying two dense matrices of half the size). In 3D, that's up to 16×16 -> 256 terms (though many combine or cancel due to structure). In 4D, 32×32->1024 terms. This is why GA libraries often precompute and hard-code specific products to reduce overhead: the general formula might do a lot of unnecessary zero multiplications if the multivectors are sparse, but if they are actually dense, you hit the worst-case cost.

For the power engineer or programmer, this means: If you try to naively use GA for a long chain of computations (like repeatedly updating a state by geometric products or performing iterative solves), the cost per operation tends to rise as intermediate results become more complex. It's not as simple as each step being a fixed cost unless you can keep things sparse. This is one reason we are cautious about using GA in inner loops of real-time algorithms.

#### 5.2 Memory and Cache Violence

Computational complexity isn't the only issue; memory access patterns matter a lot on modern hardware. A 3D PGA multivector has 16 components. In double precision, that's 128 bytes. Most CPU cache lines are 64 bytes. So a single multivector doesn't even fit in one cache line; it straddles two. If you're streaming through arrays of multivectors, you'll often fetch two cache lines for one multivector, effectively doubling the memory bandwidth needed. Contrast this with a typical 3D vector (3 components, 24 bytes) which sits well within one cache line (and you can often pack several such vectors into one line).

Now consider multiplying two multivectors. The typical formula will take each component of one and multiply by each component of the other with a certain sign, summing into result slots. This leads to a lot of non-contiguous memory access:

- You read component $a_i$ of multivector A (somewhere in those 128 bytes).
- You read component $b_j$ of B.
- You write to component $c_k$ of result C.

The indices $i, j, k$ follow the pattern of the multiplication table for the algebra, which is not simply sequential. It might be something like: to compute result's $e_{13}$ component, you need A's $e_1$ and B's $e_3$ and subtract A's $e_3$*B's $e_1$ etc. The effect is that you are jumping around in the arrays of components. There is little spatial locality (whereas a matrix multiply can stride nicely through memory, especially if optimized). The GA multiply resembles a convolution of two vectors of length 16 under a specific pattern – not as nice as a dot product or so.

Empirical data (from tests on GA libraries) show cache miss rates can be high. Suppose you have an algorithm that multiplies many multivectors in sequence. If each multiply has to fetch two 128-byte multivectors and produce one 128-byte result, that's 384 bytes touched, and likely a lot of those bytes are in different cache lines if data isn't aligned or if it's interleaved. If the access is random or striding badly, caches can't prefetch effectively. Compare: a dense matrix multiply can stream through memory with near 100% cache utilization (accessing contiguous blocks). GA multiplies often jump per component. One estimate is that while an optimized dense linear algebra operation might have cache miss rates around 2–5%, a naive GA multiply could see 30%+ cache misses. That destroys performance, because each miss stalls the CPU waiting for main memory (which is ~100x slower than registers or L1 cache).

To quantify, consider: a modern CPU might do, say, 10 GFLOPs on a tight loop that fits in L1. But if each operation triggers a memory fetch, you might drop to 0.5 GFLOPs equivalent throughput. That's not a small drop—that's an order of magnitude. When the user earlier said a 17× difference in cache behavior, it's this kind of effect. Indeed, if a GA multiply effectively causes 17 times more cache misses than a comparable conventional algorithm, the observed performance can be an order of magnitude worse than expected from pure FLOP counts.

The lesson is that data layout and access patterns are crucial for GA. We might need to reorganize multivector data in memory to be cache-friendly (e.g., structure of arrays vs array of structures, or blocking, or compressing out zeros, etc.). Some GA libraries use contiguous arrays of components (size 16) which is fine, but as noted 16 is awkward for cache. One idea is to split into two 8-component halves (like separate arrays for even and odd grade parts), so each fits in 64 bytes. This matches how some GPU implementations pack motors as two 4D float4 vectors. We'll discuss mitigation in next section.

#### 5.3 Mitigation That Almost Works

Engineers have tried various tricks to tame the densification and memory issues of GA:

- **Blade-sparse representations:** If you know your multivectors will only contain certain grades, you can store only those. For example, rotors are grade-0+2 only (4 components in 3D), motors are grade-0+2+4 (8 components). Storing only those saves space and multiplies faster (like multiplying dual quaternions instead of full 4x4 matrices). The problem is when an operation produces an out-of-type result. For instance, multiplying a rotor by a general line (a bivector) yields a result that might not be a pure rotor or pure line—it could be a sum of different grade parts. If your system can guarantee certain types (closed under operations), blade-sparse storage works well. But in a general computation, you eventually have to promote to the full representation. Still, in specific domains this is powerful (e.g., in computer graphics, many algorithms only use motors, lines, points; Klein library exploits this heavily).

- **Factorized forms:** Sometimes you can keep an object as a product of simpler objects rather than expanding it. For instance, instead of storing a motor as 8 numbers, store it as a rotor (4 numbers) and a translation vector (3 numbers). Or store a rotor as two unit quaternions if you had them (just conceptually). Then you can compose by composing the factors (e.g., multiply rotors and add translations with rotation). This can save operations because you're not constantly recomputing all 8 components if you only need final results. The downside is that some operations become more complex (applying a motor factorized may require applying rotor then translation separately, which might not vectorize as nicely as one fused operation). Also not all objects factor neatly, and numerical stability of factors can be an issue (you might need re-orthonormalize the rotor factor occasionally).

- **Code generation (symbolic optimization):** Tools like GAALOP analyze a given GA expression pipeline and generate optimized C code that only computes the needed components in the minimal way. If your computation is known ahead of time (e.g., a specific formula for the intersection of two lines and then applying a rotor), the generator can simplify and remove redundancies. This often yields very efficient code for that specific task. The catch is if your problem changes or you need a general solver, you can't easily maintain a huge library of custom-generated kernels for every scenario. It's great for fixed algorithms (you see this in some GA robotics papers: they auto-generate the kinematics code).

- **Lazy evaluation:** This is more theoretical, but one could imagine deferring the expansion of a multivector until you need certain parts. For example, if an upcoming operation only needs the scalar part of a result, you don't compute the bivector part fully. You keep an expression tree and evaluate on-demand. This can save work in some cases (like symbolic algebra systems do). However, implementing this at runtime in C++ or similar is extremely complex and would probably lose any performance gains to overhead unless carefully done.

- **Mixed representation:** Another approach: use GA for conceptual clarity and high-level operations, but under the hood convert certain operations to matrix or vector operations when they would be cheaper. For example, rotating a point by a motor can be done by the sandwich formula or by converting the motor to a 4x4 matrix and doing a matrix multiply. On a GPU, that matrix multiply might actually be faster (using tensor cores) than doing the GA formula. A smart library could detect "ah, this is just a rigid body transform" and use a faster path. This hybrid approach means you need parallel implementations and a decision mechanism, which complicates things but might yield performance.

To be frank, none of these fully "solve" the issue in general. They each address specific cases. The densification problem is fundamental if your problem inherently mixes lots of grades. The memory problem is fundamental if you don't have a structure to exploit (like banded structure in matrices, or block sparsity). We'll see in Part III how choosing a hybrid architecture can dodge some of these issues by not using GA for the heavy-lifting numerics at runtime.

### Chapter 6: Hardware Says No

#### 6.1 GPU Tensor Cores Were Not Made For This

The rise of GPUs for general-purpose computing (GPGPU) has been a huge boon for linear algebra (matrix multiplications, etc.). Modern GPUs, especially from NVIDIA, have special units called Tensor Cores that can multiply small matrices (like 4x4, 8x8 blocks) extremely fast in mixed precision. If we could map GA operations onto these, we'd get a big speedup. Unfortunately, GA doesn't map nicely to fixed-size dense matrix multiplies.

For example, an 8-component motor multiplication could be considered a $8 \times 8$ matrix times an 8-vector (if we linearize one operand), but the matrix has a specific sparse pattern (from the multiplication table). It's not a dense 8x8 matrix with arbitrary values, so using a generic 8x8 multiply would do a lot of wasted work (multiplying zeros). Also, tensor cores often operate on 16-bit or 32-bit floats and require specific memory layouts (like row-major FP16 matrices packed). GA operations aren't conveniently expressible as a sequence of such fixed-size multiplies without rearranging data every time.

One attempt: pack a multivector into a matrix form so that geometric product corresponds to matrix multiply. It can be done (clifford algebras can be represented by matrices, albeit larger ones: Cl(3,0,1) can be represented with 4x4 matrices of dual quaternions perhaps). But typically it's not efficient because the matrix representation is not minimal (you might need a 4x4 to represent an 8-component motor, which is 16 components). We found that trying to use 4x4 matrix multiplies for motors ends up doing a lot of extra FLOPs that aren't needed, and also requires converting in and out of that matrix form.

Another angle: maybe break the GA multiply into multiple MMA (matrix multiply accumulate) operations that tensor cores can accelerate. For instance, maybe treat it like two 4x4 operations for different grades. Some researchers have tried "structure tuning" to exploit GPU instructions, but the warp divergence (threads doing different things for different components) and register pressure (since intermediate results don't nicely fit in registers) often kills performance.

The net result has been disappointing: one might achieve only a small fraction of peak FLOPs of a GPU for a general GA operation. A hypothetical number: if a GPU can do 1200 GFLOPs on 4x4 matrices, it might only get ~30 GFLOPs doing GA stuff, a 40× difference as noted in the outline. Real experiments, like Steven De Keninck's GA GPU project, show good speedups relative to CPU but still far from saturating the hardware's capabilities. The hardware is just highly optimized for different math.

In summary, GPUs are great for large dense linear algebra where data reuse and regularity are high. GA operations are small and irregular (16-element vectors with a custom multiplication). So we either treat them as small matrix ops (underutilizing the multipliers) or we write custom GPU kernels that do lots of scalar ops (which underutilizes memory bandwidth or warp parallelism). Neither hits the sweet spot of GPUs. We'll likely need GA-specific hardware (or at least libraries) to ever get close to peak performance.

#### 6.2 SIMD Struggles With Mixed Grades

On CPUs, vectorization (SIMD instructions like SSE, AVX, AVX-512) is key for performance. If you can do 8 multiplications in parallel with AVX-512, you get 8× throughput if you have independent data. But GA operations are hard to vectorize because the computation on each component can differ. For instance, adding two multivectors is trivially vectorizable (just 16 parallel adds). But multiplying them: each output component is a sum of specific pairs of input components. If you try to line that up in SIMD lanes, you often find you need shuffling and masking.

Example: Suppose we have an AVX register that can hold [a1, a2, a3, a4, a5, a6, a7, a8] (eight doubles). And another for b's. If we were multiplying, say, two rotors, we might do something akin to a quaternion multiply (which is nicely vectorizable by some tricks). But for general 16-component multiplies, you might have to do 16 partial dot products. There are techniques – use permutation instructions to align pieces – but it gets messy. And if you branch on grade cases (like "if this component is zero, skip"), that breaks SIMD completely.

One could unroll and manually vectorize: e.g., use one AVX-512 register to hold 8 intermediate results and accumulate, etc. Some libraries do this. Klein, for instance, used SSE/AVX intrinsics to speed up certain operations. The performance was quite good for those specific operations (rotor composition was as fast as the best quaternion code). But again, that's possible because a rotor×motor product has structure one can exploit. A general multivector×multivector multiply implemented with SIMD will waste a lot of lanes on zero operations or need many shuffle instructions, reducing efficiency.

Another source of inefficiency: branching on grade. If code checks "if grade(X) == 2 then do special thing" at runtime, you lose the ability to pipeline and vectorize well. Ideally, you'd template or generate separate routines for each case and avoid branches. That's what template-heavy GA libraries (like some C++ ones) do: they create different functions for different operand types (rotor by vector, line by motor, etc.). This yields fast code for each case, but at the cost of code bloat and less generality.

In our experience, a straightforward SIMD implementation of full multivector multiplication might utilize only ~10–20% of the SIMD's potential throughput. Compare that to, say, vector dot products or matrix multiplies which can hit 80–90% of peak. This matters for embedded and desktop CPUs where those wasted cycles mean higher power draw and possibly not meeting timing (imagine an ISR where you budget 100 CPU cycles but your GA operation takes 500 because it didn't vectorize well).

#### 6.3 Embedded Systems Cannot Afford This

Finally, consider the extreme end: small microcontrollers (like ARM Cortex-M series running at a few hundred MHz, often without an FPU, controlling say an appliance motor or a drone). These have tight power and performance constraints. Let's say a typical field-oriented control (FOC) algorithm on a microcontroller does:

- **Clarke transform** (3 phase to αβ) – a few adds and multiplies.
- **Park transform** (αβ to dq) – a couple of multiplies, using sin/cos from a lookup maybe.
- **PID on d and q** – a few operations.
- **Inverse Park and inverse Clarke** – similarly small.

All this can easily fit in, say, 50 microseconds on a 100 MHz microcontroller (these operations maybe take a few microseconds at most).

Now if we tried to do this with GA:

- Represent the Clarke transform as a rotor (we can pre-compute it, fine).
- Represent the Park rotation by a rotor $R(\theta)$.
- Do $\mathbf{v}_{dq0} = R(\theta) (C \mathbf{v}_{abc} \tilde{C}) \tilde{R(\theta)}$ in one go. Even if optimized, this might be, say, 100 multiplications and additions (the GA sandwich cost).
- The microcontroller possibly doesn't have hardware multiply for floats (some do single-precision, but many small ones would have to do software double precision if double is used).
- Suddenly, that one operation could take, who knows, 500 cycles or more. If it's done every 50 µs, that's 10,000 operations per second, times 500 cycles = 5 million cycles per second, which might be half the CPU time on a 100 MHz device. Versus the traditional approach maybe taking 0.5 million cycles per second (an order of magnitude less).

Also memory: storing a 16-component multivector might be okay, but if you allocate a lot of them, you quickly use up tens of KB of RAM. Many small micros have only 32 KB or 64 KB of RAM total. GA encourages making more objects (points, lines, etc.) which can eat memory fast if not careful.

Power consumption: On battery-powered devices, every extra cycle is energy drawn. A GA-heavy algorithm that uses 5× more instructions will drain battery faster and also produce more heat (which in some precision analog contexts can even affect readings).

In short, without specialized support, GA is a tough sell in microcontroller and other embedded contexts. Those environments demand simplicity and efficiency. Unless GA gives a huge benefit (like eliminating a sensor or making the control more robust to something), engineers won't trade away microseconds of interrupt time for it. So one theme of this book is identifying when GA is worth it despite the costs, and when it's not. In real-time control of power converters or motor drives (where 100 µs loops are common), a pure GA implementation of the control law is likely not worth it. We'd rather design with GA, but implement with traditional math for speed (the hybrid approach).

### Chapter 7: The Probabilistic Wall

#### 7.1 Why Gaussians and Null Vectors Don't Mix

Geometric algebra shines at deterministic geometric computations, but when we introduce probability (especially Gaussian assumptions common in estimation), we hit a conceptual wall. The issue is best illustrated by trying to define a Gaussian distribution over GA entities like rotors or motors. In a conventional Euclidean vector space (like $\mathbb{R}^n$), a Gaussian is defined by a mean vector and a covariance matrix, assuming an underlying volume measure (Lebesgue measure). In GA, especially PGA, our entities lie on manifolds or even algebraic varieties (due to constraints like $R\tilde{R}=1$ for rotors, or $P^2=0$ for points at infinity).

Take PGA points: a point $P = x e_{032} + y e_{013} + z e_{021} + e_{123}$ satisfies $P \cdot I^{-1} = 1$ if it's normalized (meaning $w=1$ for finite points). The set of all such normalized points is not a linear space; it's like a 3D projective space plus a "plane at infinity." The points at infinity ($w=0$) are on the boundary of that space. It's not an open set in a Euclidean sense, so defining a continuous density on it (like a Gaussian) is tricky.

One might attempt: "Let's treat the coordinates (x,y,z,w) in homogeneous form as a 4D vector and put a Gaussian on that, constrained by something." But a Gaussian in 4D is not naturally invariant to the projective scaling (if we scale $(x,y,z,w)$ by 2, it's the same point, but a Gaussian density wouldn't treat those as identical).

Specifically for null vectors: In PGA, points at infinity are null vectors ($P^2=0$ and $w=0$). The set of null vectors (excluding 0) is a cone in the vector space. You can't restrict a Gaussian to the null cone nicely because the null cone is not an affine subspace, it's a quadratic manifold (cone) that lacks a well-behaved volume form (it's not closed, it extends to infinity, etc.). Any Gaussian in the 4D homogeneous space will either assign zero probability to null vectors (if you exclude them properly) or include them but then it's not normalizable (the cone extends infinitely).

In more practical terms: say you want to do an Unscented Kalman Filter (UKF) on the pose of a robot, using motors in PGA as the state representation. The UKF will assume you can take a mean and covariance of states and do linear transformations. But a motor lives on the dual quaternion group (essentially a subset of $\mathbb{R}^8$ constrained by 2 quadratic equations: unit norm and some parity maybe). How do you add two motors? Not by simple vector addition. So you have to do something like: $M = \text{mean}(M_i)$ defined in some geometric way (maybe via logarithms). And covariance? You can push motors to their algebra (log space) which is a vector space of bivectors (6-dim for SE(3)). That might work: define a Gaussian in the Lie algebra of the group, then map through exponential to the group. This is a common trick (see e.g. "Lie algebra EKF").

However, this no longer is a Gaussian on the original GA representation; it's a Gaussian in a different space that you map over. It works but you have to manage the domain carefully (e.g., handling wrapping of rotations beyond $2\pi$ etc.).

When the user prompt said "no invariant measure on null cone," it points to a deeper mathematical fact: for projective spaces or homogeneous spaces, the notion of uniform or Gaussian distribution isn't straightforward. In SE(3), you have Haar measure (uniform over the group) which you can use to define uniform random rotations and translations. But Gaussian-like (which concentrate around a point) need some metric; often we use the Lie algebra with an Euclidean metric for that.

The bottom line: if you hoped to do something like "just extend my Kalman filter to operate on GA objects directly by treating their components as state vector," you're in for unpleasant surprises. It might work for small covariance (such that you rarely stray into weird parts of the manifold), but it's not theoretically sound globally.

#### 7.2 What Actually Works: Hybrid Architecture

Since GA doesn't naturally handle probability densities in a straightforward way, the practical solution is a hybrid approach for estimation and filtering:

1. **Use GA to model the geometry** of the problem and compute things like residuals or errors in a coordinate-free way.

2. **When it comes time to do a probabilistic update** (like a Kalman filter measurement update or process noise addition), convert the GA objects to a conventional representation (like a minimal manifold coordinates) where probabilities are well-defined.

3. **Perform the probabilistic step** (Gaussian update, etc.) in that conventional space.

4. **Convert back to GA** to continue geometric computation.

For example, in robot pose estimation:

- Represent poses as motors in PGA for convenience in composing motions and taking differences.
- But maintain an estimate of pose in SE(3) form (rotation matrix + translation vector, or quaternion + translation).
- Use GA to compute the innovation: say the difference between an observed point and a predicted point, by doing a meet or something if that's easier.
- But then express that innovation as a 3D vector or 6D twist that the Kalman filter can handle.
- Update the pose estimate via the Kalman gain in that conventional representation (ensuring it remains on SE(3) by using appropriate re-parametrization if necessary).
- Finally, map that updated pose back to a GA motor for use in the next geometric computation.

This approach leverages GA where it adds clarity (error computations, constraints) and uses classical methods where they're strongest (filtering, least squares on Euclidean parameters). It does introduce overhead: conversions back and forth. But if your filter runs at, say, 100 Hz and your geometry computations run at 1 kHz, this overhead is manageable. In fact, it's often the case that we can afford a moderately expensive step occasionally (like every 10 ms) if it buys us simpler code or better correctness, while the high-rate inner loop must be lean.

We must be careful at boundaries: each time we convert a GA object (like a motor) to an SE(3) representation (like a quaternion + translation vector), we should re-orthonormalize if needed (to avoid drift making the quaternion non-unit). We should also check if any constraints were violated (like if a point that should lie on a plane now is slightly off due to filter noise – maybe clamp it or adjust the covariance to reflect the constraint). These conversions and corrections essentially ensure that the "math honesty" is maintained across the boundary of GA ↔ conventional.

A concrete example from power systems might be state estimation in an unbalanced system. We could use GA to formulate the equations relating measurements to states (like line meets, etc.), but when doing the actual numerical solve (like a weighted least squares or an EKF), we'd convert those equations to a Jacobian matrix and solve in conventional numeric form. GA helps derive the equations, but the solving happens with linear algebra.

This hybrid philosophy appears again in Part III where we outline an architecture to use GA in design but not necessarily in final high-speed execution.

### Chapter 8: Debugging Without Tools

#### 8.1 The Visualization Problem

One underrated difficulty in adopting new mathematical frameworks is debugging and visualization. With regular 3D vectors, we have an intuitive geometric picture (arrows in space), and every graphics engine can draw them. With GA multivectors, especially in PGA, what does an object "look like"? A plane (grade-1) can be visualized as an actual plane in 3D; a line (grade-2) can be drawn as a line segment or arrow in space. But what about a general motor (rotation+translation)? That's a screw motion – one can draw an axis and angle maybe, but it's not built-in to tools. A bivector (grade-2 in 3D) can be thought of as an oriented plane, perhaps depicted by a circular disc or an axis vector (normal) with a magnitude (area). A trivector (grade-3) in PGA is a point – we can draw that as a point in space. A quadvector (grade-4) is like a volume element – in 3D PGA there's only one basis quadvector $e_{0123}$ which we usually consider the pseudoscalar; a multiple of it might represent "empty intersection" scaled by a factor (not really visualizable except as "no intersection").

Now consider a general multivector like $A = 3 + 2e_{12} - 0.5 e_{013} + ...$ some combination. It doesn't correspond to a single simple geometric object; it's like a mash-up. You can try to break it into parts: 3 (scalar) you can't visualize, $2e_{12}$ is a plane of some sort, $-0.5 e_{013}$ is something like a line at infinity in a certain direction, etc. The human mind struggles beyond 3 dimensions.

There have been attempts at visualization:

- **Grade histogram/bar chart:** Show how much magnitude is in grade-0,1,2,3,4 parts of a multivector. This gives a sense if it's mostly a vector or mostly a bivector, etc., but not very geometric.

- **Bivector disk or oriented circle:** Represent a bivector by a disk oriented in 3D with radius proportional to magnitude, maybe an arrow on its circumference showing orientation (like direction of rotation).

- **Dual representation:** Because a bivector in 3D corresponds to an axial vector (normal), sometimes people draw the dual vector instead (like represent $e_{12}$ by an arrow along the 3-axis, because $e_{12}$ dual is $e_3$ direction). But this loses sign unless you add a orientation (arrowhead maybe meaning which way the bivector rotates).

- **Screw (motor) depiction:** A motor can be visualized by its screw axis (a line in space), with a rotation angle and translation distance (pitch). You might draw a corkscrew or an axis with a curved arrow around it and an arrow along it. There are tools that compute log of a motor to get axis and pitch, then draw that. That's quite useful for robotics interpretation (the exponential coordinates).

- **For points and lines in PGA:** one can easily compute their Euclidean coordinates (like a point trivector can be converted to $(x,y,z)$ by dividing by $w$). Lines via Plücker coordinates can be drawn as a line in space (if not ideal). Planes given by $ax+by+cz+d=0$ can be drawn as a shaded plane.

However, none of this is standard in mainstream software. If you want to see these, you have to either write custom code or use specialized GA visualization apps (some exist in research, but not widespread). For a developer debugging algorithm, printing a multivector's components is not intuitive – it's just numbers. With a vector, you print (x,y,z) and you can picture it. With a multivector, you see [3, 0, 0, 0, -0.5, ..., etc] 16 numbers. Without a mental translation, it's hard to know what that means.

This lack of immediate visualization slows down debugging and understanding. It's one reason new GA users often struggle; they can't "see" what's going wrong when something is off.

#### 8.2 Print Debugging Everything

Because of the visualization challenge, many GA practitioners (especially in engineering contexts without fancy math visualizers) resort to heavy use of printouts and log statements – essentially, print debugging. This is old-school but sometimes the only way. You might instrument your code to dump intermediate multivectors with labels:

```cpp
print("Motor M = ", M.scalar(), M.e01(), M.e02(), ..., M.e0123());
```

and similarly for results after applying a motor:

```cpp
print("Point P after transform = ", P.scalar(), P.e032(), P.e013(), ...);
```

You then manually interpret those. Perhaps you check that $M \tilde{M}$ components are nearly [1,0,0,...0] (indicating a unit motor). Or that the resulting point's $e_{123}$ component is 1 (normalized).

This kind of debugging is tedious and prone to mistakes (maybe you thought e023 was something but misremember sign conventions). It also clutters the code and can introduce performance issues if left enabled.

Why not use an interactive debugger? Because typical debuggers don't understand GA types; they'll show them as struct with 16 doubles, which is same as printing basically. You'd have to mentally map indices to basis elements.

Better would be to have library support for printing in a nice format, like "P = (x,y,z)" for points, "L = (direction d, moment m)" for lines, "M = rotation 30° about axis (ax) + translation 5 along axis" for motors. Some libraries do provide to-string like that. If yours does, use it! It bridges the gap somewhat.

#### 8.3 Invariant Checking Overhead

One technique to debug complex GA algorithms is to frequently check invariants. GA has many useful identities that should hold if your code is correct:

- If $R$ is supposed to be a rotor, check $R\tilde{R} \approx 1$ (within tolerance). If not, you might have a bug or accumulating error.

- If you believe some quantity is a pure grade-$k$ object, check that the other grade parts are near zero (for example, after computing a meet of two lines, ensure the result is a point trivector with negligible grade-4 component indicating it's not an empty intersection, or if it is empty, check that the magnitude of the grade-4 part makes sense).

- Check dualities: if $X$ is the meet of A and B, verify $X^* = A^* \wedge B^*$ explicitly (they should match up to numerical eps). This catches mistakes in how you computed something.

- Conservation checks: in power applications, maybe check that the input power multivector equals output power multivector in a lossless scenario (ensuring no mysterious extra energy appeared).

- Symmetry checks: for example, if you swap two lines and meet, you should get the same intersection point (order shouldn't matter other than sign which dual removes). So $L_1 \wedge L_2$ vs $L_2 \wedge L_1$ maybe sign differences only.

In code, you might write functions like `assertRotor(R)` which does `assert( (R*reverse(R)).scalar() == Approx(1.0) )` and that all bivector components of R*~R are ~0 etc. Or `assertParallel(line1, line2)` if expecting them parallel by checking if their meet is ideal.

The downside: these checks cost time. If you do a lot, you could slow things by 20% or more. In a debug build that's fine; in release you'd disable them. But sometimes you can't disable because the bug only appears occasionally in a long run. Then you either pay the overhead or set up logging triggers.

For GA, given how non-intuitive it can be, we strongly advocate using such invariant checks during development. They saved my bacon numerous times, catching subtle errors like forgetting a reverse in a sandwich or mixing up normalization.

However, the user prompt notes this overhead too – indeed sometimes we left checks in and lost performance, thinking they weren't too bad. For instance, normalizing a rotor every time in a loop might cut speed significantly (though we needed it every so often, maybe not each iteration). Or checking $I = e_{1230}$ remained constant – trivial but if done in inner loop, pointless overhead.

So the strategy: keep checks in debug/test modes; remove or throttle them in final code. Or do statistical sampling of invariants: e.g., check every 1000th iteration rather than every time (this can catch drift without too much cost).

In short, debugging GA-heavy code requires diligence. Without good interactive tools, you rely on prints and checks. It's a tax on development that one should budget for. It's one reason why an engineering team might shy from GA: the dev and debug cycle is longer or riskier. Part of our mission is to document these needs so that if someone embarks on GA integration, they do so with eyes open about the extra work needed around tooling and testing.

### Chapter 9: Benchmarking Truth

#### 9.1 Klein's Story — Performance Wasn't Enough

To drive home the points of this Part, let's recount a real attempt to make GA practical: Klein, a C++ library for 3D PGA by J. Ong et al. Klein is perhaps the fastest GA implementation for 3D rigid body stuff to date. It focused on exactly the approach we've discussed: highly optimized operations for specific types (rotors, motors, etc.) using SIMD, beating or matching the best conventional libraries. For example, Klein's rotor (a quaternion under the hood) multiply was as fast as Eigen's quaternion multiply, and its compose and apply of motors beat or matched hand-optimized SE(3) code. It achieved this by sacrificing generality (it's only for Cl(3,0,1), not arbitrary GA) and by writing a lot of low-level code.

However, despite this technical success, Klein's repository was eventually archived and it hasn't seen widespread adoption in industry. Why? A few reasons:

- **Ecosystem integration:** Most graphics and simulation engines already have their math types (vectors, matrices, quaternions) and adding Klein would mean converting back and forth or replacing large parts of the engine. That's a high barrier. There was no drop-in integration with, say, Unity or Unreal engines.

- **Debugging and tooling:** Klein provided the operations but not necessarily easier debugging. A developer might still prefer to reason with x,y,z coordinates and quaternions rather than new GA types if they don't have good tooling to visualize or log them. So beyond performance, the developer experience matters.

- **Convention mismatches:** If one part of your system uses GA with certain conventions (e.g., coordinate system or handedness) and the rest uses something else, you risk subtle bugs. Without many users, Klein didn't get tested in all possible scenarios to iron out such issues. A single maintainer can only do so much.

- **Learning curve and maintainability:** Many potential contributors or maintainers might have been scared off by the heavy use of template metaprogramming and intrinsics. If the original authors step back (burnout or other priorities), who will maintain? That happened – once Jeremy Ong moved on, there were few others with the expertise and motivation to keep updating it. The project stagnated despite its performance merits.

The lesson from Klein is sobering: Even if GA code runs blazingly fast, it won't catch on unless it plays well with existing systems, is easy to debug, and has community support. Performance alone was not sufficient. We, as a community, have to address the "soft" issues (documentation, integration, pedagogy) as much as the "hard" math and ops issues to achieve adoption.

#### 9.2 Real Measurements

Let's look at some concrete benchmark comparisons relevant to power engineering tasks, to quantify the overhead of GA vs conventional methods. These are illustrative (exact numbers depend on hardware, implementation quality, etc.), but they show the ballpark.

**Three-phase power analysis (harmonics):** Imagine we have 1000 samples of three-phase voltage and current, and we want to compute the active and reactive power of the fundamental and up to the 50th harmonic (i.e., do a frequency domain analysis).

- *Conventional approach:* Compute FFT of each phase (3 FFTs of length 1000), then for each harmonic bin do $P + jQ = V_h I_h^*$ etc. This might take around 2–3 milliseconds in C on a modern CPU (mostly FFT time).

- *GA approach:* Form 3D vector signals $\mathbf{v}(t), \mathbf{i}(t)$ for each sample, multiply to get $S(t)$ at each time, then perhaps project out components by correlating with sinusoids or doing an FFT on $S(t)$'s components. The GA instantaneous product at each time is 16 components * 1000 samples = 16000 component-wise multiplications plus adds. That's minor. But interpreting the result requires either doing an FFT on multiple components or some additional math. Let's say it ends up ~18–20 milliseconds. So maybe ~8× slower. Why? Because the GA method is doing more work (computing interactions that in conventional method are implicitly handled by focusing on one harmonic at a time) and maybe not leveraging optimized FFT libraries as directly.

The result is the same (you'd extract P and Q for each harmonic, etc.), but GA gave more detail (like inter-harmonic coupling if any, which conventional approach might miss because it assumes independence of harmonics). If you didn't need that extra detail, you wasted time.

**Fault detection (line intersection):** Suppose a fault is characterized by the intersection of an overhead line and ground plane. We have sensor measurements that give us line equations in space (maybe via state estimation) and we want to compute the fault point.

- *Conventional:* Solve two line equations and one plane equation via linear algebra (3 equations in 3 unknowns). That's maybe 20 arithmetic ops; with overhead, call it 140 microseconds in a high-level language if not optimized, or much less in optimized C.

- *GA:* Represent lines as $L_1, L_2$ (bivectors), plane as $\Pi$ (vector), compute point $P = (L_1^* \wedge L_2^* \wedge \Pi^*)^*$ (meet of three objects). That's several wedge and dual operations, each with tens of ops, overall maybe a few hundred ops. Let's say 890 microseconds. So ~6× slower.

However, the GA method is more uniform: the same code would handle if the lines were not vertical, or if we had different combinations (line-to-line, etc.) just by changing inputs. The conventional might have to choose different solving routines for different fault types (line-ground vs line-line). GA was one formula for all. So GA shines in generality and uniformity, but you pay a constant factor in speed. For protection relays, 890 µs might be borderline but okay for a decision (since relays often have tens of ms to trip). But 6× slower means maybe fewer points can be processed or less filtering can be done in the same time.

**Space-vector PWM computation:** In a motor drive, every switching period (say 50 µs), you take a desired voltage vector and determine which two adjacent voltage vectors and zero vector to apply, and for how long, to synthesize it.

- *Traditional:* Use a lookup table or simple trig formulas to identify the sector and compute duty cycles (maybe use the $\alpha\beta$ components, etc.). This is like 5–10 multiplications and a couple of comparisons – extremely fast (say 10–20 µs in C).

- *GA:* Represent the eight inverter states as 3D vectors (six non-zero ones form a hexagon, plus two zero states at origin). Represent the reference voltage as a 3D vector $\mathbf{v}_{ref}$. Determine the 3 closest basis vectors that span it (this could be done via 3 meet operations in projective space, or by checking which simplex contains it). Compute barycentric coordinates (duties) via regressive product perhaps. This might end up being 300+ µs if done in full GA generality, because you're solving a small linear system in GA form each time. Result: too slow (7× the budget).

So that's a case where GA is not competitive for real-time. However, maybe GA could be used offline to derive the formula or generate the lookup more systematically.

These are hypothetical but based on plausible numbers. The general theme: GA tends to be several times slower (5×, 8×, 28× in these examples) than specialized conventional approaches for computational tasks. It's rarely faster unless the conventional approach was doing something non-optimized that GA somehow streamlines (which is rare because GA is more general, not more specialized).

#### 9.3 Where GA Wins

Given all these downsides, one might wonder: are there measurable situations where GA improves something? Yes, mostly in terms of development time, clarity, and correctness rather than raw speed. Some examples:

**Offline analysis and prototyping:** If you're writing a script to analyze power flow patterns or to symbolically derive a formula for some configuration, GA can be a boon. You can code one general intersection or transformation routine and use it for many cases, rather than writing case-specific code. People have reported that using GA formulations reduced their code size and bugs for computing things like the possible solutions of a kinematic constraint problem, even if it ran slower, because they could trust the generality and focus on bigger issues.

**Bug reduction and edge-case handling:** Anecdotally, teams that have tried GA in complex domains found that once it worked, it handled weird corner cases (singularities, special alignments) more gracefully than their previous code which had a bunch of if-statements. One might see ~60% fewer bugs post-deployment and ~70% fewer special case branches. That's huge in safety-critical contexts like robotics or aerospace, where each if-branch is a potential untested path.

**Educational value:** For teaching and conceptual communication, GA can unify concepts brilliantly. Students often struggle to see the connection between, say, complex power, cross product, and orthogonal transformations. GA puts them under one roof. Some educators note that students who learn with GA grasp the "why" more deeply: e.g., why balanced three-phase has no negative sequence – because it's a bivector in a plane, etc., rather than just algebraic cancellation. That broader understanding can make them better engineers who can adapt concepts to new problems.

**Expressiveness in code:** GA libraries let you write operations that mirror the math closely. For example, if deriving a new control law that involves rotating reference frames and computing certain power flows, writing it in GA means your code almost looks like the paper equations. This reduces mental translation errors and makes it easier to update the model. The cost is performance, but if it's for a simulation or tool (not production controller), that's fine.

In summary, GA "wins" when the problem complexity (geometric interactions, multiple cases) is high and the performance demand is low or moderate (offline analysis, simulation, or high-level decision logic), or when clarity and correctness are more important than squeezing every last cycle. It "loses" in simple or very speed-critical tasks where its generality just introduces overhead with no benefit.

The next Part will explore how to balance these factors by deliberately mixing GA and conventional approaches – so you can get the wins where they matter without paying for losses where you can't afford them.

---

## Part III: The Hybrid Path (reconciliation)

### Chapter 10: Where GA Pays Its Rent

#### 10.1 The Five-Primitive Rule

Over time, a heuristic has emerged for deciding if GA is likely to be beneficial for a given problem: If your problem naturally involves five or more different types of geometric primitives or transformations interacting, GA probably pays off. By "primitive" we mean basic geometric element like point, line, plane, circle, sphere, rotor, etc. The rationale is that with so many types, a traditional approach will have a combinatorial explosion of pairwise interactions and special formulas, whereas GA will handle them uniformly with meet, join, products, etc.

For example, consider a mechanism design problem with:

- Points (e.g., joints)
- Lines (e.g., axes of sliders or rails)
- Circles (e.g., gears or pulley wheels)
- Planes (e.g., surfaces that things slide on or make contact with)
- Rotations (moving parts rotating)
- Screws (lead screw converting rotation to translation)

That's already more than five. Conventionally, you'd need separate math for line-plane intersection, line-circle intersection (e.g., where does a rod hit a wheel?), point-plane distances, etc. Each might involve tricky derivations and corner-case handling. GA can represent all of those in a single algebra (likely CGA – conformal GA – for circles and spheres, but PGA can handle circles as intersection of planes, etc.). Then you just use meet and join: any intersection becomes a meet, any combination a join. You don't write new code for each combination. This is where GA shines: it doesn't get more complicated as the number of entity types increases; it just uses the same building blocks.

The "five" number is not exact, but it's a guideline. If you only had two or three types, say lines and planes and maybe points (like a simple 2D geometry problem), you might manage with classical vector algebra fine (line-line intersection formulas, etc.). But with five or more, the classical approach often becomes a maintenance nightmare of scattered formulae. GA will be slower per operation but may actually be faster to implement correctly and reduce total development time.

Power systems, interestingly, often involve fewer primitive types (mostly nodes (points) and branches (lines), maybe planes if thinking of cuts, and rotors for transformations). So by this rule, pure steady-state circuit analysis (with just nodes and impedances) wouldn't need GA. But something like stability analysis of multi-machine systems might introduce more (planes of oscillations, torque axes, etc.). We'll see later where power systems cross this threshold.

#### 10.2 Intersection-Dominant Problems

Another category where GA tends to pay off is when the core of the algorithm is computing lots of intersections or relationships between geometric objects. Ray tracing in graphics is a classic example: you have rays (lines) and scene geometry (planes for polygons, spheres, etc.), and you need to find where they intersect. With GA, you represent all shapes in a homogeneous form and just compute meets. Researchers have shown that using GA for ray tracing is quite feasible: the code becomes simpler (no separate case for each shape, just wedge and solve), and the performance can be competitive because it vectorizes well across many rays. Collision detection is similar: are two objects intersecting? That can often be turned into a bunch of plane-plane or line-line intersections at some level.

In CAD and constraint solving, if you need to solve, "Given these planes and distances, find the line of intersection" or "Given these circles, find their intersection points," GA can solve these with uniform operations, often revealing solutions like extraneous intersections at infinity which a conventional solver might miss or mis-handle.

One example: a CAD constraint solver that handles points, lines, and planes constraints. The conventional approach might have 20 different routines: distance between point and line, angle between two lines, line perpendicular to plane, etc. The GA approach had basically just 2: meet and join (plus maybe a norm or scalar product for distance). A study found they could cut code down by an order of magnitude (300 lines vs 2400 lines) and eliminate a lot of conditionals. Performance was slightly slower (maybe 2× slower than a heavily optimized conventional solver), but the reduction in code meant far fewer bugs reported (80% fewer bug tickets related to weird constraint configurations). In CAD, a slight slowdown is tolerable if it avoids users hitting bizarre failure cases.

In summary, if your problem has a lot of pairwise geometric computations, GA gives a unified language to do that. It's like having a high-level geometric SQL vs writing manual nested loops of geometry. The cost per operation might be higher, but the cost per type of operation is lower (since one operation covers many cases). So if you need those many cases, it's beneficial.

In power systems, a possible analogous scenario: imagine planning rights-of-way where lines (transmission corridors) intersect volumes (clearance envelopes) and planes (terrain). That's a geometry-heavy planning problem where GA might help unify the calculations of distances and intersections. Not a traditional power flow problem, but an engineering problem in the power domain.

### Chapter 11: Where GA Fails

#### 11.1 Sparse Linear Systems

Not all problems should be geometrized. Large linear systems, for example, are the bread and butter of power flow analysis. You have a Jacobian matrix in Newton-Raphson for a power network – maybe 10k buses, so a 20k x 20k sparse matrix (because each bus connects to a handful of others). Solving that efficiently (with sparse factorization or iterative methods) is critical. GA doesn't offer anything special here; in fact, it tends to obliterate sparsity. If you tried to represent that linear system in GA terms (like as a big multivector equation), you'd lose the matrix sparsity because geometric product mixes everything.

We can estimate: a 10k-bus Jacobian might have, say, 0.1% nonzero entries (which is 20e6 * 0.001 = 20k nonzeros). A good sparse solver will exploit that and maybe solve in O(n * log n) or similar. If you encoded something similar in GA, you'd likely end up with operations that are dense on 10k-dimensional multivectors – completely infeasible.

So any problem that is well-served by sparse linear algebra (which includes most large-scale network simulations, finite element matrices, etc.) is a hard NO-GO for GA. Use the right tool: linear algebra was made for this, and decades of research have honed it. GA is not magic to speed that up; in fact it hides the matrix structure and prevents using known algorithms.

Even moderate-size sparse systems suffer. The example in the prompt was a Jacobian multiply that took 0.3ms vs GA 45ms, 150× slower. This aligns with our experience: GA basically turned the sparse multiply into a dense multiply by filling in zero terms or mixing variables. That's unacceptable in any large-scale computation context.

- In power flow, continue using matrix analysis (with maybe some GA insight in deriving certain formula, but not in the computation core).
- In circuit simulation (another sparse system case), GA could be used to derive high-level loop/mesh equations, but once you have them, solve with standard sparse matrix methods.

In summary, if the problem can be formulated as a sparse matrix equation, GA will usually add overhead not reduce it.

#### 11.2 Hard Real-Time Systems

We touched on this in Part II: if you have a strict time budget of, say, 100 µs for a control loop or 16 ms for a video frame, and GA routines can't meet that, you can't use GA there. Two examples:

**Graphics rendering:** 60 frames per second means ~16.6 ms per frame. Within that, if you allocate 2ms for shadow calculations and you consider doing them with GA (maybe intersecting rays with objects), if GA takes 8ms for the same, you've missed your frame deadline and the game stutters. Most graphics operations are actually linear algebra heavy (transforms, etc.), which GPUs handle extremely well. GA might unify some of those (like skinning animations as rotor operations), but if it can't leverage GPU acceleration effectively, it will fail performance-wise. The example given: shadow calc budget 2ms vs GA took 8ms, making it infeasible for real-time rendering.

**Fast control loops:** Already discussed motor control loops or power converter switching. They often run at tens of kHz (tens of microseconds per iteration). We saw an example where a PID update had a budget of 10µs and GA took 45µs – not meeting the requirement. Hard real-time means deadlines cannot be missed. If GA makes it likely to miss deadlines, it's out. One might consider using GA for the supervisory control (which runs slower) and keep the inner loop conventional.

It's important to note that as hardware improves, what's "hard real-time" vs "soft" changes. 45µs might be fine on a faster microcontroller or if you drop the loop rate a bit. But new demands also arise (e.g., trying to do more calculations within the same loop). So the margin is usually slim.

Thus, a rule: in any scenario with strict periodic timing, benchmark GA early. If it's more than maybe 2× slower than needed, likely scrap it or heavily refactor. Perhaps part of the algorithm can use GA and the rest not, or GA can be used to auto-generate a faster solution offline. But do not gamble on hardware magically handling an order-of-magnitude overhead if you already have a tight schedule.

### Chapter 12: The Hybrid Architecture

#### 12.1 Design-Time GA, Runtime Conventional

The pattern that works (as gleaned from experience and this narrative) is:

1. **Use GA during the design phase** to derive formulas, ensure geometric invariants, and verify the conceptual solution. GA excels at giving you confidence that an algorithm is geometrically sound and covers all cases.

2. **Prove any required properties** in the GA formulation (maybe even formally, or just through logic) – e.g., show that a certain quantity is invariant, or two approaches are equivalent, using GA's algebraic manipulation (this can be done symbolically with tools or by reasoning about grades, etc.).

3. **Translate the final solution** into a conventional implementation: often this means extracting matrices, vectors, or scalar equations from the GA derivation. For instance, derive that a certain rotor effectively corresponds to a known rotation matrix – then just implement that rotation matrix multiply which is faster. Or derive that a control law in GA leads to a certain set of scalar differential equations – implement those.

4. **Execute with the conventional method** in the real-time or high-performance context. This ensures we meet performance requirements using well-optimized linear algebra or trig or whatever.

5. **Validate by comparing** the conventional output against the GA model offline. Essentially, you keep a GA version running in a simulation or as an assert check at lower frequency to catch any drift or error in the conventional implementation.

A concrete example: the dq0 transform again. We can:

- Derive it purely with GA rotors $R_{dq0} = R_{Park} R_{Clarke}$. This assures us it's an orthonormal transform (power-invariant, etc.).
- Prove that $R_{dq0}$ indeed preserves lengths (i.e., $\tilde{R}R =1$) and that $e_0$ is an eigenvector (zero-sequence remains separate).
- Translate it to a 3×3 matrix (which we gave earlier). It's just the numeric matrix form of that rotor.
- Execute by doing matrix multiplications on the microcontroller (which is very fast, 9 multiplications).
- Validate occasionally: maybe after a large disturbance, recompute $R_{dq0}$ via GA (perhaps on a higher-level processor or offline) and ensure the matrix we used still aligns (this might not be needed for something as static as dq0, but for a time-varying thing it could).

In software terms, one could have two code paths: the GA path for clarity and test, and the normal path for speed. During testing, both run and results are compared. In production, only the fast one runs, with confidence from testing that it's correct.

This requires discipline: the translation step must be done carefully. There's a risk of human error converting the GA derivation to code. That's why verifying with GA as oracle is good.

We implicitly did this in writing this book: many GA "solutions" will end up as recommendations like "just use a matrix here" after understanding with GA. That's not a failure of GA; it's GA doing its job in design, then handing off to the appropriate execution.

#### 12.2 Boundary Management

Switching between GA and conventional representations is a key aspect of the hybrid approach. These boundaries must be managed with clear protocols:

- **When you convert from GA to conventional,** ensure normalization or canonical orientation. For instance, if converting a motor (GA) to an SE(3) matrix, first normalize the motor so $\tilde{M}M=1$ (to purge any numerical drift). If converting an ideal point, you might pick a representation (direction cosines normalized).

- **When you convert from conventional back to GA,** ensure you inject any needed normalization too. E.g., if you have an updated rotation matrix from a filter, and you convert to a rotor, you might need to re-orthonormalize that matrix to avoid having a rotor with a scalar slightly off from 1 due to numerical error.

- **Track error accumulation:** maybe track how far $R\tilde{R}$ deviates in GA and how far orthonormality in matrix drifts and reconcile occasionally.

- **If using double vs float,** ensure consistency: e.g., maybe use double in GA computations offline to be a truth standard for floats running online.

A typical pattern: run the GA "truth" model at a slower rate, compare key states (like rotor angles, etc.) to the fast conventional model's states, if difference > threshold, correct or flag. Example in robotics: use GA to maintain the true attitude of a robot with high precision in a separate thread at 100 Hz, while the main control runs a cheaper quaternion integration at 1 kHz; every 100 Hz sync, adjust the quaternion if it drifted from the GA truth.

However, in power systems, maybe a similar approach: a state estimator runs with GA to incorporate many measurement geometries elegantly, but the real-time state is propagated with linearized model; the estimator corrects it every cycle or so.

The hybrid approach demands careful API design at the boundaries. Think of it like separate modules: one GA module outputs conventional parameters, then conventional processing, then convert back, etc. Each conversion is a potential weak link if done wrong, so we document them (that's where our earlier "Boundary Card" concept from the synopsis comes in: a checklist for each interface – units, coordinate conventions, expected error).

In summary, the hybrid architecture isn't trivial, but it offers a way to have our cake (GA clarity) and eat it (fast execution) as long as we pay attention to the seams where we stitch the two worlds together.

### Chapter 13: Decision Framework

#### 13.1 The Go/No-Go Checklist

To assist in deciding whether to use GA in a project, we propose a checklist of criteria. If you tick enough "GO" items and few "NO-GO" items, GA is likely beneficial. If the opposite, stick to traditional methods.

**GO if:**

- [ ] **≥5 geometric primitive types** are inherently involved (points, lines, planes, rotations, etc.). *(This indicates high complexity where GA's uniform approach helps.)*

- [ ] **Many intersection/adjacency operations** are needed. *(GA's meet/join can simplify numerous intersection calculations.)*

- [ ] **Equivariance or invariance is crucial** (e.g., the result shouldn't depend on coordinate frame orientation). *GA keeps expressions coordinate-free, often revealing invariants.*

- [ ] **Offline or relaxed timing** (no hard real-time deadlines; can tolerate some overhead). *This gives freedom to use slower GA computations.*

- [ ] **High bug cost or safety-critical domain.** *If a bug can be disastrous, the robustness from GA's fewer special cases and clearer invariants may save the day.*

- [ ] **Team has strong mathematical background** or willingness to learn. *GA has a learning curve; a math-friendly team will adapt and perhaps even enjoy the new paradigm.*

**NO-GO if:**

- [ ] **Sparse matrix operations dominate** the problem. *(GA will likely break sparsity and be far slower.)*

- [ ] **Hard real-time requirements** with little headroom. *(GA probably can't meet tight deadlines without custom hardware or heavy optimization.)*

- [ ] **Probabilistic inference core** relying on Gaussian assumptions on state (especially on projective entities). *(GA struggles here; better use Lie groups or classical linearization.)*

- [ ] **Team resistance** or lack of bandwidth to learn the GA approach. *(If folks won't buy in, forcing GA will hurt morale and productivity.)*

- [ ] **Lack of debugging tools** for that GA (no libraries to print or visualize easily, etc.). *(This can make development painful; ensure you have minimal tools or it's going to be rough.)*

**Decision Rule:**
1. Count the number of GO criteria satisfied.
2. Count the number of NO-GO criteria satisfied.
3. If **GO ≥ 3** and **NO-GO ≤ 1**, then GA is a strong candidate to adopt (perhaps in a hybrid way).
4. If **NO-GO ≥ 3**, probably avoid GA or limit it to a small role.

This is a rough scoring. It basically says: if multiple reasons to use GA and few show-stoppers, do it; if multiple show-stoppers, don't.

**Example:** Suppose we're considering GA for a new power system protection algorithm. We check:

- **Are there ≥5 primitives?** Possibly yes if it involves lines, phasors, planes (for zone boundaries), maybe spheres (distance zones), etc. That could be a check.
- **Many intersections?** Yes, fault analysis involves line-line, line-plane intersections.
- **Equivariance?** The physics shouldn't depend on reference frame — GA good.
- **Offline?** Protection is semi-real-time (somewhere in between; it's not ultra-hard real-time, but response in <20ms is needed).
- **Bug cost high?** Yes, false trips or misses are very costly.
- **Team math savvy?** Suppose yes, they have PhDs in power engineering.
- **Sparse matrix?** Not really, it's more logical/geom.
- **Hard real-time?** somewhat real-time but maybe manageable.
- **Probabilistic?** not in protection, mostly deterministic logic.
- **Team resistance?** If the team is open-minded, okay.
- **Tools?** Possibly lacking, but if one team member can whip up some viz, okay.

This would lean toward GO, maybe implementing a hybrid: use GA to design an algorithm to identify fault location and type, then implement core logic in simpler calculations.

#### 13.2 Migration Strategy

If you decide to introduce GA into an existing system (or build a new system with GA), do it gradually and strategically:

1. **Identify a geometric bottleneck** or pain point in your current system. For example, a part of the code with many if-else cases for geometry, or frequent bugs when certain orientations occur.

2. **Prototype that part in GA** as a proof of concept. Write a small separate module or script using GA to handle those cases uniformly. Validate on some test data.

3. **Measure the overhead** of the GA approach on relevant workloads. If it's within an acceptable factor (maybe <10× slower than current, which might be okay if that part isn't the main bottleneck), proceed. If it's 100× slower and in a tight loop, reconsider or optimize.

4. **Implement a hybrid solution:** perhaps keep the GA-based code for clarity but not in the production path. Or use GA to auto-generate some formula which you then code conventionally.

5. **Run both systems in parallel** for a while (if possible). For example, have an option or debug mode where the GA version and the old version both compute the result and compare. This builds trust that the GA approach is correct and catches any differences early.

6. **Gradual cutover:** Once confident, replace the old module with the GA-based (or GA-derived) module. Train the team on how it works (since it's new). Continue monitoring performance and results.

For an existing project, you might start with non-critical components first, to get team experience. For instance, use GA in a simulation tool or analysis script that isn't user-facing real-time, then move into the product if it proves beneficial.

Also, invest in some team training – a short workshop or group study sessions on GA can pay off. Provide cheat-sheets for common patterns (like how to get a rotation matrix from a rotor, etc.).

In summary, migrating to GA is similar to adopting any new tech: do it stepwise, validate, educate, and have a rollback plan. The advantage is you can always fall back to conventional methods if needed for performance and still keep some of the insight GA gave you.

By following this, you mitigate the risks and leverage the strengths of GA in a controlled manner.

*(End of Part III. We have laid out how to determine if GA is right for a task and how to integrate it in a balanced way. Next, we apply these ideas concretely to power systems engineering problems in Part IV.)*

---

## Part IV: Power Systems Deep Dive (proof of concept)

### Chapter 14: Three-Phase Unbalance as Geometry

#### 14.1 Fortescue's Theorem Geometrically

Every power engineer knows Fortescue's theorem: any unbalanced three-phase set can be decomposed into a combination of balanced positive-sequence, balanced negative-sequence, and (if four-wire) zero-sequence components. Traditionally, this is an algebraic decomposition using complex numbers or matrix transforms. But what is it geometrically?

In GA terms, balanced three-phase voltages (positive sequence) form a plane in the 3D phase-space. Specifically, if $v_a + v_b + v_c = 0$ and they are 120° apart, the vector $\mathbf{v}_{abc} = v_a e_1 + v_b e_2 + v_c e_3$ lies in a 2D subspace (since one degree of freedom is redundant). The positive-sequence component can be seen as a bivector $B_+$ oriented in that plane (or equivalently a dual vector normal to that plane). The negative-sequence is another bivector $B_-$ oriented in the opposite sense (phase rotation reversed). The zero-sequence is a grade-0 scalar (times the vector $e_0$ if using PGA to allow a common-mode).

Thus:

- **Positive sequence** = some magnitude bivector in (say) $e_{12}$ plane (if we align the reference frame to one phase or use Clarke's basis).
- **Negative sequence** = a bivector in the same plane but with opposite orientation (if positive sequence was $e_1 \wedge e_2$, negative would be $e_2 \wedge e_1 = -e_{12}$, essentially).
- **Zero sequence** = a scalar multiple of $e_1 + e_2 + e_3$ (or in GA terms, just a 0-grade when represented in symmetrical component basis).

We can formalize: Let $V = v_a e_1 + v_b e_2 + v_c e_3$ for an arbitrary unbalanced set. We find bivectors $B_+$ and $B_-$ and scalar $V_0$ such that:
$$V = \langle B_+ \rangle_1 + \langle B_- \rangle_1 + V_0,$$
with conditions that $B_+$ and $B_-$ lie in the plane $e_{123}$ (meaning $B_+ = \alpha(e_1 \wedge e_2)$ and $B_- = \beta(e_1 \wedge e_2)$ for some orientation we choose). Actually, it might be easier: $B_+$ and $B_-$ are not purely grade-1; they are grade-2 bivectors, but each bivector in 3D is dual to a vector (the sequence component phasor). Perhaps more straightforward: Represent positive and negative sequence as complex numbers usually. In GA, represent them as 2D vectors in the $\alpha\beta$ plane. Then $V$ splits into a sum of an $\alpha\beta$ vector (pos seq), another $\alpha\beta$ vector (neg seq, rotated opposite), and a vector along $(1,1,1)$ (zero seq).

The key insight GA gives is that these sequences are just basis projections onto orthogonal subspaces:

- The positive sequence subspace is spanned by $e_+$ and $e_+'$ (some orthonormal 2D basis for the balanced plane).
- The negative by another 2D plane (which in fact is the same physical plane but consider orientation).
- The zero sequence by the 1D line along (1,1,1).

These subspaces are orthogonal (the dot product of any vector from one subspace with one from another is zero). Clarke's and Fortescue's transforms are basically finding these orthogonal axes.

So Fortescue's theorem is nothing mystical: it's just the statement that the space of three-phase vectors decomposes into three orthogonal subspaces (2D + 2D + 1D). GA expresses this as grade projections. If we had an operator to project a 3D vector onto the positive-sequence plane, that's one operation (which could be done by a GA rotor or by a dot with dual basis). The GA approach would find $V_+$ as $\frac{1}{3}(V + R_{120}V + R_{240}V)$ or something (like averaging rotated versions, which is basically what the Fortescue formula does with complex $a$ operator).

Anyway, the message: symmetrical components correspond to geometric eigenspaces of a 120° rotation operator (positive seq = eigenvector with eigenvalue 1, negative seq = eigenvalue 1 in opposite rotation sense, zero seq = trivial eigenvector). GA can derive them by noticing that a 120° rotation in phase space is represented by a rotor $R_{120}$ and then splitting the vector into parts that are even or odd under that rotor.

This viewpoint can yield alternate ways to compute unbalance metrics. For instance, one could measure how far the system's voltage vector is from lying in the balanced plane by measuring the component of the trivector (volume). If there's a non-zero trivector part in $V_1 \wedge V_2 \wedge V_3$, that indicates zero-sequence presence, etc.

#### 14.2 Unbalance Metrics That Make Sense

Traditionally, we quantify unbalance by percentage difference between phases or by symmetrical component ratios (like ratio of negative to positive sequence voltage magnitude). Those are fine but somewhat abstract. GA suggests more geometric measures:

**Angular deviation:** Consider the actual three-phase voltage vector $\mathbf{v}_{abc}$ in 3D. If it's perfectly balanced, it lies in the $\alpha\beta$ plane (say the $e_{12}$ plane after Clarke). If unbalanced, it will have a component out of that plane (due to negative or zero sequence). We could define an angle $\theta_{\text{unb}}$ between $\mathbf{v}_{abc}$ and the nearest balanced plane (the $e_{12}$ plane if we oriented properly). That angle directly indicates how unbalanced it is. 0° means perfectly balanced, a larger angle means more unbalance.

**Ratio of bivector magnitudes:** The GA power of positive sequence is in a bivector $B_+$ and negative in $B_-$. The ratio $|B_-|/|B_+|$ is essentially the ratio of negative to positive sequence (like the complex method but here it's magnitude of grade-2 parts). So that's similar to old metric but derived as lengths of geometric objects.

**Dual space angle:** Alternatively, in the dual (since a bivector has a dual vector representing the axis of sequence components), measure the angle between that axis for positive vs negative part. If they align, then one is just smaller amplitude along same axis (just under-voltage?), but if they are rotated relative, that indicates phase shift in negative seq etc.

One could also derive a scalar invariant: e.g., for a three-phase set of vectors, define $U = \frac{\|\mathbf{v} \wedge R_{120}\mathbf{v}\|}{\|\mathbf{v}\|^2}$. If $\mathbf{v}$ is balanced, rotating by 120° and wedging with original yields maximal area (balanced yields some fixed ratio). If unbalanced, maybe that wedge is smaller. This could be normalized to give a 0–1 unbalance factor.

The point is, GA can give coordinate-free measures. Traditional unbalance metrics often depend on referencing a particular phase as 0° and computing phasors. GA ones could say: no matter how you label phases, the angle of the vector out of the balanced plane is an invariant measure of unbalance.

Power standards like IEEE define unbalance as negative seq % of positive. GA doesn't overthrow that; it just re-expresses it. But it ensures we're clear that it's an invariant (doesn't depend on frame orientation or phase labeling).

In practice, we'd likely still compute symmetrical components via matrix (like Fortescue matrix). But GA assures that metric is tied to geometric angles which might give better intuition (like "your voltage vector is 5° out of the balanced plane" vs "7% unbalance").

### Chapter 15: Harmonic Power Under Distortion

#### 15.1 The Grade Explosion of Harmonics

We earlier discussed how mixing different frequency components can cause the power multivector to have higher-grade parts. Let's detail that with a small example:

Suppose $v(t) = V_1 \cos(\omega t) + V_3 \cos(3\omega t)$ (a fundamental and a 3rd harmonic in one phase), and $i(t) = I_1 \cos(\omega t - \phi) + I_5 \cos(5\omega t - \psi)$ (fundamental plus 5th harmonic current). If you form the geometric product of their corresponding vectors (summing contributions from all phases possibly), you'll get terms at various frequencies:

- The **1×1** (fundamental * fundamental) gives DC (active) and 2ω (reactive oscillation).
- The **1×5** gives 4ω and 6ω components (sum and difference).
- The **3×1** gives 2ω and 4ω.
- The **3×5** gives 2ω and 8ω.

So the instantaneous power has frequencies at 0, 2ω, 4ω, 6ω, 8ω maybe.

In GA, those appear as different grade components likely because to represent something oscillating at 4ω in a static algebra, it might manifest as a 4-vector (which is weird physically but mathematically possible in PGA as a pseudoscalar multiple varying sign). This is speculation: a rigorous way is possibly to embed time as another dimension or just treat time variation as parametric.

Another viewpoint: the presence of 5th harmonic current means current vector is not just a single rotating vector in $\alpha\beta$ plane at $\omega$; it's a combination of one at $\omega$ and one at $5\omega$ (which in a synchronous reference frame would appear as oscillating at 4ω relative). That can be captured by a bivector at one frequency plus another bivector for the other. When you multiply with voltage's combination, you get wedge products between bivectors of different rotation rates, which yield grade-4 (like two independent planes in a 4D space if you considered time a dimension).

Yes, if we considered the space expanded by adding an axis for the 5th harmonic component, then $v_5$ and $i_5$ live partly in that extended space. The product can yield an object that spans dimensions outside the original 3D (like $e_{1230}$ a grade-4 if $e_0$ was an extra dimension for an independent frequency component). The user text suggests that cross-harmonic products produce grade 4 in 3D, which indicates "something trying to exist outside the space" – basically an artifact that the 3D model isn't enough to house both frequencies' interaction. In physical terms, that might indicate an oscillatory energy exchange that doesn't simply fit into P or Q.

Inter-harmonics (non-integer multiples) would complicate further – they produce incommensurate oscillations that can't be neatly separated by sequence components. The GA model would probably then require an infinite-dimensional basis (or at least many dimensions, one per independent frequency). That's not practical, so we handle it by time-domain or frequency analysis.

So GA acknowledges that complex numbers fail when distortion is present, because you can't reduce everything to one phasor. But GA itself doesn't magically solve it; it just says "we now have to consider higher-grade components or more dimensions". It reveals the complexity rather than hiding it.

#### 15.2 Measurement Implementation

One practical aspect: measuring power in distorted conditions often involves windowing and filtering (like computing a sliding FFT or using a specific formula for reactive power like p-q theory with LPFs). GA could provide a geometric interpretation for these.

For instance, a window function in time (like Hanning window) when applied to instantaneous power might correspond to a grade projection in GA that smooths certain components. A rectangular window is like an abrupt cut-off, which can cause mixing (spectral leakage, in Fourier terms). In GA, that might manifest as a non-commuting projection: if you rotate frame during the window it matters.

Perhaps we can design window filters that respect GA structure: maybe ensure the window is aligned with the fundamental frequency rotor motion so that it doesn't mix real and reactive (like using a whole number of cycles in the window to isolate P and Q clearly). This is known in signal processing, but GA highlights that those leakage terms are literally extra grades creeping in.

For example, if $S(t)$ has grade-0 and grade-2 parts, integrating (averaging) over an integer number of fundamental cycles will cancel out the grade-2 oscillatory part (assuming it's at 2ω) yielding just scalar P. But if you use a shorter window, some of that bivector part remains in the average, giving an incorrect P reading and some spurious residual reactive reading.

A Hanning window reduces leakage by tapering edges, which in GA terms might reduce the spur of grade mixing by smoothing the time weighting (less abrupt, so less high-frequency content created).

So the GA view would say: choose a window that is symmetric with respect to the rotation by $\omega t$ in the $\alpha\beta$ plane, to avoid coupling into unwanted grades.

While this is a bit abstract, it leads to recommendations: prefer windows that match the periodicity of dominant components or use filters that cleanly separate grades (maybe a GA grade filter is something like taking the part of $S$ that commutes or anti-commutes with some known rotor? Perhaps one could extract the fundamental component by such algebraic means).

At any rate, GA doesn't eliminate the need for spectral analysis, but it could unify understanding of power components across time and frequency. There's ongoing research (like in references [28]) into GA power theory that covers these distortions.

### Chapter 16: Fault Analysis Via Incidence

#### 16.1 All Faults Are Meets

Power system faults (short circuits) can be classified by which conductors are connected:

- **Single line-to-ground (SLG):** one phase touches ground.
- **Line-to-line (LL):** two phases touch each other.
- **Double line-to-ground (DLG):** two phases to ground.
- **Three-phase fault (LLL or LLLG** if with ground).

Geometrically, we can think of:

- **Phases as lines** (wires) in space (or their fields as lines in some configuration space).
- **Ground as a plane** (the earth plane at 0 potential).
- **A fault is where a line meets** either another line or the ground plane (or all three lines meet at a point in a three-phase short).

In PGA, a line is a bivector $L$. Ground plane is a vector $\Pi_{\text{ground}}$. The meet $P = L \vee \Pi_{\text{ground}}$ (or $(L^* \wedge \Pi_{\text{ground}}^*)^*$) gives the point of intersection (the fault location on that line). If two lines (phases) fault, you do $P = L_1 \vee L_2$ (their meet). Three lines faulting means $P = L_a \vee L_b \vee L_c$ (all three meet at a point) – in GA you'd do pairwise meets or a single expression with two wedges inside.

So indeed:

- **SLG fault:** $P_{\text{fault}} = L_{\text{phase}} \wedge \Pi_{\text{ground}}$ (should be a point, possibly at infinity if no physical intersection in space means maybe the fault is at infinite distance? Actually if a line is truly parallel to ground and never touches, no fault).

- **LL fault:** $P_{\text{fault}} = L_1 \wedge L_2$ (point or line if they overlap).

- **LLG fault:** two lines meet ground: you could find each line-ground meet and see if they're same or perhaps treat it as meet of all three objects.

- **3-phase fault:** $P_{\text{fault}} = L_a \wedge L_b \wedge L_c$ (if non-coplanar, actually three lines in 3D rarely meet exactly unless at substation or via an arc that forms some geometry; anyway in network terms, it's a node where all join).

The nice thing is, GA doesn't require writing separate equations for each fault type – the meet covers them. And if lines are parallel (no intersection) the meet yields the ideal line (no finite solution), telling you fault is at infinity (which in real terms means no fault / or a fault at an equivalent infinite distance).

Uniformly, after computing the meet point, you can determine fault distance or location along a line by further inner products (like distance from source node).

The residual or condition $|L \wedge \Pi|$ can serve as an index of how direct the fault is:

- If exactly zero, the line and ground meet in a finite point (definite fault).
- If very small but not zero, near-miss: maybe leakage or an arcing fault that hasn't fully shorted but nearly (the lines or ground are approaching).
- For skew lines (like a transmission line falling on another that's not parallel), $L_1 \wedge L_2$ yields a line (they don't meet, just skew). The GA result would indicate something like a line of shortest approach (that line is the line connecting the two closest points).
- For a true short, $L_1$ and $L_2$ become coincident at the fault (their meet is a well-defined point).

Therefore, one could create a protective relay logic: compute $P = L \wedge \Pi_{\text{zone}}$ (where $\Pi_{\text{zone}}$ is a plane demarcating zone boundary), if $P$ is finite and within the zone volume, trip.

#### 16.2 Protection Coordination Geometry

In protection coordination, each protective device covers a certain zone (line segment, bus, etc.). If a fault happens, you want only the device whose zone contains the fault to trip (primary protection) and maybe one next up if primary fails (backup).

In GA terms, a zone can be represented as a volume or region (like an intersection of half-spaces/planes – essentially a polyhedron or something). Determining if a fault point lies in a zone is an incidence test: does point $P$ lie on the correct side of all zone boundary planes? This can be done by plugging $P$ into each plane's equation (via inner product in GA) and checking sign.

So indeed:
$$\text{Operate if } P_{\text{fault}} \in \text{ProtectionZone},$$
meaning for all boundary planes $\Pi_i$ of the zone, $(P_{\text{fault}} \cdot \Pi_i)$ has the sign that indicates inside.

GA can unify checking all those with wedge perhaps: one could form a big wedge of $P$ with all boundary planes and see if it yields a pseudoscalar of correct sign. But simpler: just evaluate signs individually.

Overlapping zones for backup is natural: zone2 might overlap zone1 partly. That's just geometry.

Selectivity problems (overtripping) often come because of CT saturation, etc., not geometry. But GA could at least ensure the logic of inclusion is correct.

#### 16.3 Real Implementation Results

From the prompt: for a test system:

- **Traditional distance relay computation** used an impedance measure (voltage/current) and took ~140 μs, with separate logic for each fault type (phase-phase vs phase-ground, etc.).

- **GA implementation:** single meet formula, 400 lines of code instead of 2400, but took 890 μs on same platform – 6× slower.

For primary protection (which might need to operate within, say, 1-2 cycles ~ 20-40ms), 0.89ms is actually fine. Even 6x slower, it's within margin. So GA could be acceptable. For backup or analysis, speed isn't issue.

But if we needed it in a super fast auto-reclosing or something within microseconds, maybe not.

Anyway, the GA code handling all faults uniformly is easier to maintain and verify (80% fewer bugs as noted), which is worth a slightly slower runtime that's still within requirements.

Thus, in fault analysis, GA is likely beneficial for off-line studies and possibly on-line relays if carefully optimized. But one must ensure enough hardware capacity (maybe use a DSP that can handle the extra math).

### Chapter 17: Space-Vector Modulation as Rotor Choreography

#### 17.1 Eight Switching States as Geometric Objects

A three-phase two-level inverter has 8 possible switching states (2^3):

- **6 active states** where one phase is at +Vdc/2 and two at -Vdc/2 (or combinations, basically corresponding to 6 non-zero space vectors evenly spaced 60° apart in the αβ plane).
- **2 zero states** where all outputs are equal (both at + or both at -, yielding 0 vector).

These 6 active vectors indeed form a hexagon in the αβ plane. The reference voltage vector (the desired output) rotates as a rotor in that plane (if we neglect zero-seq, which in SVM we do by balancing the zero times).

SVM essentially says: at each cycle, find the triangle of the hexagon in which the reference lies, and use the 2 adjacent active vectors (the vertices of that triangle) plus zero to approximate the reference by time averaging.

So geometrically: the reference vector $v^*$ at some time is inside a triangle formed by $(0, V_k, V_{k+1})$ where $V_k$ and $V_{k+1}$ are two adjacent state vectors, and 0 is the origin (the null state). Actually, some implementations use the two active and one of the zero states as needed.

The duty cycles are basically the barycentric coordinates of $v^*$ in that triangle. If $v^* = d_k V_k + d_{k+1} V_{k+1} + d_0 (0)$ with $d_k+d_{k+1}+d_0=1$, then $d_k,d_{k+1},d_0$ are the fraction of time to spend in each state.

In GA, each switching state can be represented as a vector in 3D or in αβ0 space. Actually, in $\alpha\beta0$ coordinates:

- Active states correspond to certain combinations, e.g., (2/3, -1/3, -1/3) in abc leads to a certain αβ vector.
- The two zero states correspond to the zero vector plus maybe a zero-sequence (common-mode, which we usually ignore by clamping or if using symmetrical, they produce same output line-to-line).

So treat each switching state as a vector $\mathbf{S}_i$ in R^2 (for αβ, ignoring zero). Then the reference is a rotor rotating vector $\mathbf{v}(t)$.

Finding nearest three states can be done by identifying sector (k). Usually done by checking the sign of $\alpha$ and $\beta$ etc. In GA, one might do something like: Take the 6 active vectors (bivectors? Actually vectors in plane), wedge the reference with each to see which wedge is positive/negative to tell on which side of each separating line it is. But anyway, one can identify the containing triangle by a series of dot products.

Once you have the two nearest active states, computing duty cycles is solving $v^* = d_1 S_1 + d_2 S_2$ (since d0 is just leftover to make sum 1). That's two equations if we do in αβ plane, easily solved (like using cross product or wedge to find area ratios).

A GA viewpoint: if $\mathbf{v}$, $\mathbf{S}_i$, $\mathbf{S}_j$ are all in a plane, then
$$d_j = \frac{\mathbf{S}_i \wedge \mathbf{v}}{\mathbf{S}_i \wedge \mathbf{S}_j},$$
which is essentially computing areas (barycentric coordinates by area ratio). This wedge formula is neat because if $v$ is outside, it might give negative duty which tells you logically it's in adjacent sector. So GA could unify the logic: you wouldn't separately code each sector's formula; you could plug into a general wedge-based formula.

But computing those wedges (which are just scalar area values in 2D) is trivial anyway (a couple multiplications). GA not needed to get formula but it ensures it's correct for all cases.

The overhead came in performing these for every switching cycle in GA (340 µs vs conventional 12 µs). Likely because the GA approach did more steps:
- Maybe constructing multivectors, computing wedge via a GA library vs just a simple multiply-subtract for cross product.

So while conceptually elegant, it's overkill at runtime. People hand-code SVM with if-else and a few math ops because it's so time-critical in high frequency switching.

Perhaps if we had GA hardware, we could do it differently, but currently not. So SVM is a case where GA clarity might help in understanding or writing a generic SVM algorithm that works for any number of levels or phases (like a GA generalization could possibly handle multi-level inverters by just adding more state vectors in the space and doing a polytope barycentric calc), but at runtime you'd still implement that as specific if-else or linear solves.

Thus, GA is more of a tool to derive or generalize SVM rather than to implement it directly on a microcontroller.

#### 17.2 Why It's Too Slow

This was already partly answered: GA didn't fit the microsecond budget because:

- Identifying the sector via GA might have involved checking multiple wedge signs or meet with each region's half-plane, causing warp divergences or just more ops than a simple compare network.
- Duty calculation via wedge operations is minor overhead but maybe still more than a direct formula.

SVM is heavily optimized in practice (some do it with lookup tables or precomputed sin values). GA approach might be dynamic and elegant but microcontrollers like simple arithmetic.

So GA fails in this *very high speed* control inner loop scenario by ~7x as given. One could conceive maybe using GA offline to precompute switching tables or optimize modulation schemes (some research uses geometric methods for optimal modulation), but the actual modulation implementation will remain straightforward code.

---

### Chapter 18: Motor Control With Geometric Tools

#### 18.1 Everything Is a Rotor

Field-oriented control (FOC) of AC machines essentially aligns the stator current vector with the rotor flux vector. In an induction or synchronous motor:

- The stator has a rotating magnetic field (the current produces an MMF that can be seen as a rotating vector in space).
- The rotor (either actual magnets in synchronous or induced currents in induction) has its own field vector (rotor flux).
- Torque is proportional to the cross product of stator and rotor field vectors (in 3-phase, $T \propto \mathbf{B}_{stator} \times \mathbf{B}_{rotor}$ or sine of angle between them, etc.).

In GA terms, these magnetic field vectors (or rather the current and flux linkage) can be treated as vectors or bivectors. Actually, in STA (space-time algebra) one might use bivectors for E and B fields, but here simpler: treat them as 2D vectors in the dq plane attached to rotor or stator reference.

FOC aims to keep the stator current vector (in stator frame it's oscillating) aligned with rotor flux (which in the rotor frame is mostly DC on the direct axis). We do Park transform to rotor frame, then control id to match flux, and iq to produce torque.

From GA perspective:
- The rotation from stator to rotor frame is a rotor (the angle difference between stator field and rotor).
- Once aligned, the quadrature component (perpendicular) generates torque, the direct component aligns with flux (like active vs reactive magnetizing current).
- One could describe the dynamic of the machine as the rotor flux vector being dragged along by rotor motion, and the stator current vector being driven by the inverter rotor (like a chase).

Saliency (in e.g. IPMSM) means the inductance differs by axis – in GA, that might be seen as the reluctance (inverse of inductance) being a bivector that is not isotropic but oriented with rotor, meaning the machine differential equations have rotor-angle-dependent terms (harder to express in GA without breaking linearity... perhaps a rotor that maps an isotropic inductance matrix into an anisotropic one).

Reluctance torque (as in a reluctance motor) basically arises because the current rotor tries to align where inductance is highest for given current, which is also a geometric condition (minimize co-energy etc.). GA might express that elegantly by saying the co-energy is $\frac{1}{2} \mathbf{i} L \mathbf{i}$ with L as a rotor-variant operator, etc.

But overall yes, "Everything is a rotor" – the core idea is all these rotating fields and transformations can be described with GA rotors:
- The stator field is a rotor times some reference orientation.
- The rotor itself can be described by a rotor (physical angle).
- The transformation between frames is a rotor.
- The optimal control is to set a certain rotor equal to another or offset by 90° etc.

This unified view could conceptually simplify deriving control laws, e.g. how to handle a sudden change in rotor angle: it's just updating one rotor variable.

#### 18.2 Why FOC Is Natural in GA

In GA, one can represent the rotor's rotation by a time-varying rotor $R_{rotor}(t) = \exp(-\frac{\theta_r(t)}{2} e_{12})$ (assuming $e_{12}$ is plane of rotation). The stator currents in stationary frame, represented as $\mathbf{i}_{\alpha\beta}$, can be "seen" in rotor frame by sandwich:
$$\mathbf{i}_{dq} = R_{rotor}(t) \mathbf{i}_{\alpha\beta} \tilde{R}_{rotor}(t),$$
which essentially does the Park transform (with $\theta_r$). If rotor angle is from encoder, we feed that in.

Now FOC says: control $\mathbf{i}_{dq}$ such that $i_q$ yields desired torque and $i_d$ yields desired flux (or is zero for surface PM). That means we want $\mathbf{i}_{dq}$ to align a certain way with rotor flux $\mathbf{\psi}_{r}$ (which might be mostly along d-axis if magnet or built by i_d if induction).

In GA, alignment means the wedge of $\mathbf{i}_{dq}$ and $\mathbf{\psi}_r$ is zero (if aligned) or proportional to torque if perpendicular. So one can say: we want $\mathbf{i}_{dq}$ to equal some reference $\mathbf{i}^*_{dq}$. That can be enforced by controlling it via voltage (which is another vector in that frame).

So one rotor eq could be: $R_{control} = R_{\text{desired}} R_{rotor}^{-1}$ – basically the rotor that would rotate current into alignment with flux.

Actually: If rotor flux is at angle 0 (by definition d-axis), and we want current aligned with it (for d component), then set $i_q^*=0$. Often, FOC implementation: measure current, transform to dq (via rotor angle), control with PI to force $i_d$ to reference (e.g. 0) and $i_q$ to reference (for torque), then transform voltage commands back to αβ and output to PWM.

This is literally composing and inverting rotors:
$$v_{\alpha\beta} = \tilde{R}_{rotor}(t) [v_d^* + v_q^*] R_{rotor}(t),$$
since to command those $v_d, v_q$ in rotor frame, you invert Park to stator frame.

So one might say: the entire FOC loop is doing $R_{rotor}^{-1}$ on measurements, then $R_{rotor}$ on outputs. So in code, you do the Park and inverse Park. GA acknowledges that $R_{rotor}(t) R_{rotor}(t)^{-1} =1$ so it's lossless transform except numerical stuff, and that $R_{rotor}$ being time-varying requires decoupling terms (in synchronous frame, that appears as +$\omega L i_q$ terms etc. – which GA can derive by differentiating the rotor action).

Anyway, GA doesn't change the control law results much (we still use PIs etc.), but it assures the geometric correctness. One can envision more exotic control using GA: e.g., directly command a rotor for voltage that rotates the voltage vector by some angle to achieve decoupling without computing separate d and q components – that might be an interesting GA-based controller (basically treating the complex voltage as one entity and using a complex controller that rotates outputs based on errors – which some sensorless control algorithms do with vector integrators).

So GA clarifies and potentially leads to alternative formulations (maybe for sensorless observer design, using rotors to represent error in angle and letting a estimator adapt that rotor via some gradient).

Thus, FOC is "natural" because it's just aligning rotors: one for electrical angle, one for control angle, etc. GA would handle all that elegantly if running on a platform that could do it (but we implement as usual due to speed).

---

## Part V: Beyond Power *(surgical evidence)*

### Chapter 19: Robotics — Seeing Singularities

In robot kinematics, a singularity is when the Jacobian loses rank (the robot loses a degree of freedom, or vice versa cannot move in some direction). Traditionally, we find singularities by computing det(J*J^T) = 0 or something.

GA offers a geometric insight: often singularity occurs when some set of joint axes become coplanar (for a 6-axis arm, e.g., wrist aligned with elbow axis etc.). Using GA, one can represent each joint's screw axis as a line (in 3D PGA, a bivector for each revolute joint). The reachable twist space of the robot is basically the join of these screws (span in Lie algebra terms). A singularity arises when these screws become linearly dependent (their wedge product all together goes to zero volume).

So if you wedge all 6 joint axes (lines) in PGA and get a 0 (grade-6 blade etc.), you're singular. Actually for 6 joints, wedge all 6 lines gives a 12-vector (the pseudoscalar in 6d or something) which being zero means dependence.

So GA says: singularity ↔ the volume spanned by the screw axes is zero. And the volume (pseudoscalar magnitude) can serve as a *singularity measure* — its absolute value tells how far you are from singular (like manipulability measure in classical, which uses det(JJ^T)).

This is much more intuitive: volume of parallelepiped of axes → zero means flattening. For a 6R robot, that volume in 6D is hard to imagine, but GA offers incremental: check if any 3 axes coplanar (that kills some volume), etc., which is easier to visualize.

So GA can help in singularity analysis and perhaps design: you want axes arranged so that volume stays large in workspace.

We might implement:
$$\text{singularity\_index} = \| L_1 \wedge L_2 \wedge ... \wedge L_6 \|,$$
for screw axes $L_i$. If it's zero, singular; else positive value. We've effectively replaced computing all 6×6 minors with one geometric quantity.

In practice, computing wedge of 6 lines (each 6 components in Plücker coords, wedge yields 20-something dimension?) might not be simpler numeric, but conceptually it's one operation.

So robotics can benefit from GA especially in areas like:
- Singularity analysis
- Inverse kinematics (solving via meet of geometric constraints rather than using DH and trig)
- Error propagation on SE(3) (though probability issues apply, but GA can do deterministic error).

### Chapter 20: Computer Graphics — Natural Interpolation

Quaternions are used for smoothly interpolating rotations (slerp), but they have the double-cover issue: $q$ and $-q$ same rotation, which can cause interpolation to take the long path if not handled (need to choose shortest arc by possibly negating one quaternion to align). People handle that by ensuring dot product positive before slerp.

Rotors in GA have the same double cover property (since rotor and -rotor yield same rotation). However, if one stays in GA, one might notice earlier if the two rotors have an angle > 90° between them (in their 4D space) and flip one. So similar logic but maybe GA users incorporate that naturally.

One advantage of GA in interpolation: you can interpolate not just rotations, but combined rotation-translation (motors) with similar exponential and logarithm. Dual quaternions (which are essentially motors) can do screw linear interpolation (slerp equivalent for rigid motions). That avoids having to separately interpolate position and orientation that can produce weird combined paths.

GA's versor interpolation gives a proper screw motion – which is the natural shortest path in SE(3). This is great for keyframe animation: if you have two poses of a character, you can interpolate by a single motor rather than separately easing rotation and translation which can cause non-rigid intermediate (like rotation then translation as separate might arc differently).

As noted, the overhead is 2.6× or more. But for offline rendering or during a setup phase it's fine. For real-time games, maybe too heavy (maybe precompute splines offline).

Nevertheless, GA provides unique capabilities like blending transformations by addition in algebra (taking log and averaging). You keep equivariance: if you transform all keyframes by something, the interpolation transforms accordingly – not always true with naive Euler angle interpolation.

So graphics can use GA in authoring tools, maybe less so in the inner loop of a game engine. But as GPUs get stronger and if someone implements dual quaternion skinning on GPU (which they do, actually many engines use dual quaternion for skinning now because it's better than linear blend), that's essentially GA concept.

(They likely don't mention GA, they just use formula, but it's GA under hood.)

### Chapter 21: Machine Learning — Built-in Equivariance

Equivariance means model outputs transform predictably when inputs transform (like a rotationally equivariant network will rotate outputs if input is rotated). Graph networks and transformer architectures now incorporate symmetry by design.

A Geometric Algebra Transformer (GATr) presumably uses GA objects as features so that rotations etc. are handled implicitly by the algebra rather than learned from data.

For example, if input is a molecule (with 3D coordinates), a standard transformer might have to learn that rotating the molecule shouldn't change chemical properties. But a GA transformer could encode interatomic distances and oriented planes etc. so that the representation is inherently invariant to rotation.

**Data efficiency:** if the model doesn't have to learn symmetry (it's built-in), it needs fewer examples. The user says 10× data efficiency (5K vs 50K samples) for 90% accuracy in a molecular property prediction. That's a huge win in domains where data is scarce (like drug discovery).

Training time was 3× longer, likely because GA operations are heavier or the model is more complex per sample. But in scenarios where data collection is expensive (labs, simulations), using more compute is fine to save on data.

So GA in ML is niche but promising: one example is Clifford neural layers where multivector multiplications are used to preserve rotation equivariance. Research exists on Cl(3) networks for 3D (a paper by Chappell et al perhaps).

We just note: if you need to encode something like orientation or reference frames in ML, GA is a potential toolkit. It's complicated though; most practitioners use group theory simpler (like quaternion layers or so). But given interest in symmetry, GA might come in more.

### Chapter 22: Crystallography — All 230 Space Groups

Crystallographic space groups describe 3D lattice symmetries (rotations, reflections, translations combinations). There are 230 distinct ones in Euclidean space.

If we view all rigid motions (rotations+translations) as the group of motors in PGA, each space group is a subgroup generated by some set of symmetries (rotors for rotations, plus translators for lattice periodicity).

In GA terms, all these can be expressed as combinations of versors (reflections and translations). Possibly each space group can be generated by 3-4 fundamental motions (like a 2π/n rotation about an axis, a translation along certain vector, etc.).

Using GA, one can represent these generatively:
- Represent each generator as a motor $M_i$.
- Then any symmetry is a product of some sequence of these motors.

By working in GA, one might unify the representation: one algebra (Cl(3,0,1) plus maybe inversion for reflection) could handle all, whereas traditionally crystallographers have to use matrices and lists of group elements.

So instead of coding 230 cases, one could programmatically generate group elements by just specifying the generating versors for each group. This reduces code (70% less as said). Bug reduction 90% because you rely on algebra constraints rather than enumerating each possible element.

Performance 2× slower maybe if computing on the fly vs using specialized lookups, but probably fine for analyzing a crystal structure offline.

So a GA approach might be:
- Input the set of generators (maybe gleaned from standard crystallographic data).
- Use GA to compute or verify all combinations up to needed order (like find all distinct group elements by multiplying out).
- Then apply these to a base motif to generate the full crystal structure.

Because GA ensures closure and uses actual group law of versors, it's less error-prone than manually writing transformation matrices (where a slip in a sign or axis can break symmetry and is hard to debug).

This was somewhat done by F. Gao or S. De Keninck maybe? But anyway, it's plausible.

In summary, GA can unify concepts in crystallography or any group theory where the transformations are geometric. It doesn't change underlying math, but speaking in versors often simplifies reasoning (like combining screw operations elegantly vs 4x4 matrices full of numbers).

Speed being 2× slower is fine because we are not doing it a billion times, just analyzing structure.

This theme extends beyond crystals: any parametric group of transformations (architectural design patterns, robot gait cyclic motions, etc.) could be elegantly handled with GA too.

---

## Part VI: Conditional Futures *(what could change everything)*

### Chapter 23: Dimensional Fluidity and Sparsity

#### 23.1 What If Dimension Isn't Fixed?

We often treat GA in a fixed dimension (3D, 4D, etc.). But mathematically, one can extend to fractional dimensions via analytic continuation of formulas like the gamma function. The statement about n-ball volume vs surface: $V_n / S_n = 1/n$ for integer n (volume of unit ball and surface of unit sphere in n dims). Actually known formula: $V_n = \pi^{n/2}/\Gamma(n/2+1)$, $S_n = 2\pi^{n/2}/\Gamma(n/2)$. Their ratio indeed simplifies to $1/n$. This holds for all real $n$ if interpreted via Gamma, meaning one can define a "volume" in fractional dims.

If GA could operate in non-integer grade spaces (like an algebra where you allow grade count to be 2.5, wild idea), maybe you can smoothly tune the number of basis vectors as a parameter.

**Implication:** We might adjust effective dimension to preserve sparsity. For example, if some operation would normally fill all grades in 3D (like near 16 components), maybe working in a space of dimension 2.5 somehow restricts coupling?

This is speculative and far-fetched, but they hint at a dream: make GA computational cost scale linearly with some effective dimension rather than $2^n$ by using continuous $n$ that matches actual used complexity.

Alternatively, maybe using fractional dimension relates to fractal or non-integer basis for signals. Not sure how realistic, but it's an open theoretical horizon.

#### 23.2 p-Adic Geometric Algebra

p-adic numbers use an alternate metric (ultrametric) where distance is measured by divisibility by a prime p. Triangles are weird: any triangle, the longest side is at least as long as others combined (ultrametric inequality).

Mapping that to GA: possibly if one could define a GA over p-adic number field, lengths and angles behave differently. It might cause "sparsity" because small angles become effectively zero beyond a threshold (due to p-adic norm being discrete in a sense).

If GA were p-adic, maybe sums that produce small components (which cause densification in real because nothing is truly zero) might actually vanish exactly in p-adic if under threshold. That could keep things sparse by ignoring components beneath a certain significance automatically (like because 10^-5 is just smaller p-adic valuation maybe sets it aside).

This is quite theoretical. But maybe a p-adic GA would not mix grades as easily since the ultrametric might prevent multiple non-orthonormal basis from all being significant at once (like triangle collapse analogy, maybe wedge of nearly parallel things yields something with large p-adic norm consequences that effectively treat them as exactly parallel beyond some resolution).

Alternatively, p-adic GA might represent hierarchical or multi-scale geometry, which could compress information.

This is a long shot as said (10+ years). It's more a creative suggestion: to fix GA's computational heaviness, maybe consider number systems where "nearly special case = exactly special case" due to limited resolution, thus algorithmically simpler.

It's not clear if that pans out. But interestingly, there's a connection: GA operations are polynomial. One might solve them mod some base to glean structure.

I think the main idea: if computing GA in a different metric or number system that prunes insignificant interactions, then complexity might drop from $n^2$ to linear. Certainly not mainstream currently.

### Chapter 24: Hardware That Could Flip the Verdict

#### 24.1 FPGA Geometric Products

One way to speed GA: build custom hardware that directly does multivector multiply in parallel. An FPGA with 16 multipliers can multiply two 8-element motors in one go perhaps.

Actually, Klein's code was sort of that on CPU with SSE, but an FPGA could go further:
- Pipeline the operations (so each clock new result).
- Grade-aware routing: only route necessary pairs to multipliers to not waste power.

If someone built a chip or core specialized for Cl(3,0,1) operations, they saw 10× speed vs CPU (likely referencing something like a research FPGA that did GA, maybe by Dan Fontijne or others?), but it used 5× the power of a general CPU (so it's not more efficient, just faster in absolute terms at cost of power).

For embedded but powered (like in a car or robot, not battery-critical small devices), that could be fine. If you had a GA coprocessor delivering results quickly, it opens using GA in real-time.

We might imagine 10 years out, like there are now AI accelerators, maybe a geometry accelerator library becomes plausible if enough use cases align (like AR/VR needing lots of small transformations could benefit from a dedicated unit computing them in parallel with main CPU).

Still, it's niche now. But if available, it flips a lot of our earlier performance issues.

#### 24.2 Neuromorphic GA

Neuromorphic chips mimic brain neurons (spikes etc). They excel at certain parallel pattern recognition tasks. Mapping GA to neuromorphic: maybe representing a bivector as relative timing between spikes, a rotor as a synchrony pattern, etc.

If one can encode the geometric product as a detection of coincidences (like a neuron fires if two particular spikes coincide = multiplication?), maybe we can compute GA operations using neuromorphic's event-driven style super efficiently (since brain handles large parallel ops with low power).

For instance, reflecting a vector $\mathbf{v}$ across $\mathbf{n}$ requires computing $\mathbf{n}(\mathbf{n}\cdot\mathbf{v})$ essentially. A spiking network might do dot product by some weighted coincidence detection.

This is very speculative, but given neuromorphic research, any structured math could potentially be compiled to spiking network. We're likely a decade away from a stable neuromorphic computing paradigm that could do general computing tasks well.

But if it matures, GA, with its inherently parallel operations (all those independent products that sum up) could map nicely (like each neuron computes one basis component product, others sum them appropriately through network layers, etc., all in parallel asynchronously).

So if neuromorphic yields high throughput at low power for GA, that indeed flips the verdict – GA could become the naturally parallel algorithm that leverages those chips, and then maybe it becomes faster than sequential code.

This is hopeful: brain does geometry (our brain certainly can reason spatially much faster in analog ways than computers do in digital sometimes). Emulating that might involve GA-like structures.

These futures are uncertain, but including them highlights that the current computational drawbacks of GA are not fundamental to the math – they're contingent on current hardware biases. Change the hardware or arithmetic domain, and GA might flourish.

---

## Part VII: Reference and Synthesis

### Appendix A: Core Mathematics
Derivations of key formulas with a bit more detail and proofs. For example: double reflection derivation, proving rotor composition = matrix multiplication equivalence, deriving dq0 rotor from orthonormal Gram-Schmidt, etc., and proofs of identities like meet/join duality formula.

### Appendix B: Performance Data
Here we'd list exact FLOP counts for small ops vs measured times on certain hardware, memory layouts tested, caches, etc. Possibly tabulate things like:
- Rotor composition: 88 ops (theory), measured cycles on CPU vs GPU.
- Sandwich op: ops count and measured time.
- Meet of line-plane: ops vs if else code ops.
- etc, as well as memory usage differences.

### Appendix C: Convention Reconciliation
Comparison between different fields and how to translate. Eg:
- Physics uses Minkowski (1,3) vs GA (3,1) perhaps, table of sign differences.
- Graphics uses left-handed vs GA right-handed default, how to swap e2 sign to fix.
- Electric engineering uses complex for phasor, mapping that to GA bivector in plane or rotor with e0.

Provide cheat sheet for converting known formulas: e.g., complex power S = P+jQ corresponds to $P + Q I$ in GA with I the unit pseudoscalar of plane.

### Appendix D: Diagnostic Catalog
Collection of tricky examples and how to catch/handle:
- near parallel lines example and approach to detect as earlier.
- normalization strategy recommended values.
- maybe pitfalls like gimbal lock in GA (does it appear? rotor avoids singularities but if you parameterize, it can).
- etc.

### Appendix E: The Library Graveyard
A brief review of GA libraries:
- Klein (archived, reasons).
- Versor (cool demos, but not maintained or heavy to integrate).
- GAL (maybe refers to GA-FuL, which is a general library in dev).
- Others like Garamon, etc.

Focus on lessons: common pattern: one person projects, performance good but community small, so need integration into bigger ecosystems to last.

### Epilogue: Engineering Makes Math Honest

This final epilogue circles back to the theme: GA revealed beautiful unified mathematics; engineering forced us to confront the ugly realities of computation. The hybrid approach is not a compromise but an optimization – use the best of math and best of hardware.

Remind that future tech could align better with GA, but even if not, the discipline of being clear about invariants (math honest) and practical about implementation (hardware decides) is universally useful.

So we sign off encouraging using GA as a thinking tool and being measured about using it in practice.

The entire journey: seduction by mathematical elegance (Part I), feeling of betrayal by computational cost (Part II), reconciliation in combining with conventional (Part III), proof in a domain (Part IV), seeing it apply elsewhere (Part V), dreaming of better future (Part VI).

We close by reinforcing that engineering and mathematics need each other: math provides vision, engineering provides reality-check. GA is a case study of that interplay: it made many things clear (honest math), but also taught us that without considering hardware early, even the best math can stumble.

The hybrid path we chart is essentially engineering making the math work in practice, hence "makes math honest".

This arc hopefully left the reader inspired to use GA where appropriate, but armed with a realistic plan to do so effectively.
